{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real datanetwork with small sythetic network\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "# %run Check_Derivation_of_Two_Opinions.ipynb\n",
    "#%run pure_strategy_selection.ipynb  #include simple selection algorithm\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n",
    "from itertools import count\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "import scipy.optimize\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "# add parent directory to path so that utils is available\n",
    "sys.path.append('..')\n",
    "from utils import *\n",
    "\n",
    "from game import *\n",
    "\n",
    "# Game Parameters\n",
    "game_rounds = 5\n",
    "memory = 50\n",
    "k = 2\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# Create the 'results' directory if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "np.set_printoptions(precision=5)\n",
    "\n",
    "calculate_polarization = calculate_polarization1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Fixed initial condition + memeory = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Synthetic Network Data\n",
    "df = pd.read_csv(f'{current_path}/data/Adjacency Matrix.csv', header=None)\n",
    "G = np.array(df[df.columns[:]])\n",
    "# print(G)\n",
    "df1 = pd.read_csv(f'{current_path}/data/Innate Opinion.csv', header=None)\n",
    "s = np.array(df1[df1.columns[:]])\n",
    "# print(G.shape)\n",
    "\n",
    "# Set n according to the data\n",
    "n = len(s[:])\n",
    "# print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Doesn't work with network_anl\n",
    "\n",
    "# Import Synthetic Network Data\n",
    "df = pd.read_csv(f'{current_path}/data/5Adj_matrix.csv', header=None)\n",
    "G = np.array(df[df.columns[:]])\n",
    "# # # print(G)\n",
    "df1 = pd.read_csv(f'{current_path}/data/5innate_op.csv', header=None)\n",
    "s = np.array(df1[df1.columns[:]])\n",
    "\n",
    "# print(s)\n",
    "# print(G.shape)\n",
    "\n",
    "# # Set n according to the data\n",
    "n = len(s[:])\n",
    "# # print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Process the Network Data\n",
    "L = scipy.sparse.csgraph.laplacian(G, normed=False)  # Return the Laplacian matrix\n",
    "A = np.linalg.inv(np.identity(n) + L)  # A = (I + L)^(-1)\\n  Stanford paper theory\n",
    "m = num_edges(L, n)                    # call the function to calculate the number of edges\n",
    "# what the twitter graph looks like\n",
    "nxG = nx.from_numpy_array(G)\n",
    "#plt.figure(figsize=(20, 20))\n",
    "# nx.draw(nxG)\n",
    "columnsum_ij = np.sum(A, axis=0)\n",
    "# print(columnsum_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________Degree Centrality_____________________________\n",
      "{4: 0.25, 0: 0.75, 1: 0.75, 2: 0.75, 3: 1.0}\n",
      "                           \n",
      "_______________Closeness Rank_____________________________\n",
      "{4: 0.5714285714285714, 0: 0.8, 1: 0.8, 2: 0.8, 3: 1.0}\n",
      "                           \n",
      "_______________Page Rank_____________________________\n",
      "{4: 0.16965100981173117, 0: 0.4820442559301269, 1: 0.4820442559301269, 2: 0.4820442559301269, 3: 0.5235633112090369}\n",
      "                           \n",
      "{0: array([0.19703]), 3: array([0.21003]), 2: array([0.21351]), 4: array([0.26584]), 1: array([0.45939])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def network_anl2(s,n,G,agent):\n",
    "    print(str(agent)+' opinion: ' + str(s[agent]))\n",
    "    print(str(agent)+' neighbors: '+ str(np.nonzero(G[agent])))\n",
    "\n",
    "    s_aa = s[:, 0]\n",
    "    my_dict = {index: value for index, value in enumerate(s_aa)}\n",
    "    sorting_s = sorted(my_dict.items(), key=lambda x:x[1])\n",
    "    sorted_S = dict(sorting_s)\n",
    "\n",
    "    temp = list(sorted_S.items())\n",
    "    res = [idx for idx, key in enumerate(temp) if key[0]==agent]\n",
    "    # printing result\n",
    "    print(\"Opinion rank of this agent is : \" + str(res))\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"___________________Max Analyze__________________________________________\")\n",
    "    nxG = nx.from_numpy_array(G)\n",
    "    # G = nx.karate_club_graph()\n",
    "    print(\"_______________Degree Centrality___________________\")\n",
    "    deg_centrality = nx.degree_centrality(nxG)\n",
    "    sortedDict = sorted(deg_centrality.items(), key=lambda x:x[1])\n",
    "    converted_dict = dict(sortedDict)\n",
    "    temp1 = list(converted_dict.items())\n",
    "    res1 = [idx for idx, key in enumerate(temp1) if key[0]==agent]\n",
    "    print(\"rank of this agent is : \" + str(res1))\n",
    "    print(converted_dict[agent])\n",
    "\n",
    "    # print(converted_dict)\n",
    "    print(\"                           \")\n",
    "    print(\"_______________Closeness Rank________________________\")\n",
    "    close_centrality = nx.closeness_centrality(nxG)\n",
    "    sortedDict1 = sorted(close_centrality.items(), key=lambda x:x[1])\n",
    "    converted_dict1 = dict(sortedDict1)\n",
    "    temp2 = list(converted_dict1.items())\n",
    "    res2 = [idx for idx, key in enumerate(temp2) if key[0]==agent]\n",
    "    print(\"rank of this agent is : \" + str(res2))\n",
    "    print(converted_dict1[agent])\n",
    "    # print(converted_dict1)\n",
    "    print(\"                           \")\n",
    "    print(\"_______________Page Rank_____________________________\")\n",
    "    pr = nx.eigenvector_centrality(nxG)\n",
    "    sortedDict3 = sorted(pr.items(), key=lambda x:x[1])\n",
    "    converted_dict3 = dict(sortedDict3)\n",
    "    temp3 = list(converted_dict3.items())\n",
    "    res3 = [idx for idx, key in enumerate(temp3) if key[0]==agent]\n",
    "    print(\"rank of this agent is : \" + str(res3))\n",
    "    print(converted_dict3[agent])\n",
    "    # print(converted_dict3)\n",
    "\n",
    "    print(\"                           \")\n",
    "\n",
    "    def gap(op, n):\n",
    "        ones = np.ones((n, 1))\n",
    "        x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "        return abs(x)\n",
    "\n",
    "    gap = gap(s,n)\n",
    "    my_gap = {index: value for index, value in enumerate(gap)}\n",
    "    sorting_gap = sorted(my_gap.items(), key=lambda x:x[1])\n",
    "    sorted_gap = dict(sorting_gap)\n",
    "    #print(sorted_gap)\n",
    "    temp4 = list(sorted_gap.items())\n",
    "    res4 = [idx for idx, key in enumerate(temp4) if key[0]==agent]\n",
    "    print(\"Agent's opinion gap to mean opinion is ranked as: \" + str(res4))\n",
    "\n",
    "\n",
    "    print(\"___________________Max Analyze__________________________________________\")\n",
    "nxG = nx.from_numpy_array(G)\n",
    "# G = nx.karate_club_graph()\n",
    "print(\"_______________Degree Centrality_____________________________\")\n",
    "plt.figure(figsize =(15, 15))\n",
    "deg_centrality = nx.degree_centrality(nxG)\n",
    "sortedDict = sorted(deg_centrality.items(), key=lambda x:x[1])\n",
    "converted_dict = dict(sortedDict)\n",
    "print(converted_dict)\n",
    "print(\"                           \")\n",
    "print(\"_______________Closeness Rank_____________________________\")\n",
    "close_centrality = nx.closeness_centrality(nxG)\n",
    "sortedDict1 = sorted(close_centrality.items(), key=lambda x:x[1])\n",
    "converted_dict1 = dict(sortedDict1)\n",
    "print(converted_dict1)\n",
    "print(\"                           \")\n",
    "print(\"_______________Page Rank_____________________________\")\n",
    "pr = nx.eigenvector_centrality(nxG)\n",
    "sortedDict3 = sorted(pr.items(), key=lambda x:x[1])\n",
    "converted_dict3 = dict(sortedDict3)\n",
    "print(converted_dict3)\n",
    "\n",
    "print(\"                           \")\n",
    "\n",
    "def gap(op, n):\n",
    "    ones = np.ones((n, 1))\n",
    "    x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "    return abs(x)\n",
    "\n",
    "gap = gap(s,n)\n",
    "my_gap = {index: value for index, value in enumerate(gap)}\n",
    "sorting_gap = sorted(my_gap.items(), key=lambda x:x[1])\n",
    "sorted_gap = dict(sorting_gap)\n",
    "print(sorted_gap)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO resolve network_anl (from this file) and network_anl2 (from utils) produce different results\n",
    "# agent = 6\n",
    "# # network_anl(s,n,G,agent)\n",
    "# print('_' * 10)\n",
    "# network_anl2(s,n,G,agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Equilibrium & Polarization \n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\t0.41022891110361115\n",
      "Equi_polarization:\t0.04411213470472514\n",
      "Difference:\t\t-0.366116776398886\n"
     ]
    }
   ],
   "source": [
    "calculate_polarization1(s, n, A, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_(result: GameResult, k: int, experiment: int):\n",
    "    pd.DataFrame(result.payoff_matrix).to_csv(\n",
    "        f'results/Payoff-Matrix-k-{k}-experiment-{experiment}.csv')\n",
    "\n",
    "    # Save the original standard output\n",
    "    original_stdout = sys.stdout\n",
    "\n",
    "    with open(f'results/Result-k-{k}-experiment-{experiment}.txt', \"w\") as f:\n",
    "        # Change the standard output to the file we created.\n",
    "        sys.stdout = f\n",
    "\n",
    "        print('Initial Condition - (agent, opinion, pol)')\n",
    "        # print(f'Innate op {s}')\n",
    "        # print(f'Adjacency matrix {G}')\n",
    "        # print('Selected Nodeset, k_Opinions, Steady-state polarization')\n",
    "        print(f'Max:{result.first_max}')\n",
    "        print(f'Min{result.first_min}')\n",
    "\n",
    "        print('_____________________')\n",
    "        print(f'Max Pol: {result.equi_max} Min Pol: {result.equi_min}')\n",
    "        # MAXimizer's distribution of LAST 100 iteration\n",
    "        print('Max_distribution_last_100')\n",
    "        max_l100_fre = result.max_history_last_100/100\n",
    "        print(max_l100_fre[np.nonzero(max_l100_fre)])\n",
    "        # print for small network\n",
    "        # print(max_history_last_100)\n",
    "        # # Print for Large Network\n",
    "        print(np.nonzero(max_l100_fre))\n",
    "\n",
    "        print('Max_distribution_all')\n",
    "        max_fre = result.max_history/result.game_rounds\n",
    "        print(max_fre[np.nonzero(max_fre)])\n",
    "        print([np.nonzero(max_fre)])\n",
    "\n",
    "        # MINimizer's Strategy in the last 100 round\n",
    "        counter = collections.Counter(result.min_touched_last_100)\n",
    "        # frequency of all min options in order\n",
    "        fla_min_fre = np.array(list(counter.values()))/(100)\n",
    "        print('Min_distribution_last_100')\n",
    "        print(fla_min_fre)\n",
    "        print(counter)\n",
    "        # print(min_touched_last_100)\n",
    "\n",
    "        # a dictionary include {'min_option': count of this choice}\n",
    "        counter_1 = collections.Counter(result.min_touched_all)\n",
    "        # frequency of all min options in order\n",
    "        fla_min_fre_1 = np.array(list(counter_1.values()))/result.game_rounds\n",
    "        print('Min_distribution_all')\n",
    "        print(fla_min_fre_1)\n",
    "        print(counter_1)\n",
    "        np.set_printoptions(precision=3)\n",
    "\n",
    "        # a dictionary include {'min_option': count of this choice}\n",
    "        counter_a = collections.Counter(result.min_history)\n",
    "        print(counter_a)\n",
    "\n",
    "        print(f'min_recent_{memory}_touched')  # then stop at Game 202\n",
    "        print(result.min_touched)\n",
    "        print(f'max_recent_{memory}_touched')\n",
    "        print(result.max_touched)\n",
    "\n",
    "        # Reset the standard output to its original value\n",
    "        sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes, opinions\n",
      "[2, 4] (1, 1)\n",
      "Nodes, opinions\n",
      "[0, 3] (0.5, 0.5)\n",
      "min_history [((0, 3), (0.5, 0.5))]\n",
      "--------------------\n",
      "Game 1\n",
      "----------\n",
      "min_history [((0, 3), (0.5, 0.5))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0]\n",
      "fla_min_fre [1.]\n",
      "Max_por 0.09181344162282333\n",
      "Max_por 0.10085992937712412\n",
      "Max_por 0.1244223977280837\n",
      "all_por [0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.00916 0.0416\n",
      " 0.0416  0.09181 0.      0.      0.      0.      0.03724 0.08109 0.10086\n",
      " 0.02249 0.      0.      0.      0.      0.05643 0.07023 0.12442 0.016\n",
      " 0.      0.      0.      0.     ]\n",
      "column - best action: 34\n",
      "Maximizer found its target 2 agent: [2, 4] op: (1, 0)\n",
      "fre_max at spot 0.5\n",
      "M\n",
      "[[-0.02392 -0.02392  0.17608 -0.00431 -0.12392]]\n",
      "M\n",
      "[[-0.14059 -0.14059  0.05941 -0.03764  0.25941]]\n",
      "champion:  [0, 1] [0.39593 0.39593] 0.06153260863178252\n",
      "M\n",
      "[[-0.03145  0.14597  0.16855 -0.0629  -0.22016]]\n",
      "M\n",
      "[[-0.14812  0.0293   0.05188 -0.09624  0.16317]]\n",
      "M\n",
      "[[ 0.02562 -0.02051  0.17949 -0.04102 -0.14358]]\n",
      "M\n",
      "[[-0.09104 -0.13718  0.06282 -0.07436  0.23975]]\n",
      "champion:  [1, 3] [0.46358 0.54856] 0.06045968913443649\n",
      "Minimizer finds its target agents: [1, 3]\n",
      "fla_min_fre: [0.5 0.5]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.06045968913443649 and Equi_Max = 0.1244223977280837\n",
      "--------------------\n",
      "Game 2\n",
      "----------\n",
      "min_history [((0, 3), (0.5, 0.5)), ((1, 3), (0.46357728831578965, 0.5485636155789475))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.5 0.5]\n",
      "Max_por 0.09851149537862852\n",
      "all_por [0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.03695 0.09282 0.09851 0.03216\n",
      " 0.      0.      0.      0.     ]\n",
      "column - best action: 34\n",
      "Maximizer found its target 2 agent: [2, 4] op: (1, 0)\n",
      "fre_max at spot 0.6666666666666666\n",
      "M\n",
      "[[-0.02392 -0.02392  0.17608 -0.00431 -0.12392]]\n",
      "M\n",
      "[[-0.14059 -0.14059  0.05941 -0.03764  0.25941]]\n",
      "champion:  [0, 1] [0.21965 0.21965] 0.06336418338187749\n",
      "M\n",
      "[[-0.03145  0.14597  0.16855 -0.0629  -0.22016]]\n",
      "M\n",
      "[[-0.14812  0.0293   0.05188 -0.09624  0.16317]]\n",
      "M\n",
      "[[ 0.02562 -0.02051  0.17949 -0.04102 -0.14358]]\n",
      "M\n",
      "[[-0.09104 -0.13718  0.06282 -0.07436  0.23975]]\n",
      "champion:  [1, 3] [0.24428 0.45207] 0.06218372012728898\n",
      "Minimizer finds its target agents: [1, 3]\n",
      "fla_min_fre: [0.33333 0.33333 0.33333]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.06218372012728898 and Equi_Max = 0.09851149537862852\n",
      "--------------------\n",
      "Game 3\n",
      "----------\n",
      "min_history [((0, 3), (0.5, 0.5)), ((1, 3), (0.46357728831578965, 0.5485636155789475)), ((1, 3), (0.2442790427017544, 0.452072387508772))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.33333 0.33333 0.33333]\n",
      "Max_por 0.10670865261455842\n",
      "all_por [0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.02752 0.10671 0.086   0.04297\n",
      " 0.      0.      0.      0.     ]\n",
      "column - best action: 33\n",
      "Maximizer found its target 2 agent: [2, 4] op: (0, 1)\n",
      "fre_max at spot 0.25\n",
      "M\n",
      "[[-0.12392 -0.12392 -0.12392 -0.00431  0.37608]]\n",
      "M\n",
      "[[-0.02392 -0.02392  0.17608 -0.00431 -0.12392]]\n",
      "M\n",
      "[[-0.14059 -0.14059  0.05941 -0.03764  0.25941]]\n",
      "champion:  [0, 1] [0.43439 0.43439] 0.07678028138007312\n",
      "M\n",
      "[[-0.13145  0.04597 -0.13145 -0.0629   0.27984]]\n",
      "M\n",
      "[[-0.03145  0.14597  0.16855 -0.0629  -0.22016]]\n",
      "M\n",
      "[[-0.14812  0.0293   0.05188 -0.09624  0.16317]]\n",
      "M\n",
      "[[-0.07438 -0.12051 -0.12051 -0.04102  0.35642]]\n",
      "M\n",
      "[[ 0.02562 -0.02051  0.17949 -0.04102 -0.14358]]\n",
      "M\n",
      "[[-0.09104 -0.13718  0.06282 -0.07436  0.23975]]\n",
      "Minimizer finds its target agents: [0, 1]\n",
      "fla_min_fre: [0.25 0.25 0.25 0.25]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.07678028138007312 and Equi_Max = 0.10670865261455842\n",
      "--------------------\n",
      "Game 4\n",
      "----------\n",
      "min_history [((0, 3), (0.5, 0.5)), ((1, 3), (0.46357728831578965, 0.5485636155789475)), ((1, 3), (0.2442790427017544, 0.452072387508772)), ((0, 1), (0.4343921844230768, 0.43439218442307703))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.25 0.25 0.25 0.25]\n",
      "Max_por 0.105292016495467\n",
      "all_por [0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.02583 0.10529 0.08517 0.04241\n",
      " 0.      0.      0.      0.     ]\n",
      "column - best action: 33\n",
      "Maximizer found its target 2 agent: [2, 4] op: (0, 1)\n",
      "fre_max at spot 0.4\n",
      "M\n",
      "[[-0.12392 -0.12392 -0.12392 -0.00431  0.37608]]\n",
      "M\n",
      "[[-0.02392 -0.02392  0.17608 -0.00431 -0.12392]]\n",
      "M\n",
      "[[-0.14059 -0.14059  0.05941 -0.03764  0.25941]]\n",
      "champion:  [0, 1] [0.56324 0.56324] 0.07971427066332097\n",
      "M\n",
      "[[-0.13145  0.04597 -0.13145 -0.0629   0.27984]]\n",
      "M\n",
      "[[-0.03145  0.14597  0.16855 -0.0629  -0.22016]]\n",
      "M\n",
      "[[-0.14812  0.0293   0.05188 -0.09624  0.16317]]\n",
      "M\n",
      "[[-0.07438 -0.12051 -0.12051 -0.04102  0.35642]]\n",
      "M\n",
      "[[ 0.02562 -0.02051  0.17949 -0.04102 -0.14358]]\n",
      "M\n",
      "[[-0.09104 -0.13718  0.06282 -0.07436  0.23975]]\n",
      "Minimizer finds its target agents: [0, 1]\n",
      "fla_min_fre: [0.2 0.2 0.2 0.2 0.2]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.07971427066332097 and Equi_Max = 0.105292016495467\n",
      "--------------------\n",
      "Game 5\n",
      "----------\n",
      "min_history [((0, 3), (0.5, 0.5)), ((1, 3), (0.46357728831578965, 0.5485636155789475)), ((1, 3), (0.2442790427017544, 0.452072387508772)), ((0, 1), (0.4343921844230768, 0.43439218442307703)), ((0, 1), (0.5632383382692306, 0.563238338269231))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.2 0.2 0.2 0.2 0.2]\n",
      "Max_por 0.10098895790093523\n",
      "all_por [0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.02766 0.10099 0.08843 0.03954\n",
      " 0.      0.      0.      0.     ]\n",
      "column - best action: 33\n",
      "Maximizer found its target 2 agent: [2, 4] op: (0, 1)\n",
      "fre_max at spot 0.5\n",
      "M\n",
      "[[-0.12392 -0.12392 -0.12392 -0.00431  0.37608]]\n",
      "M\n",
      "[[-0.02392 -0.02392  0.17608 -0.00431 -0.12392]]\n",
      "M\n",
      "[[-0.14059 -0.14059  0.05941 -0.03764  0.25941]]\n",
      "champion:  [0, 1] [0.64914 0.64914] 0.07953873455395724\n",
      "M\n",
      "[[-0.13145  0.04597 -0.13145 -0.0629   0.27984]]\n",
      "M\n",
      "[[-0.03145  0.14597  0.16855 -0.0629  -0.22016]]\n",
      "M\n",
      "[[-0.14812  0.0293   0.05188 -0.09624  0.16317]]\n",
      "champion:  [0, 3] [0.65818 0.67799] 0.07764083208444837\n",
      "M\n",
      "[[-0.07438 -0.12051 -0.12051 -0.04102  0.35642]]\n",
      "M\n",
      "[[ 0.02562 -0.02051  0.17949 -0.04102 -0.14358]]\n",
      "M\n",
      "[[-0.09104 -0.13718  0.06282 -0.07436  0.23975]]\n",
      "Minimizer finds its target agents: [0, 3]\n",
      "fla_min_fre: [0.16667 0.16667 0.16667 0.16667 0.16667 0.16667]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.07764083208444837 and Equi_Max = 0.10098895790093523\n",
      "MAX_last_100,  all\n",
      "[0.03 0.02] [0.6 0.4 0.2]\n",
      "[33 34] [33 34 35]\n",
      "Max Nodes: [2, 4] | Opinion: (0, 1)\n",
      "Max Nodes: [2, 4] | Opinion: (1, 0)\n",
      "MIN_last_100,  all\n",
      "[0.02 0.02 0.01] [0.4 0.4 0.4]\n",
      "Counter({(1, 3): 2, (0, 1): 2, (0, 3): 1}) Counter({(0, 3): 2, (1, 3): 2, (0, 1): 2})\n",
      "Max Pol: 0.10098895790093523 Min Pol: 0.07764083208444837\n"
     ]
    }
   ],
   "source": [
    "game = Game(s, A, L, k)\n",
    "result = game.run(game_rounds, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 1\n",
    "save_(result, k, experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
