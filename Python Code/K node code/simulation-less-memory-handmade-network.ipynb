{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real data network with small synthetic network\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n",
    "import scipy.optimize\n",
    "import os\n",
    "import os.path\n",
    "from game import Game, GameResult, exportGameResult\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Fixed initial condition + memeory = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = 'manual'\n",
    "# network_name = 'reddit'\n",
    "# network_name = 'karate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = import_network(network_name)\n",
    "G, s, n = network.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the Network Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Laplacian matrix\n",
    "L = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "# A = (I + L)^(-1) - Stanford paper theory\n",
    "A = np.linalg.inv(np.identity(n) + L)\n",
    "nxG = nx.from_numpy_array(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Equilibrium & Polarization \n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\t0.41022891110361115\n",
      "Equi_polarization:\t0.04411213470472514\n",
      "Difference:\t\t-0.366116776398886\n"
     ]
    }
   ],
   "source": [
    "# Calculate network polarization\n",
    "calculate_polarization(s, n, A, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Parameters\n",
    "game_rounds = 3\n",
    "memory = 10\n",
    "k = 2\n",
    "experiment = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes, opinions\n",
      "[3, 4] (1, 1)\n",
      "Nodes, opinions\n",
      "[0, 1] (0.5, 0.5)\n",
      "min_history [((0, 1), (0.5, 0.5))]\n",
      "--------------------\n",
      "Game 1\n",
      "----------\n",
      "min_history [((0, 1), (0.5, 0.5))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1]\n",
      "fla_min_fre [1.]\n",
      "Max_por 0.06093770082282333\n",
      "Max_por 0.09177122902822495\n",
      "Max_por 0.09177122902822495\n",
      "all_por [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.0140544  0.02336702\n",
      " 0.0609377  0.05913921 0.02752321 0.09177123 0.09177123 0.03379702\n",
      " 0.06038921 0.04868523 0.06437572 0.04156062]\n",
      "column - best action: 33\n",
      "Maximizer found its target 2 agent: [2, 4] op: (0, 1)\n",
      "fre_max at spot 0.5\n",
      "M\n",
      "[[-0.12392262 -0.12392262 -0.12392262 -0.0043095   0.37607738]]\n",
      "M\n",
      "[[-0.16068691 -0.16068691 -0.03244402  0.07862619  0.27519165]]\n",
      "champion:  [0, 1] [1 1] 0.03004517245100789\n",
      "Minimizer finds its target agents: [0, 1]\n",
      "fla_min_fre: [0.5 0.5]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.03004517245100789 and Equi_Max = 0.09177122902822495\n",
      "--------------------\n",
      "Game 2\n",
      "----------\n",
      "min_history [((0, 1), (0.5, 0.5)), ((0, 1), (1, 1))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1]\n",
      "fla_min_fre [0.5 0.5]\n",
      "Max_por 0.10326824100060109\n",
      "Max_por 0.14278412503933605\n",
      "Max_por 0.14278412503933605\n",
      "all_por [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.04749605 0.05125312\n",
      " 0.10326824 0.0959142  0.06964722 0.07278413 0.14278413 0.02369881\n",
      " 0.10942223 0.03660713 0.10785318 0.02392697]\n",
      "column - best action: 34\n",
      "Maximizer found its target 2 agent: [2, 4] op: (1, 0)\n",
      "fre_max at spot 0.3333333333333333\n",
      "M\n",
      "[[-0.12392262 -0.12392262 -0.12392262 -0.0043095   0.37607738]]\n",
      "M\n",
      "[[-0.02392262 -0.02392262  0.17607738 -0.0043095  -0.12392262]]\n",
      "M\n",
      "[[-0.16068691 -0.16068691 -0.03244402  0.07862619  0.27519165]]\n",
      "champion:  [0, 1] [0.66696841 0.66696841] 0.07181285055837117\n",
      "Minimizer finds its target agents: [0, 1]\n",
      "fla_min_fre: [0.33333333 0.33333333 0.33333333]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.07181285055837117 and Equi_Max = 0.14278412503933605\n",
      "--------------------\n",
      "Game 3\n",
      "----------\n",
      "min_history [((0, 1), (0.5, 0.5)), ((0, 1), (1, 1)), ((0, 1), (0.666968408128205, 0.6669684081282052))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 1]\n",
      "fla_min_fre [0.33333333 0.33333333 0.33333333]\n",
      "Max_por 0.09644002685880759\n",
      "Max_por 0.1349946977961572\n",
      "Max_por 0.1349946977961572\n",
      "all_por [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.04165191 0.04602403\n",
      " 0.09644003 0.08970103 0.06284187 0.07274431 0.1349947  0.02267492\n",
      " 0.10185199 0.03580243 0.10089799 0.02373732]\n",
      "column - best action: 34\n",
      "Maximizer found its target 2 agent: [2, 4] op: (1, 0)\n",
      "fre_max at spot 0.5\n",
      "M\n",
      "[[-0.12392262 -0.12392262 -0.12392262 -0.0043095   0.37607738]]\n",
      "M\n",
      "[[-0.02392262 -0.02392262  0.17607738 -0.0043095  -0.12392262]]\n",
      "M\n",
      "[[-0.16068691 -0.16068691 -0.03244402  0.07862619  0.27519165]]\n",
      "champion:  [0, 1] [0.46699743 0.46699743] 0.07909271784164598\n",
      "Minimizer finds its target agents: [0, 1]\n",
      "fla_min_fre: [0.25 0.25 0.25 0.25]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.07909271784164598 and Equi_Max = 0.1349946977961572\n",
      "MAX_last_100,  all\n",
      "[0.01 0.02] [0.33333333 0.66666667 0.33333333]\n",
      "[33 34] [33 34 39]\n",
      "Max Nodes: [2, 4] | Opinion: (0, 1)\n",
      "Max Nodes: [2, 4] | Opinion: (1, 0)\n",
      "MIN_last_100,  all\n",
      "[0.03] [1.33333333]\n",
      "Counter({(0, 1): 3}) Counter({(0, 1): 4})\n",
      "Max Pol: 0.1349946977961572 Min Pol: 0.07909271784164598\n"
     ]
    }
   ],
   "source": [
    "# configure the game\n",
    "game = Game(s, A, L, k)\n",
    "# run the game\n",
    "result = game.run(game_rounds, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportGameResult(game, result, k, memory, experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
