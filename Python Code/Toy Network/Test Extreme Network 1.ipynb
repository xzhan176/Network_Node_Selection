{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real datanetwork with small sythetic network \n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "#%run pure_strategy_selection.ipynb  #include simple selection algorithm\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n",
    "from itertools import count\n",
    "import os.path\n",
    "\n",
    "save_path = 'C:/Users/xzhan/OneDrive/08102022 - updated Mixed Opinion/Karate Result'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Fixed initial condition + memeory = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers the opinion vector around 0\\n\",\n",
    "def mean_center(op, n):\n",
    "    ones = np.ones((n, 1))\n",
    "    x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "    return x\n",
    "    \n",
    "# compute number of edges, m\\n\n",
    "def num_edges(L, n):\n",
    "    m = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i > j and L[i,j] < 0:\n",
    "                m += 1            \n",
    "    return m\n",
    "\n",
    "# maximizing polarization only: \\\\bar{z}^T \\\\bar{z}   \n",
    "def obj_polarization(A, L, op, n):\n",
    "    op_mean = mean_center(op, n)\n",
    "    z_mean = np.dot(A, op_mean) \n",
    "    return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "# def obj_polarization_1(A, L, op, n):  #z_mean is the same as s_mean - according to Stanford paper theory\n",
    "#     z = np.dot(A, op) \n",
    "#     z_mean = mean_center(z, n)\n",
    "#     return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "# Calculate innate polarization\n",
    "def obj_innate_polarization(s, n):  \n",
    "#     np.set_printoptions(precision=5)\n",
    "    op_mean = mean_center(s, n)\n",
    "    return np.dot(np.transpose(op_mean), op_mean)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the network\n",
    "n = 3\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Network\n",
    "### 1. Make Random Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. ]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "weight1\n",
      "[[0.  ]\n",
      " [0.02]\n",
      " [0.02]]\n",
      "G for agents completed!\n",
      "[[0.   0.   0.02]\n",
      " [0.   0.   0.02]\n",
      " [0.02 0.02 0.  ]]\n",
      "[[9.8076e-01 3.6996e-04 1.8868e-02]\n",
      " [3.6996e-04 9.8076e-01 1.8868e-02]\n",
      " [1.8868e-02 1.8868e-02 9.6226e-01]]\n",
      "Column Sum\n",
      "[1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ############################ Make Innate Opinion ################################\n",
    "# ##create two set of weights connected with density 1) inviduals  2) individual & informaton Source\n",
    "c1 = np.sort(np.random.choice(n, n, replace=False)) #assume (1-r) are individuals\n",
    "# print('c1')\n",
    "# print(c1)\n",
    "l1 = len(c1)\n",
    "\n",
    "\n",
    "##################################Creating Adjacency Matrix ########################\n",
    "np.set_printoptions(precision=4)\n",
    "### Prepare for create adjacent matrix\n",
    "p1 = 1 # density within ind.\n",
    "p2 = 0 # density of edges between Info Source and Indivisuals\n",
    "\n",
    "#pre_weights1 = scipy.sparse.random(1, int(0.5*l1*(l1 - 1)), density=p1).A[0] \n",
    "# pre_weights1 = scipy.sparse.rand(1, int(0.5*l1*(l1 - 1)), density=p1, format='coo', dtype=None, random_state=None)\n",
    "pre_weights1  = np.full((3, 1), 1/2, dtype=float)\n",
    "pre_weights1[0] = 0\n",
    "print(pre_weights1)\n",
    "\n",
    "\n",
    "weights1 = pre_weights1 /25\n",
    "\n",
    "\n",
    "print(\"weight1\")\n",
    "print(weights1)\n",
    "weights1.shape\n",
    "\n",
    "\n",
    "# create n x n adjacency matrix with existing init_G\n",
    "G = np.zeros((n, n))\n",
    "    \n",
    "## Assign edges between ind to ind \n",
    "idx = 0\n",
    "for i in c1:\n",
    "    for j in c1:\n",
    "            if i == j:\n",
    "                G[i][j] =0\n",
    "                continue\n",
    "            elif i < j:\n",
    "                G[i][j] = weights1[idx]\n",
    "                idx += 1\n",
    "#                 print(idx)\n",
    "#                 print (G1[i][j])\n",
    "            else:\n",
    "                G[i][j] = G[j][i]\n",
    "print(\"G for agents completed!\")\n",
    "print(G)\n",
    "\n",
    "L = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "A = np.linalg.inv(np.identity(n) + L)\n",
    "m = num_edges(L, n)\n",
    "print(A)\n",
    "columnsum_ij = np.sum(A, axis=0)\n",
    "print('Column Sum')\n",
    "print(columnsum_ij)\n",
    "##what the twitter graph looks like \n",
    "nxG = nx.from_numpy_matrix(G)\n",
    "plt.figure(figsize=(6, 6))\n",
    "nx.draw(nxG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Equilibrium & Polarization  - based on derivation\n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\n",
      "0.5\n",
      "Equi_polarization:\n",
      "0.45389476266008816\n",
      "Difference:\n",
      "-0.04610523733991184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## s =  make_innat_opinions(n, c1)\n",
    "# print('Innate Opinion')\n",
    "# print(s)\n",
    "# print('Equilibrium Opinion')\n",
    "# print(np.dot(A, s))\n",
    "# s = make_innat_opinions(n, c1)\n",
    "# op = s\n",
    "# print(s)\n",
    "n = 3\n",
    "s_0=[1]\n",
    "s_1=[0.5]\n",
    "s_2=[0]\n",
    "s = np.vstack((s_0,s_1,s_2))\n",
    "\n",
    "\n",
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, s, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing players' behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "  \n",
    "    v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "    new_op = random.randint(0, 1)  # randomly select an opininon between 0 and 1\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play1(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "#     max_opi_option = random.uniform(0, 1)   # options that maximizer have\n",
    "    \n",
    "    v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "#     v = 1\n",
    "    new_op = random.uniform(0, 1)  # randomly select an opininon between 0 and 1\n",
    "    #new_op = 0\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# min_touched =[]\n",
    "# (v1, maxmize_op, innat_equi_por, max_por) = choose_max_vertex(s, n, min_touched)\n",
    "# print(v1, maxmize_op, innat_equi_por, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# (v2, min_opinion, min_pol) = random_play(s,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizer_fir_play(s,n,min_touched):    # maxmizer first-time play, greedy algorithm\n",
    "    op = copy.copy(s)\n",
    "\n",
    "    print('Maximizer Play')\n",
    "\n",
    "    max_champion = choose_max_vertex(op, n, min_touched) # The best choice among all opinions and vertexs, function is in \"pure_strategy_selection.ipynb\"\n",
    "    (v1, max_opinion, innate_obj, max_pol) = max_champion # find agent v1, and max_opinion that can maxmize the equi_polarization(max_pol)\n",
    "\n",
    "    if v1 == None:   # if maximizer cannot find one\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "#         print('v1', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(max_champion)\n",
    "\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        ##### change the agent's opinion with best action(agent v1, max_op)\n",
    "        op[v1,0] = max_opinion\n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_touched = []\n",
    "# min_touched = []\n",
    "# (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "# print(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### minimizer first-time play, greedy algorithm\n",
    "def minimizer_fir_play(s,n,max_touched): \n",
    "    \n",
    "    op = copy.copy(s)\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = choose_min_vertex(op, n, max_touched)\n",
    "    (v2, min_opinion, innat_equi_por, min_pol) = min_champion\n",
    "    \n",
    "   #Store innate_op of the min_selected vertex\n",
    "    old_opinion_min = op[v2,0]\n",
    "    \n",
    "    if v2 == None:\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "\n",
    "        ##### change the agent's opinion\n",
    "        op[v2,0] = min_opinion   #-------------------------------------------------> store minimize strategy\n",
    "\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return (v2,min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_touched = []\n",
    "# min_touched = []\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "# print(v2, min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing above functions\n",
    "# min_touched=[]\n",
    "# max_touched=[]\n",
    "# # Game start from maximizer random play\n",
    "# print('Maximizer random selection')\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# max_touched.append(v1)\n",
    "# # print('v1, max_opinion, max_pol')\n",
    "# # print(v1, max_opinion, max_pol)\n",
    "# # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Game start from minimizer random play \n",
    "# print('Minimizer random selection')\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "# min_touched.append(v2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row are Column are depended on min and max's choice: agent v and opinion \n",
    "def row_index(v2, min_opinion):\n",
    "    row = 11*v2 + min_opinion*10 \n",
    "    return int(row)\n",
    "def column_index(v1,max_opinion):\n",
    "    column = 2*v1 + max_opinion\n",
    "    return int(column)  #the python dataframe index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Strategy Payoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_payoff_row(op1,v2):\n",
    "    payoff_row = np.zeros(2*n)\n",
    "\n",
    "#     print('one opinion changed -min')\n",
    "#     print(op1)\n",
    "    for column in range(2*n):\n",
    "#         print(column)\n",
    "        v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "        max_opinion = column%2\n",
    "#         print(v1, max_opinion)\n",
    "        # update the maximizer's change to the opinion array that has changed by minimizer(op1)\n",
    "        op2 = copy.copy(op1)\n",
    "#         temp = op1[v1]\n",
    "        op2[v1,0] = max_opinion\n",
    "\n",
    "        # calculate the polarization with both max and min's action\n",
    "        payoff_row[column] = obj_polarization(A, L, op2, n)\n",
    "#         op1[v1,0] = temp # restore\n",
    "#         print(op2,payoff_row[column])\n",
    "\n",
    "        ############# CAN DELETE \n",
    "#         if column==33:\n",
    "# #         print('max_opinion')\n",
    "# #         print(v1, max_opinion)\n",
    "#             print('_________________________Payoff row start')\n",
    "#             print('two opinion changed -min +  max')\n",
    "#             print(op2)\n",
    "        \n",
    "    # when v1 == v2, the polarization should be negative for max, infinet for min. \n",
    "    # Replace the the column_index of agent v2 with 0 for max\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "    \n",
    "    return payoff_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.4539    0.1572    0.6289    0.     -100.     -100.    ]\n"
     ]
    }
   ],
   "source": [
    "# #(1,0) (2,0.3928571428571428)\n",
    "# op1=copy.copy(s)\n",
    "# print(op1)\n",
    "\n",
    "op1 = copy.copy(s)\n",
    "# print(op1)\n",
    "op1[2,0] = 1  #op1 is the opinion array that updated by minimizer\n",
    "# print(op1)\n",
    "payoff_row_1 = make_payoff_row(op1,2)\n",
    "print(payoff_row_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEEDDDDDDD UPDAE\n",
    "\n",
    "# Calculate polarization of minimizer's Mixed Strategy\n",
    "def mixed_min_polarization(s,v2,weight_op,fla_max_fre):\n",
    "\n",
    "    op1 =  copy.copy(s) # make a copy of the innate opinion array \n",
    "    op1[v2,0] = weight_op # then only updated by minimizer's current change\n",
    "#     print('Min update')\n",
    "#     print(v2, weight_op)\n",
    "    # calculate the polarization with both min(did here) and max's action(in make_payoff_row)\n",
    "    payoff_row = make_payoff_row(op1,v2)  # the vector list out 2*n payoffs after min's action combine with 2*n possible max's actions\n",
    "    #print(payoff_row)\n",
    "\n",
    "    # Replace the the column_index of agent v2 with 100 for min\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = 100\n",
    "    payoff_row[j_2] = 100\n",
    "    \n",
    "#     print('Min Payoff Row')\n",
    "#     print(payoff_row)\n",
    "    #calculate fictitious payoff - equi_min  \n",
    "    payoff_cal = payoff_row * fla_max_fre # fla_max_fre recorded the frequency of each maximizer's action, frequency sum = 1\n",
    "                                             # payoff (2*n array) * maximizer_action_frequency (2*n array)\n",
    "# can DELETE - use to check if function works as expected\n",
    "#     if v2 ==6 and v1==16:\n",
    "#         print('Payoff row')\n",
    "#         column = column_index(16,1)\n",
    "#         print(payoff_row[column],column)\n",
    "#         print('fla_max_fre')\n",
    "#         print(np.nonzero(fla_max_fre))\n",
    "#         print(fla_max_fre [np.nonzero(fla_max_fre)])\n",
    "#         print('compare to: '+str(fla_max_fre[column]))\n",
    "    \n",
    "    mixed_pol = np.sum(payoff_cal) # add up all, calculate average/expected payoff\n",
    "\n",
    "\n",
    "#     print('min_mixed_polarization')\n",
    "#     print(mixed_pol)\n",
    "        # Replace the the column_index of agent v2 with 100 for min\n",
    "\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "\n",
    "    return (mixed_pol,payoff_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # op2=op\n",
    "# # op2[0,0]=1\n",
    "# # min_opinion1 = derivate_s(op2,n,1)\n",
    "# # # print(min_opinion1)\n",
    "# # min_opinion2 = derivate_s1(op2,n,1)\n",
    "# # print(min_opinion2)\n",
    "# v2 = 254\n",
    "# min_opinion = 0\n",
    "# (mixed_pol, payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "# print(np.nonzero(fla_max_fre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_s(op,n,v2):\n",
    "               #op - opinion array that updated by maximizer\n",
    "    c = [1/n] * n\n",
    "#     print(c)\n",
    "    sum_term = 0\n",
    "    j = 0\n",
    "\n",
    "    sum_term = np.dot(np.dot((A-c),(A[v2]-c)),op)  # sum up all terms\n",
    "    \n",
    "    term_out = op[v2]*np.dot((A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "    sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "    s_star = -sum_s/np.dot((A[v2]-c),(A[v2]-c))\n",
    "    s_star = s_star[0] #take value out of array\n",
    "    min_opinion =min(max(0,s_star),1)\n",
    "    \n",
    "#     print('Min opinion-should be unique')\n",
    "#     print(min_opinion)\n",
    "    return min_opinion\n",
    "\n",
    "# def derivate_s1(op,n,v2):\n",
    "#                #op - opinion array that updated by maximizer\n",
    "#     c = [1/n] * n\n",
    "# #     print(c)\n",
    "#     sum_term = 0\n",
    "#     j = 0\n",
    "#     for j in range(0,n):\n",
    "#         term = op[j]*np.dot(np.transpose(A[j]-c),(A[v2]-c))\n",
    "# #             print(A[j])\n",
    "# #             print(A[v])\n",
    "#         sum_term = sum_term + term  # sum up all terms\n",
    "    \n",
    "#     term_out = op[v2]*np.dot(np.transpose(A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "#     sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "#     s_star = -sum_s/np.dot(np.transpose(A[v2]-c),(A[v2]-c))\n",
    "#     s_star = s_star[0] #take value out of array\n",
    "#     min_opinion =min(max(0,s_star),1)\n",
    "            \n",
    "#     return min_opinion\n",
    "\n",
    "\n",
    "\n",
    "def min_mixed_opinion(op, n, v2, fla_max_fre):\n",
    "    \n",
    "    weight_op = 0\n",
    "    \n",
    "    # loop for each max_action(in total 2*n) \n",
    "    for column in range(2*n):\n",
    "\n",
    "        if fla_max_fre[column] !=0:\n",
    "            v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "            max_opinion = column%2\n",
    "            \n",
    "##             temp = op[v1,0] \n",
    "          \n",
    "##             op[v1,0]= max_opinion #update innate opinion array with max_action  \n",
    "\n",
    "            min_opinion = derivate_s(op, n, v2)# find min_s_star for each max_action\n",
    "#             print(fla_max_fre[column],min_opinion)\n",
    "            print(min_opinion)\n",
    "            op1 = copy.copy(op)\n",
    "            op1[v2] = min_opinion\n",
    "            min_por = obj_polarization(A, L, op1, n)\n",
    "            #(min_por, row) = mixed_min_polarization(s, v2, min_opinion,fla_max_fre)\n",
    "\n",
    "    \n",
    "            weight_op = weight_op + fla_max_fre[column]*min_opinion # sum up p_i*s_i\n",
    "\n",
    "    print('Weighted opinion')\n",
    "    print(weight_op)\n",
    "    \n",
    "    (mixed_por, payoff_row) = mixed_min_polarization(s, v2, weight_op,fla_max_fre)\n",
    "    \n",
    "    print('Weighted polarization')\n",
    "    print(mixed_por)\n",
    "    \n",
    "    return(weight_op,payoff_row,mixed_por)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out weighted opinion proved that we need to do this step insetead of min_mixed_opinion - we are weighting\n",
    "# different min_opinion here\n",
    "def min_mixed_opinion_1(s, n, v2, fla_max_fre):\n",
    "    \n",
    "    weight_op = 0\n",
    "    \n",
    "    # loop for each max_action(in total 2*n) \n",
    "    for column in range(2*n):\n",
    "\n",
    "        if fla_max_fre[column] !=0:\n",
    "            v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "            max_opinion = column%2\n",
    "            op = copy.copy(s)\n",
    "            op[v1] = max_opinion\n",
    "#             print(op)\n",
    "\n",
    "#             print('Weight')\n",
    "#             print(fla_max_fre[column])\n",
    "            min_opinion = derivate_s(op, n, v2)# find min_s_star for each max_action\n",
    "\n",
    "\n",
    "            \n",
    "            op1 = copy.copy(op)\n",
    "            op1[v2] = min_opinion   #after max action, update min action on opinion array\n",
    "#             print(min_opinion)\n",
    "            min_por = obj_polarization(A, L, op1, n)\n",
    "            t = 0  \n",
    "            weight_op = weight_op + fla_max_fre[column]*min_opinion # sum up p_i*s_i\n",
    "\n",
    "#     print('Weighted opinion')\n",
    "#     print(weight_op)\n",
    "    \n",
    "  \n",
    "    (mixed_por, payoff_row) = mixed_min_polarization(s, v2, weight_op,fla_max_fre)\n",
    "\n",
    "    return(weight_op,payoff_row,mixed_por)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fla_max_fre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9a49f72ba520>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# # print(21,fla_max_fre)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mweight_op_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpayoff_row\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_por\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_mixed_opinion_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfla_max_fre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fla_max_fre' is not defined"
     ]
    }
   ],
   "source": [
    "# op=copy.copy(s)\n",
    "# op[21] = 1\n",
    "# print(op)\n",
    "# # print(21,fla_max_fre)\n",
    "v2 = 6\n",
    "(weight_op_1,payoff_row,min_por) = min_mixed_opinion_1(s, n, v2, fla_max_fre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = obj_polarization(A, L, s, n) #min_por- set a standard to compare with pol after min's action\n",
    "# # maxup_por = min_por # store innate max updated polarization\n",
    "# print(a)\n",
    "# print(s[253])\n",
    "# op = copy.copy(s)\n",
    "# op[253] = 0\n",
    "# b = obj_polarization(A, L, op, n)\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizer search: Go through each agent \n",
    "\n",
    "def mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre, min_touched_all):\n",
    "    # current polarization that changed by maximizer, \"innate\" objective that min start with\n",
    "    op = copy.copy(s)\n",
    "    op[v1,0] = max_opinion\n",
    "#     print('Check if op has been updated by Maximizer')\n",
    "#     print(op)\n",
    "    min_por = obj_polarization(A, L, op, n) #min_por- set a standard to compare with pol after min's action\n",
    "    maxup_por = min_por # store innate max updated polarization\n",
    "#     print('check maxup por')\n",
    "#     print(maxup_por)\n",
    "#     payoffs = []    # create an empty list to store all polarizations   \n",
    "    champion = (None, None, 0, None)  # assume the best action is champion\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in max_touched]  # for the vertice that Maximizer has not touched\n",
    "    \n",
    "    for v2 in C1:  \n",
    "\n",
    "#         print('_________________________________')\n",
    "#         print('Min start with agent '+ str(v2) )\n",
    "        (changed_opinion, payoff_row, por) =  min_mixed_opinion_1(s, n, v2, fla_max_fre) # find the best new_op option \n",
    "        if v2 in min_touched_all:\n",
    "            print('____')\n",
    "            print('Min start with agent '+ str(v2) )\n",
    "            print('Weighted polarization')\n",
    "            print(str(por)+'...')\n",
    "            \n",
    "#         print('changed opinion, por, Maxup_por')\n",
    "#         print(changed_opinion, por, maxup_por)\n",
    "\n",
    "        if por < min_por:  # if the recent polarization is smaller than the minimum polarization in the history\n",
    "            min_por = por\n",
    "                                 # update the recent option as champion\n",
    "            champion = (v2, changed_opinion, payoff_row, min_por)  \n",
    "#         else:\n",
    "#             print('Innate polarization is smaller than Min action')\n",
    "\n",
    "    return (champion)  # find the best minimizer's action after going through every new_op option of every agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-4c54050ff24d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmax_opinion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mchampion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixed_choose_min_vertex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_opinion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_touched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfla_max_fre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_touched_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# print(champion)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-31de1799b4cb>\u001b[0m in \u001b[0;36mmixed_choose_min_vertex\u001b[1;34m(s, n, v1, max_opinion, max_touched, fla_max_fre, min_touched_all)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# current polarization that changed by maximizer, \"innate\" objective that min start with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_opinion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#     print('Check if op has been updated by Maximizer')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     print(op)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 16 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "# print('v1,max_opinion')\n",
    "# print(v1,max_opinion)\n",
    "min_touched_all = [6, 29]\n",
    "v1 = 16\n",
    "max_opinion = 1\n",
    "champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre, min_touched_all)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by maximizer, fla_max_fre includes max's hisotry, so minimizer react to the innate op after that\n",
    "def mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre, min_touched_all): \n",
    "\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre, min_touched_all)\n",
    "    (v2, min_opinion, payoff_row, min_pol) = min_champion\n",
    "    \n",
    "    if v2 == None:    # if minimizer cannot find a action to minimize polarization after maximizer's action\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "#         print('v2', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(v2, min_opinion, innat_equi_por, min_pol)\n",
    "\n",
    "        # Store innate_op of the min_selected vertex\n",
    "        old_opinion_min = op[v2,0]\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "        print('fla_max_fre')\n",
    "        print(np.nonzero(fla_max_fre))\n",
    "        print(fla_max_fre [np.nonzero(fla_max_fre)])\n",
    "\n",
    "\n",
    "#         print(\"Payoff row\")\n",
    "#         print(payoff_row)\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "    return (v2, payoff_row, min_opinion, min_pol)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-b0ec8aebf14d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_touched\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayoff_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_opinion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolarization\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixed_min_play\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_opinion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_touched\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfla_max_fre\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mmin_touched_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# print('v2, payoff_row, min_opinion, polarization')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# print(v2, payoff_row, min_opinion, polarization)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-a1e2f5c5b1b9>\u001b[0m in \u001b[0;36mmixed_min_play\u001b[1;34m(s, v1, max_opinion, n, max_touched, fla_max_fre, min_touched_all)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     print(op)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmin_champion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixed_choose_min_vertex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_opinion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_touched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfla_max_fre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_touched_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_opinion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayoff_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_pol\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_champion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-31de1799b4cb>\u001b[0m in \u001b[0;36mmixed_choose_min_vertex\u001b[1;34m(s, n, v1, max_opinion, max_touched, fla_max_fre, min_touched_all)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# current polarization that changed by maximizer, \"innate\" objective that min start with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_opinion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#     print('Check if op has been updated by Maximizer')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     print(op)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 16 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "print(max_touched)\n",
    "(v2, payoff_row, min_opinion, polarization) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre,  min_touched_all)\n",
    "# print('v2, payoff_row, min_opinion, polarization')\n",
    "# print(v2, payoff_row, min_opinion, polarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by minimizer, fla_min_fre includes min's hisotry, so maxmizer react to the innate op after that\n",
    "def mixed_max_polarization(payoff_matrix,v1,max_opinion,fla_min_fre):\n",
    "\n",
    "    # create payoff matrix for maxmizer\n",
    "    column = int(column_index(v1,max_opinion))\n",
    "#     print(payoff_matrix)\n",
    "#     print(\"column\"+str(column))\n",
    "    payoff_vector = payoff_matrix[:,column]\n",
    "    \n",
    "#     print('payoff vector')\n",
    "#     print(payoff_vector)\n",
    "\n",
    "    #calculate fictitious payoff - equi_max   \n",
    "    payoff_cal = payoff_vector * fla_min_fre #payoff * frequency\n",
    "    \n",
    "#     print('max_payoff_calculation')\n",
    "#     print(payoff_cal)\n",
    "    mixed_pol = np.sum(payoff_cal) # add up\n",
    "#     print(\"Max_mixed_polarization\")\n",
    "#     print(mixed_pol)\n",
    "\n",
    "    return mixed_pol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_pol = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines if value of opinion at v should be set to 0 or 1 to maximize equilibrium polarization \n",
    "def max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre):\n",
    "    \n",
    "    por_arr = np.zeros(2)  # create a two_element array to store polarization value of each option\n",
    "\n",
    "\n",
    "    max_opi_option = [0, 1.0]   # Maximizer has two options to change agent v1's opinion\n",
    "    \n",
    "    # objective if set opinion to 0, 1.0\n",
    "    j = 0\n",
    "    for new_op in max_opi_option:\n",
    "#         print('change op to '+ str(i/10))\n",
    "        max_opinion = new_op\n",
    "\n",
    "        por_arr[j] = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)\n",
    "    \n",
    "        j = j + 1   # index increase 1, put the polarization in array\n",
    "\n",
    "#     print('Polarization Options')\n",
    "#     print(por_arr)\n",
    "    \n",
    "    maxmize_op = np.argmax(por_arr)  # the index of maximum polarization = max_opinion --[0,1]\n",
    "    max_por = np.max(por_arr)        # find the maximum polarization in the record\n",
    " \n",
    "#     print('new_op', 'innat_equi_por', 'max_por')\n",
    "#     print(maxmize_op, innat_equi_por, max_por)\n",
    "\n",
    "    return (maxmize_op, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fla_min_fre = [0, 0, 0, 0, 0.65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.35, 0, 0, 0]\n",
    "# v1 = 2\n",
    "# champion = max_mixed_opinion(payoff_matrix, n, v1, v2, fla_min_fre)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which agent maximizer should select to maximizer the equilibrium polarization\n",
    "def mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre):\n",
    "#     print('Check if op has been updated by minimizer')\n",
    "#     print(op)\n",
    "    max_por = obj_polarization(A, L, op, n)  # use \"innate\"(after min action) polarization as a comparable standard to find max_por\n",
    "    minup_por = max_por # store innate min_update polarization\n",
    "#     print('check minup por')\n",
    "#     print(minup_por)\n",
    "    champion = (None, None, max_por)  # assume champion is the best action\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in min_touched]  # for the vertice that Minimizer has not touched\n",
    "    for v1 in C1:  \n",
    "#             print('Maximizer start from agent'+str(v1))\n",
    "            (changed_opinion, por) = max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre)\n",
    "#             print('changed_opinion, por, minup_por')\n",
    "#             print(changed_opinion, por,minup_por)\n",
    "            \n",
    "            if por > max_por: # if the polarization of most recent action > maximum polarization of previous actions\n",
    "                max_por = por\n",
    "                champion = (v1, changed_opinion,max_por)   # save the this action as champion    \n",
    "#             else:\n",
    "#                 print('Innate polarization is bigger than max action')\n",
    " \n",
    "    return (champion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.1572    0.4539    0.6289    0.5933 -100.     -100.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)\n",
    "champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # min_touched = []\n",
    "# # payoff_matrix = np.empty((0, 2*n), float)\n",
    "# # fla_min_fre = np.empty((0,n))\n",
    "# # champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)\n",
    "# # print(champion)\n",
    "# print(c1)\n",
    "# vertices = np.where(c1)\n",
    "# print(vertices)\n",
    "# por=0\n",
    "# for i in c1:\n",
    "#     print(i)\n",
    "#     max_por = 0.75\n",
    "#     if por > max_por:\n",
    "#         max_por = por\n",
    "#         print('yes')\n",
    "#     else:\n",
    "#         print('por<max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre): \n",
    "    op = copy.copy(s)   # op is a copy of innate opinion\n",
    "    \n",
    "    #update innat opinion \n",
    "    op[v2,0] = min_opinion  # Op has been updated by minimizer, so maximizer react to the innate op after that\n",
    "    \n",
    "\n",
    "    max_champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre) # The best choice among all opinions and vertexs\n",
    "    (v1, max_opinion, max_pol) = max_champion\n",
    "\n",
    "    if v1 == None:\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        \n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Testing function -- NO NEED TO RUN\n",
    "# min_touched = []\n",
    "# v2 = 0\n",
    "# min_opinion = 0\n",
    "# b = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "# print('v1,max_opinion,max_pol')\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Player's Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\n",
      "0.5\n",
      "Equi_polarization:\n",
      "0.45389476266008816\n",
      "Difference:\n",
      "-0.04610523733991184\n"
     ]
    }
   ],
   "source": [
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, op, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = 50\n",
    "\n",
    "\n",
    "# with open('Network_'+str(Network)+'.txt', \"a\") as fi:\n",
    "#     print('Innate Opinion', file=fi)\n",
    "#     print(s, file=fi)\n",
    "#     print('Adjacency Matrix', file=fi)\n",
    "#     print(G,file=fi)\n",
    "\n",
    "# Game Preparation\n",
    "def push(obj, element):\n",
    "    if len(obj) >= memory:\n",
    "        obj.pop(0)\n",
    "        print('pop')\n",
    "    obj.append(element)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Parameters\n",
    "Game_rounds =201 # Rounds + 1- use for printing data\n",
    "memory = 1\n",
    "def all_fre_limited_touch(s, n):\n",
    "    # Preparation for the game\n",
    "    op = copy.copy(s)\n",
    "    payoff_matrix = np.empty((0, 2*n), float)\n",
    "    max_history = np.zeros([n, 2])  # n*2 matrix, agent i & opinion options\n",
    "    min_history = []  # append a list of (agent i, min_opinion), min_opinion can be any value\n",
    "#     print(type(min_history))\n",
    "\n",
    "    max_history_last_100 = np.zeros([n, 2]) \n",
    "    min_history_last_100= []\n",
    "\n",
    "    max_touched = []\n",
    "    min_touched = []\n",
    "    min_touched_all = []\n",
    "    min_touched_last_100 =[]\n",
    "    print('min_touched')\n",
    "    print(min_touched)\n",
    "    \n",
    "    \n",
    "    # Game start from maximizer random play\n",
    "    print('Maximizer first selection')\n",
    "    (v1, max_opinion, max_pol) = random_play(op,n)   # Maximizer does random action \n",
    "    #(v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "    #(v1, max_opinion, max_pol) = (1, 1, 0.14833274000237331)\n",
    "    First_max = (v1, max_opinion, max_pol) \n",
    "\n",
    "\n",
    "#     (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    # Maximizer start with greedy play\n",
    "    # (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)   # Maximizer choose action greedily\n",
    "    max_touched.append(v1)    # save Maximizer's action history\n",
    "\n",
    "    # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "    max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "    # print('max_history')\n",
    "    # print(max_history)\n",
    "    print('history at spot')\n",
    "    print(max_history[v1,int(max_opinion)])\n",
    "\n",
    "    max_frequency = max_history/1  # its frequency, only played  1 time so far, divided by 1 \n",
    "    # print('fre_max at spot')\n",
    "    # print(max_frequency[v1,int(max_opinion)])\n",
    "\n",
    "    fla_max_fre = max_frequency.flatten()   # flatten the n*2 matrix to a 2n*1 matrix\n",
    "                                            # so we can multiply the freuency (2n*1)with payoff array (1*2n) \n",
    "                                            # to get average payoff of fictitious play\n",
    "    print('fre_max at spot')\n",
    "    print(fla_max_fre)\n",
    "\n",
    "    column = int(column_index(v1,max_opinion))    # the frequency of maximizer's most recent action (v1,max_opinion)\n",
    "\n",
    "    print(fla_max_fre[column])\n",
    "\n",
    "    # print(np.shape(fla_max_fre.shape))\n",
    "\n",
    "\n",
    "    # if game start from minimizer random play - make sure two random play are not same agent!!!\n",
    "    print('Minimizer first selection')\n",
    "    (v2, min_opinion, min_pol) = random_play(op,n) \n",
    "    #(v2, min_opinion, min_pol) = minimizer_fir_play(s,n,min_touched)\n",
    "    \n",
    "    #(v2, min_opinion, min_pol) = (2, 0.5, 0.5933309600094931)\n",
    "    First_min = (v2, min_opinion, min_pol)\n",
    "\n",
    "    if v1==v2:   # if Max and Min randomly selected the same agent, then we need to restart - cannot choose same agent\n",
    "        sys.exit()\n",
    "\n",
    "    # Minimizer start with greedy play\n",
    "    # (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    min_touched.append(v2)\n",
    "   \n",
    "\n",
    "    # store minimizer play history\n",
    "    min_history.append((v2,min_opinion))\n",
    "    print('min_history')\n",
    "    print(min_history)\n",
    "\n",
    "\n",
    "    counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/1 #return only frequency of all min options in order\n",
    "#     print('fla_min_fre')\n",
    "#     print(fla_min_fre)\n",
    "\n",
    "\n",
    "    (a,payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "    payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "#     print('Payoff Matrix')\n",
    "#     print(payoff_matrix)\n",
    "    print('fla_min_fre at the spot')\n",
    "    min_counter = dict(counter)\n",
    "    print(min_counter) \n",
    "    print(min_counter[(v2,min_opinion)]) \n",
    "#     print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "\n",
    "    equi_min = min_pol\n",
    "    equi_max = max_pol\n",
    "    # print(equi_min)\n",
    "    # print(equi_max)\n",
    "\n",
    "\n",
    "\n",
    "    Flag = 0\n",
    "\n",
    "    i = 0\n",
    "    while Flag == 0: \n",
    "        i = i + 1\n",
    "        print(\"Game \" + str(i))\n",
    "        print(\"_____________________\")\n",
    "\n",
    "    #     if max_pol == min_pol:\n",
    "        if i == Game_rounds:            # i == # of iterations you want to run + 2\n",
    "                                # because Game 101 is skipped for collecting data, to get 200 game result, we need to run 201 iteration\n",
    "            print('min_recent_'+str(memory)+'_touched')# then stop at Game 202\n",
    "            print(min_touched)\n",
    "            print('max_recent_'+str(memory)+'_touched')\n",
    "            print(max_touched)\n",
    "            print('Min last 100 action')\n",
    "            print(min_touched_last_100)\n",
    "\n",
    "            break\n",
    "\n",
    "        elif equi_min == equi_max:\n",
    "            print(\"Reached Nash Equilibrium at game\"+ str(i) + \"and Equi_Por = \" + str(equi_min))\n",
    "            print('max_distribution')\n",
    "            print(max_frequency)\n",
    "            print('min_distribution')\n",
    "            print(fla_min_fre)\n",
    "            Flag = 1\n",
    "            break\n",
    "        ############################## maximizer play  \n",
    "        else:\n",
    "            if i == Game_rounds-100:    #if Game_round = 200, after 100 iteration, Game 101 print previous historical result\n",
    "    #             max_touched_100 = max_touched \n",
    "    #             min_touched_100 = min_touched\n",
    "    #             max_fre_100 = max_frequency  # store the max_frequency of first 100 iterataions\n",
    "    #             print('max_history')\n",
    "    #             print(max_history)\n",
    "    #             min_fre_100 = fla_min_fre  # max_frequency of first 100 iterations\n",
    "    #             print('min_history')\n",
    "    #             print(min_history)\n",
    "    # Remove max frequncy less than 0.1--\n",
    "                max_history_last_100 = np.zeros([n, 2]) \n",
    "                min_history_last_100 = [] \n",
    "                min_touched_last_100 =[]\n",
    "\n",
    "            (v1, max_opinion, equi_max) = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "            max_touched = push(max_touched, v1)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "    #             print('equi_max')\n",
    "    #             print(equi_max)\n",
    "    #         print(v1, max_opinion, max_pol)\n",
    "            # cumulate strategy \n",
    "            max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "\n",
    "            max_history_last_100[v1,int(max_opinion)] = max_history_last_100[v1,int(max_opinion)] +1\n",
    "    #         print('max_history')\n",
    "    #         print(max_history)\n",
    "    #________________________________________________________________\n",
    "            max_frequency = max_history/(i+1)  # its frequency \n",
    "    #         print('max_distribution')\n",
    "    #         print(max_frequency)\n",
    "        #     print(i+1) \n",
    "            fla_max_fre = max_frequency.flatten() #flaten max_frequency to calculate average payoff\n",
    "#             print('fla_max_fre')\n",
    "#             print(fla_max_fre)\n",
    "            print('fre_max at spot')\n",
    "            print(fla_max_fre[column])\n",
    "            # create payoff matrix for maxmizer\n",
    "            row = int(row_index(v2, min_opinion))\n",
    "            column = int(column_index(v1,max_opinion))\n",
    "\n",
    "    # _________________________________________________________________\n",
    "    #         ######################Visualize Maximizer's selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v1:\n",
    "    #                 color_map.append('Red')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True,node_size = 50)\n",
    "    #         plt.figure(figsize=(200, 200))\n",
    "    #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    ############################### minimizer play\n",
    "            (v2, payoff_row, min_opinion, equi_min) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre, min_touched_all)\n",
    "            min_touched = push(min_touched, v2)\n",
    "            min_touched_all.append(v2) \n",
    "            min_touched_last_100.append(v2)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('equi_min')\n",
    "    #         print(equi_min)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "            #         print(v2, min_opinion, min_pol)\n",
    "            if (v2,min_opinion) in counter.keys():\n",
    "                payoff_matrix = payoff_matrix # if this min_option is in min_history, no need to update paryoff matrix, only update frequency\n",
    "                print(\"Same history\")\n",
    "                print((str(v2),str(min_opinion)))\n",
    "            else:\n",
    "                payoff_matrix = np.vstack([payoff_matrix, payoff_row]) # if this is a new option, append to previous matrix\n",
    "    #                 print('payoff_row')\n",
    "    #                 print(payoff_row)\n",
    "            min_history.append((v2,min_opinion))\n",
    "            min_history_last_100.append((v2,min_opinion))\n",
    "            #         print('min_history')\n",
    "            #         print(min_history)\n",
    "            counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "            #print(counter)\n",
    "    #         print('counter.keys')\n",
    "    #         print(counter.keys())\n",
    "            fla_min_fre = np.array(list(counter.values()))/(i+1) #return only frequency of all min options in order\n",
    "    #         print('fla_min_fre')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "    #         print('fla_min_fre at the spot')\n",
    "    #         min_counter = dict(counter)\n",
    "    #         print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "            # create payoff matrix for minimizer\n",
    "            row = row_index(v2, min_opinion)\n",
    "            column = column_index(v1,max_opinion)\n",
    "            #     print('row, column')\n",
    "            #     print(row, column)\n",
    "\n",
    "            print(\"Not Reached Nash Equilibrium at Equi_Min = \" + str(equi_min) + \" and Equi_Max = \"+ str(equi_max)) \n",
    "    #         print('min_distribution')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "            ######################Visualize Minimizer selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G1, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G1)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v2:\n",
    "    #                 color_map.append('Blue')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True)\n",
    "    #         plt.figure(figsize=(25, 25))\n",
    "    #         plt.show()\n",
    "    return (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, fla_max_fre, max_history_last_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_touched\n",
      "[]\n",
      "Maximizer first selection\n",
      "    Agent0 's opinion 1.0 changed to 0\n",
      "Network reaches equilibrium Polarization: 0.1572292826553415\n",
      "history at spot\n",
      "1.0\n",
      "fre_max at spot\n",
      "[1. 0. 0. 0. 0. 0.]\n",
      "1.0\n",
      "Minimizer first selection\n",
      "    Agent2 's opinion 0.0 changed to 0\n",
      "Network reaches equilibrium Polarization: 0.45389476266008816\n",
      "min_history\n",
      "[(2, 0)]\n",
      "Counter({(2, 0): 1})\n",
      "fla_min_fre at the spot\n",
      "{(2, 0): 1}\n",
      "1\n",
      "Game 1\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.375\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.3096360403870187 and Equi_Max = 0.628917130621366\n",
      "Game 2\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.6666666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.3686790007421542...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.41666666666666663\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.333 0.667]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.3686790007421542 and Equi_Max = 0.5593861587452534\n",
      "Game 3\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.75\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.3974279145655427...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4375\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.25 0.75]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.3974279145655427 and Equi_Max = 0.5344923539994848\n",
      "Game 4\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.8\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.41443004162623864...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.45\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.2 0.8]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.41443004162623864 and Equi_Max = 0.521594787919996\n",
      "Game 5\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.8333333333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.42566178415281203...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.45833333333333337\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.167 0.833]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.42566178415281203 and Equi_Max = 0.5136893739398002\n",
      "Game 6\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.8571428571428571\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4336340040731528...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.46428571428571425\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.143 0.857]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4336340040731528 and Equi_Max = 0.5083435581317053\n",
      "Game 7\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.43958557735790194...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.46875\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.125 0.875]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.43958557735790194 and Equi_Max = 0.5044860770147305\n",
      "Game 8\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.8888888888888888\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.44419822819092536...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4722222222222222\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.111 0.889]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.44419822819092536 and Equi_Max = 0.5015707943109676\n",
      "Game 9\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4478780479726216...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.47500000000000003\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.1 0.9]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4478780479726216 and Equi_Max = 0.49928984024505624\n",
      "Game 10\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9090909090909091\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.45088199910989546...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.47727272727272724\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.091 0.909]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.45088199910989546 and Equi_Max = 0.4974563784674506\n",
      "Game 11\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9166666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.45338060950396186...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.47916666666666663\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.083 0.917]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.45338060950396186 and Equi_Max = 0.49595042253607347\n",
      "Game 12\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9230769230769231\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4554914936454539...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4807692307692308\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.077 0.923]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4554914936454539 and Equi_Max = 0.4946913800528216\n",
      "Game 13\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9285714285714286\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4572983972695451...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.48214285714285715\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.071 0.929]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4572983972695451 and Equi_Max = 0.4936231060294642\n",
      "Game 14\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9333333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.45886256926585767...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.48333333333333334\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.067 0.933]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.45886256926585767 and Equi_Max = 0.49270528349467835\n",
      "Game 15\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9375\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.46022984017985585...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.484375\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.062 0.938]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.46022984017985585 and Equi_Max = 0.49190821158000336\n",
      "Game 16\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9411764705882353\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4614351863962152...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4852941176470588\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.059 0.941]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4614351863962152 and Equi_Max = 0.49120952628190323\n",
      "Game 17\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9444444444444444\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.46250576367965207...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4861111111111111\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.056 0.944]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.46250576367965207 and Equi_Max = 0.4905920662734398\n",
      "Game 18\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9473684210526315\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.463462977222012...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.48684210526315785\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.053 0.947]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.463462977222012 and Equi_Max = 0.49004244286345344\n",
      "Game 19\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.95\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.46432392725830857...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4875\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.05 0.95]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.46432392725830857 and Equi_Max = 0.48955005714027955\n",
      "Game 20\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9523809523809523\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.46510243948013263...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4880952380952381\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.048 0.952]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.46510243948013263 and Equi_Max = 0.48910640921234033\n",
      "Game 21\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9545454545454546\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4658098130152717...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.48863636363636365\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.045 0.955]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4658098130152717 and Equi_Max = 0.48870460303431784\n",
      "Game 22\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9565217391304348\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.466455372342689...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4891304347826087\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.043 0.957]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.466455372342689 and Equi_Max = 0.48833898511320434\n",
      "Game 23\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9583333333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.46704688059099225...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.48958333333333337\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.042 0.958]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.46704688059099225 and Equi_Max = 0.4880048766803583\n",
      "Game 24\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.96\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.46759085320444577...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.04 0.96]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.46759085320444577 and Equi_Max = 0.48769837228319785\n",
      "Game 25\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9615384615384616\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4680927989147685...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4903846153846154\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.038 0.962]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4680927989147685 and Equi_Max = 0.48741618634046974\n",
      "Game 26\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9629629629629629\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.46855740694269743...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4907407407407407\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.037 0.963]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.46855740694269743 and Equi_Max = 0.4871555348428185\n",
      "Game 27\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9642857142857143\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.468988693925137...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4910714285714286\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.036 0.964]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.468988693925137 and Equi_Max = 0.48691404315180753\n",
      "Game 28\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9655172413793104\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.46939012032654737...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4913793103448276\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.034 0.966]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.46939012032654737 and Equi_Max = 0.4866896734178094\n",
      "Game 29\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9666666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.46976468348131467...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4916666666666667\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.033 0.967]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.46976468348131467 and Equi_Max = 0.48648066691287667\n",
      "Game 30\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.967741935483871\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4701149925629325...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49193548387096775\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.032 0.968]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4701149925629325 and Equi_Max = 0.48628549782104347\n",
      "Game 31\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.96875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4704433294472764...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4921875\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.031 0.969]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4704433294472764 and Equi_Max = 0.48610283591522735\n",
      "Game 32\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9696969696969697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4707516984722516...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49242424242424243\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.03 0.97]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4707516984722516 and Equi_Max = 0.48593151618871544\n",
      "Game 33\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9705882352941176\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4710418673873622...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4926470588235294\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.029 0.971]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4710418673873622 and Equi_Max = 0.4857705139748293\n",
      "Game 34\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9714285714285714\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.471315401260847...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4928571428571428\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.029 0.971]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.471315401260847 and Equi_Max = 0.48561892443141513\n",
      "Game 35\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9722222222222222\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4715736907179957...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4930555555555555\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.028 0.972]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4715736907179957 and Equi_Max = 0.48547594552213896\n",
      "Game 36\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.972972972972973\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47181797558637345...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49324324324324326\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.027 0.973]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47181797558637345 and Equi_Max = 0.4853408638184036\n",
      "Game 37\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9736842105263158\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47204936479654164...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.493421052631579\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.026 0.974]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47204936479654164 and Equi_Max = 0.4852130425911319\n",
      "Game 38\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9743589743589743\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4722688532122976...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49358974358974356\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.026 0.974]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4722688532122976 and Equi_Max = 0.4850919117728135\n",
      "Game 39\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.975\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47247733592927604...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49374999999999997\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.025 0.975]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47247733592927604 and Equi_Max = 0.48497695945584995\n",
      "Game 40\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.975609756097561\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47267562047532186...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49390243902439024\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.024 0.976]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47267562047532186 and Equi_Max = 0.4848677246596942\n",
      "Game 41\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9761904761904762\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47286443726328176...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.494047619047619\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.024 0.976]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47286443726328176 and Equi_Max = 0.4847637911512218\n",
      "Game 42\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9767441860465116\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47304444858143846...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4941860465116279\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.023 0.977]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47304444858143846 and Equi_Max = 0.4846647821436383\n",
      "Game 43\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9772727272727273\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47321625635483927...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4943181818181818\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.023 0.977]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47321625635483927 and Equi_Max = 0.484570355731573\n",
      "Game 44\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9777777777777777\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47338040886918276...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4944444444444444\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.022 0.978]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47338040886918276 and Equi_Max = 0.4844802009457804\n",
      "Game 45\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9782608695652174\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4735374066155291...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4945652173913044\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.022 0.978]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4735374066155291 and Equi_Max = 0.48439403433149514\n",
      "Game 46\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9787234042553191\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47368770738708127...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49468085106382975\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.021 0.979]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47368770738708127 and Equi_Max = 0.48431159697110787\n",
      "Game 47\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9791666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47383173073737117...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49479166666666663\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.021 0.979]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47383173073737117 and Equi_Max = 0.4842326518852713\n",
      "Game 48\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9795918367346939\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4739698618912878...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49489795918367346\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.02 0.98]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4739698618912878 and Equi_Max = 0.4841569817574817\n",
      "Game 49\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.98\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4741024551857194...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.495\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.02 0.98]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4741024551857194 and Equi_Max = 0.4840843869361188\n",
      "Game 50\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9803921568627451\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47422983710451866...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49509803921568624\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.02 0.98]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47422983710451866 and Equi_Max = 0.4840146836752564\n",
      "Game 51\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9807692307692307\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4743523089625168...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49519230769230765\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.019 0.981]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4743523089625168 and Equi_Max = 0.48394770258160197\n",
      "Game 52\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9811320754716981\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4744701492850396...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49528301886792453\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.019 0.981]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4744701492850396 and Equi_Max = 0.48388328723992946\n",
      "Game 53\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9814814814814815\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47458361592247755...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4953703703703704\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.019 0.981]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47458361592247755 and Equi_Max = 0.48382129299351656\n",
      "Game 54\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9818181818181818\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47469294793370287...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4954545454545454\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.018 0.982]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47469294793370287 and Equi_Max = 0.48376158585957413\n",
      "Game 55\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9821428571428571\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47479836726728186...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49553571428571425\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.018 0.982]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47479836726728186 and Equi_Max = 0.48370404156254776\n",
      "Game 56\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9824561403508771\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47490008026536734...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4956140350877193\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.018 0.982]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47490008026536734 and Equi_Max = 0.48364854467061313\n",
      "Game 57\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9827586206896551\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4749982790117119...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49568965517241376\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.017 0.983]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4749982790117119 and Equi_Max = 0.48359498782273713\n",
      "Game 58\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9830508474576272\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4750931425423319...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4957627118644068\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.017 0.983]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4750931425423319 and Equi_Max = 0.483543271035413\n",
      "Game 59\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9833333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4751848379348761...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4958333333333333\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.017 0.983]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4751848379348761 and Equi_Max = 0.48349330107965033\n",
      "Game 60\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9836065573770492\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47527352129064454...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4959016393442623\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.016 0.984]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47527352129064454 and Equi_Max = 0.48344499092005144\n",
      "Game 61\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9838709677419355\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4753593386213998...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4959677419354839\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.016 0.984]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4753593386213998 and Equi_Max = 0.48339825920887197\n",
      "Game 62\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9841269841269841\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4754424266515675...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.496031746031746\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.016 0.984]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4754424266515675 and Equi_Max = 0.48335302982888084\n",
      "Game 63\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.984375\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4755229135450974...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49609375\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.016 0.984]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4755229135450974 and Equi_Max = 0.4833092314796144\n",
      "Game 64\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9846153846153847\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4756009195651138...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49615384615384617\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.015 0.985]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4756009195651138 and Equi_Max = 0.48326679730229166\n",
      "Game 65\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9848484848484849\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47567655767349176...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4962121212121212\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.015 0.985]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47567655767349176 and Equi_Max = 0.4832256645392441\n",
      "Game 66\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9850746268656716\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4757499340766524...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4962686567164179\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.015 0.985]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4757499340766524 and Equi_Max = 0.48318577422420794\n",
      "Game 67\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9852941176470589\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.475821148723117...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4963235294117647\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.015 0.985]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.475821148723117 and Equi_Max = 0.48314707090026576\n",
      "Game 68\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9855072463768116\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4758902957577255...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49637681159420294\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4758902957577255 and Equi_Max = 0.4831095023625992\n",
      "Game 69\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9857142857142858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47595746393685867...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49642857142857144\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47595746393685867 and Equi_Max = 0.4830730194235432\n",
      "Game 70\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9859154929577465\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47602273700851455...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4964788732394366\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47602273700851455 and Equi_Max = 0.4830375756977202\n",
      "Game 71\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9861111111111112\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4760861940606626...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4965277777777778\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4760861940606626 and Equi_Max = 0.4830031274052764\n",
      "Game 72\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9863013698630136\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4761479098409204...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4965753424657534\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4761479098409204 and Equi_Max = 0.48296963319146924\n",
      "Game 73\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9864864864864865\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4762079550502727...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49662162162162166\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.014 0.986]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4762079550502727 and Equi_Max = 0.48293705396104036\n",
      "Game 74\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9866666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47626639661325487...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4966666666666667\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.013 0.987]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47626639661325487 and Equi_Max = 0.4829053527259802\n",
      "Game 75\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9868421052631579\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47632329792677686...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49671052631578944\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.013 0.987]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47632329792677686 and Equi_Max = 0.4828744944654367\n",
      "Game 76\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.987012987012987\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47637871908952695...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4967532467532467\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.013 0.987]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47637871908952695 and Equi_Max = 0.48284444599665455\n",
      "Game 77\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9871794871794872\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4764327171137058...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4967948717948718\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.013 0.987]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4764327171137058 and Equi_Max = 0.48281517585594375\n",
      "Game 78\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9873417721518988\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4764853461206543...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4968354430379747\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.013 0.987]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4764853461206543 and Equi_Max = 0.48278665418877853\n",
      "Game 79\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47653665752179064...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.496875\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.013 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47653665752179064 and Equi_Max = 0.4827588526482228\n",
      "Game 80\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9876543209876543\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47658670018612326...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49691358024691357\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47658670018612326 and Equi_Max = 0.4827317443009531\n",
      "Game 81\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9878048780487805\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47663552059549186...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4969512195121951\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47663552059549186 and Equi_Max = 0.4827053035402245\n",
      "Game 82\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9879518072289156\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47668316298856966...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4969879518072289\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47668316298856966 and Equi_Max = 0.4826795060051893\n",
      "Game 83\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9880952380952381\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47672966949456697...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49702380952380953\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47672966949456697 and Equi_Max = 0.48265432850603224\n",
      "Game 84\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9882352941176471\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4767750802574825...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4970588235294118\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4767750802574825 and Equi_Max = 0.48262974895444166\n",
      "Game 85\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9883720930232558\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4768194335516759...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49709302325581395\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.012 0.988]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4768194335516759 and Equi_Max = 0.4826057462989746\n",
      "Game 86\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9885057471264368\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4768627658894592...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4971264367816092\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4768627658894592 and Equi_Max = 0.482582300464922\n",
      "Game 87\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9886363636363636\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47690511212134257...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49715909090909094\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47690511212134257 and Equi_Max = 0.4825593922983095\n",
      "Game 88\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9887640449438202\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4769465055295132...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49719101123595505\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4769465055295132 and Equi_Max = 0.48253700351370854\n",
      "Game 89\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9888888888888889\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4769869779150755...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49722222222222223\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4769869779150755 and Equi_Max = 0.4825151166455569\n",
      "Game 90\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.989010989010989\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4770265596795297...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49725274725274726\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4770265596795297 and Equi_Max = 0.4824937150027171\n",
      "Game 91\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9891304347826086\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4770652799009329...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49728260869565216\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4770652799009329 and Equi_Max = 0.48247278262602455\n",
      "Game 92\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.989247311827957\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47710316640513895...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49731182795698925\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47710316640513895 and Equi_Max = 0.4824523042485967\n",
      "Game 93\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9893617021276596\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47714024583248477...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49734042553191493\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47714024583248477 and Equi_Max = 0.4824322652586987\n",
      "Game 94\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9894736842105263\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4771765437002615...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49736842105263157\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.011 0.989]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4771765437002615 and Equi_Max = 0.48241265166497244\n",
      "Game 95\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9895833333333334\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4772120844612768...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49739583333333337\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4772120844612768 and Equi_Max = 0.48239345006385864\n",
      "Game 96\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9896907216494846\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.477246891558787...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49742268041237114\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.477246891558787 and Equi_Max = 0.48237464760904847\n",
      "Game 97\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9897959183673469\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47728098747806813...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4974489795918367\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47728098747806813 and Equi_Max = 0.4823562319828227\n",
      "Game 98\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.98989898989899\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.477314393794852...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4974747474747475\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.477314393794852 and Equi_Max = 0.48233819136913997\n",
      "Game 99\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.99\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4773471312208559...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4975\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4773471312208559 and Equi_Max = 0.48232051442835455\n",
      "Game 100\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9900990099009901\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4773792196466046...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4975247524752475\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4773792196466046 and Equi_Max = 0.4823031902734458\n",
      "Game 101\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9901960784313726\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47741067818172883...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49754901960784315\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47741067818172883 and Equi_Max = 0.48228620844765907\n",
      "Game 102\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9902912621359223\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.477441525192915...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4975728155339806\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.477441525192915 and Equi_Max = 0.48226955890345613\n",
      "Game 103\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9903846153846154\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47747177833966387...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49759615384615385\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47747177833966387 and Equi_Max = 0.4822532319826929\n",
      "Game 104\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9904761904761905\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4775014546080009...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4976190476190476\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.01 0.99]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4775014546080009 and Equi_Max = 0.4822372183979353\n",
      "Game 105\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9905660377358491\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4775305703422796...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4976415094339623\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4775305703422796 and Equi_Max = 0.48222150921484247\n",
      "Game 106\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9906542056074766\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47755914127519744...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49766355140186913\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47755914127519744 and Equi_Max = 0.48220609583554447\n",
      "Game 107\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9907407407407407\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47758718255614313...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4976851851851852\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47758718255614313 and Equi_Max = 0.4821909699829504\n",
      "Game 108\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9908256880733946\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47761470877798273...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49770642201834864\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47761470877798273 and Equi_Max = 0.4821761236859268\n",
      "Game 109\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.990909090909091\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47764173400238086...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49772727272727274\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47764173400238086 and Equi_Max = 0.48216154926528976\n",
      "Game 110\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.990990990990991\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4776682717837546...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4977477477477477\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4776682717837546 and Equi_Max = 0.48214723932056003\n",
      "Game 111\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9910714285714286\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4776943351919415...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49776785714285715\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4776943351919415 and Equi_Max = 0.48213318671743316\n",
      "Game 112\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9911504424778761\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4777199368336615...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.497787610619469\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4777199368336615 and Equi_Max = 0.4821193845759186\n",
      "Game 113\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9912280701754386\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47774508887285094...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49780701754385964\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47774508887285094 and Equi_Max = 0.48210582625910803\n",
      "Game 114\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.991304347826087\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47776980304993244...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49782608695652175\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47776980304993244 and Equi_Max = 0.4820925053625339\n",
      "Game 115\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9913793103448276\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47779409070008777...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4978448275862069\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47779409070008777 and Equi_Max = 0.4820794157040818\n",
      "Game 116\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9914529914529915\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4778179627705927...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49786324786324787\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.009 0.991]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4778179627705927 and Equi_Max = 0.48206655131442366\n",
      "Game 117\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9915254237288136\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4778414298372696...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4978813559322034\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4778414298372696 and Equi_Max = 0.48205390642794066\n",
      "Game 118\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9915966386554622\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4778645021201101...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49789915966386555\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4778645021201101 and Equi_Max = 0.4820414754741072\n",
      "Game 119\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9916666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.477887189498115...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4979166666666667\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.477887189498115 and Equi_Max = 0.4820292530693085\n",
      "Game 120\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9917355371900827\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4779095015233979...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49793388429752067\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4779095015233979 and Equi_Max = 0.48201723400906565\n",
      "Game 121\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9918032786885246\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4779314474345955...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4979508196721311\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4779314474345955 and Equi_Max = 0.48200541326064716\n",
      "Game 122\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.991869918699187\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47795303616962187...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4979674796747967\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47795303616962187 and Equi_Max = 0.4819937859560416\n",
      "Game 123\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9919354838709677\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4779742763778056...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49798387096774194\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4779742763778056 and Equi_Max = 0.4819823473852736\n",
      "Game 124\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.992\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4779951764314433...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.498\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4779951764314433 and Equi_Max = 0.4819710929900424\n",
      "Game 125\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9920634920634921\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47801574443680434...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.498015873015873\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47801574443680434 and Equi_Max = 0.4819600183576646\n",
      "Game 126\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9921259842519685\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4780359882446122...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49803149606299213\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4780359882446122 and Equi_Max = 0.4819491192153063\n",
      "Game 127\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9921875\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4780559154600359...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.498046875\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4780559154600359 and Equi_Max = 0.4819383914244857\n",
      "Game 128\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9922480620155039\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47807553345221715...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49806201550387597\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47807553345221715 and Equi_Max = 0.48192783097583325\n",
      "Game 129\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9923076923076923\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4780948493633551...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4980769230769231\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4780948493633551 and Equi_Max = 0.48191743398409703\n",
      "Game 130\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9923664122137404\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47811387011737627...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4980916030534351\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47811387011737627 and Equi_Max = 0.4819071966833755\n",
      "Game 131\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9924242424242424\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47813260242820954...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4981060606060606\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47813260242820954 and Equi_Max = 0.4818971154225698\n",
      "Game 132\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9924812030075187\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4781510528076881...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4981203007518797\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.008 0.992]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4781510528076881 and Equi_Max = 0.48188718666104247\n",
      "Game 133\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9925373134328358\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4781692275730955...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4981343283582089\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4781692275730955 and Equi_Max = 0.481877406964471\n",
      "Game 134\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9925925925925926\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4781871328543784...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4981481481481481\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4781871328543784 and Equi_Max = 0.48186777300088734\n",
      "Game 135\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9926470588235294\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47820477460103983...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49816176470588236\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47820477460103983 and Equi_Max = 0.4818582815368931\n",
      "Game 136\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9927007299270073\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4782221585887289...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49817518248175185\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4782221585887289 and Equi_Max = 0.4818489294340407\n",
      "Game 137\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9927536231884058\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47823929042554664...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49818840579710144\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47823929042554664 and Equi_Max = 0.48183971364537437\n",
      "Game 138\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9928057553956835\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47825617555807715...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4982014388489209\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47825617555807715 and Equi_Max = 0.48183063121211933\n",
      "Game 139\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9928571428571429\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4782728192771603...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4982142857142857\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4782728192771603 and Equi_Max = 0.4818216792605154\n",
      "Game 140\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9929078014184397\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47828922672342006...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4982269503546099\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47828922672342006 and Equi_Max = 0.4818128549987847\n",
      "Game 141\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9929577464788732\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4783054028925576...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4982394366197183\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4783054028925576 and Equi_Max = 0.48180415571422974\n",
      "Game 142\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.993006993006993\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4783213526404213...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4982517482517483\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4783213526404213 and Equi_Max = 0.4817955787704519\n",
      "Game 143\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9930555555555556\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4783370806878697...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4982638888888889\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4783370806878697 and Equi_Max = 0.48178712160468906\n",
      "Game 144\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.993103448275862\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47835259162542637...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4982758620689655\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47835259162542637 and Equi_Max = 0.48177878172526084\n",
      "Game 145\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9931506849315068\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47836788991774964...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4982876712328767\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47836788991774964 and Equi_Max = 0.48177055670912267\n",
      "Game 146\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9931972789115646\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47838297990791595...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49829931972789115\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47838297990791595 and Equi_Max = 0.4817624441995176\n",
      "Game 147\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9932432432432432\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47839786582153004...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4983108108108108\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47839786582153004 and Equi_Max = 0.4817544419037254\n",
      "Game 148\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9932885906040269\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4784125517706712...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49832214765100674\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4784125517706712 and Equi_Max = 0.4817465475909023\n",
      "Game 149\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9933333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4784270417576794...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4983333333333333\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4784270417576794 and Equi_Max = 0.48173875909000685\n",
      "Game 150\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9933774834437086\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4784413396787921...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49834437086092714\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4784413396787921 and Equi_Max = 0.48173107428781053\n",
      "Game 151\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.993421052631579\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4784554493276372...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49835526315789475\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4784554493276372 and Equi_Max = 0.48172349112698487\n",
      "Game 152\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9934640522875817\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47846937439858855...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49836601307189543\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.007 0.993]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47846937439858855 and Equi_Max = 0.48171600760426614\n",
      "Game 153\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9935064935064936\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.478483118489991...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4983766233766234\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.478483118489991 and Equi_Max = 0.4817086217686906\n",
      "Game 154\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9935483870967742\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4784966851072603...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4983870967741935\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4784966851072603 and Equi_Max = 0.48170133171989865\n",
      "Game 155\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9935897435897436\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47851007766586434...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4983974358974359\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47851007766586434 and Equi_Max = 0.48169413560650487\n",
      "Game 156\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9936305732484076\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47852329949418865...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4984076433121019\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47852329949418865 and Equi_Max = 0.4816870316245305\n",
      "Game 157\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9936708860759493\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4785363538362953...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49841772151898733\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4785363538362953 and Equi_Max = 0.4816800180158958\n",
      "Game 158\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9937106918238994\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4785492438545758...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49842767295597484\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4785492438545758 and Equi_Max = 0.4816730930669689\n",
      "Game 159\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.99375\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47856197263230577...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49843750000000003\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47856197263230577 and Equi_Max = 0.4816662551071711\n",
      "Game 160\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9937888198757764\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47857454317610276...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49844720496894407\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47857454317610276 and Equi_Max = 0.4816595025076319\n",
      "Game 161\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9938271604938271\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4785869584182949...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4984567901234568\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4785869584182949 and Equi_Max = 0.4816528336798965\n",
      "Game 162\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9938650306748467\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4785992212192005...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49846625766871167\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4785992212192005 and Equi_Max = 0.4816462470746794\n",
      "Game 163\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9939024390243902\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47861133436932574...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49847560975609756\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47861133436932574 and Equi_Max = 0.4816397411806648\n",
      "Game 164\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9939393939393939\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47862330059148106...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4984848484848485\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47862330059148106 and Equi_Max = 0.4816333145233503\n",
      "Game 165\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9939759036144579\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47863512254282176...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4984939759036145\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47863512254282176 and Equi_Max = 0.4816269656639327\n",
      "Game 166\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9940119760479041\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4786468028168151...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.498502994011976\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4786468028168151 and Equi_Max = 0.48162069319823453\n",
      "Game 167\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9940476190476191\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4786583439451373...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49851190476190477\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4786583439451373 and Equi_Max = 0.48161449575566817\n",
      "Game 168\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9940828402366864\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47866974839950244...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4985207100591716\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47866974839950244 and Equi_Max = 0.4816083719982377\n",
      "Game 169\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9941176470588236\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4786810185934278...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4985294117647059\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4786810185934278 and Equi_Max = 0.48160232061957575\n",
      "Game 170\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9941520467836257\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47869215688393607...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4985380116959064\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47869215688393607 and Equi_Max = 0.48159634034401466\n",
      "Game 171\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9941860465116279\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47870316557319936...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.498546511627907\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47870316557319936 and Equi_Max = 0.48159042992568935\n",
      "Game 172\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9942196531791907\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47871404691012454...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4985549132947977\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47871404691012454 and Equi_Max = 0.4815845881476729\n",
      "Game 173\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9942528735632183\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4787248030918861...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4985632183908046\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4787248030918861 and Equi_Max = 0.4815788138211404\n",
      "Game 174\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9942857142857143\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4787354362654045...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4985714285714286\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4787354362654045 and Equi_Max = 0.4815731057845629\n",
      "Game 175\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop\n",
      "fre_max at spot\n",
      "0.9943181818181818\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47874594852877417...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4985795454545454\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47874594852877417 and Equi_Max = 0.4815674629029283\n",
      "Game 176\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9943502824858758\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4787563419326456...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49858757062146897\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4787563419326456 and Equi_Max = 0.48156188406698897\n",
      "Game 177\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9943820224719101\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4787666184815564...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4985955056179775\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4787666184815564 and Equi_Max = 0.4815563681925348\n",
      "Game 178\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.994413407821229\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47877678013522196...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49860335195530725\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47877678013522196 and Equi_Max = 0.48155091421969076\n",
      "Game 179\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9944444444444445\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47878682880978096...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4986111111111111\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47878682880978096 and Equi_Max = 0.48154552111223803\n",
      "Game 180\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.994475138121547\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4787967663789994...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49861878453038677\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.006 0.994]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4787967663789994 and Equi_Max = 0.4815401878569574\n",
      "Game 181\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9945054945054945\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47880659467543557...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49862637362637363\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47880659467543557 and Equi_Max = 0.48153491346299515\n",
      "Game 182\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.994535519125683\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4788163154915672...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49863387978142076\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4788163154915672 and Equi_Max = 0.4815296969612489\n",
      "Game 183\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9945652173913043\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47882593058088085...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49864130434782605\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47882593058088085 and Equi_Max = 0.4815245374037739\n",
      "Game 184\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9945945945945946\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47883544165892616...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49864864864864866\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47883544165892616 and Equi_Max = 0.4815194338632095\n",
      "Game 185\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9946236559139785\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4788448504043366...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4986559139784946\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4788448504043366 and Equi_Max = 0.48151438543222247\n",
      "Game 186\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9946524064171123\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4788541584598172...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49866310160427807\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4788541584598172 and Equi_Max = 0.48150939122297004\n",
      "Game 187\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9946808510638298\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47886336743310054...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4986702127659574\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47886336743310054 and Equi_Max = 0.48150445036657874\n",
      "Game 188\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9947089947089947\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4788724788978726...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49867724867724866\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4788724788978726 and Equi_Max = 0.48149956201264144\n",
      "Game 189\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9947368421052631\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4788814943946687...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4986842105263158\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4788814943946687 and Equi_Max = 0.48149472532872806\n",
      "Game 190\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9947643979057592\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4788904154317429...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4986910994764398\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4788904154317429 and Equi_Max = 0.48148993949991387\n",
      "Game 191\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9947916666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4788992434859083...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49869791666666663\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4788992434859083 and Equi_Max = 0.48148520372832176\n",
      "Game 192\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9948186528497409\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47890798000335233...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49870466321243523\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47890798000335233 and Equi_Max = 0.4814805172326781\n",
      "Game 193\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9948453608247423\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47891662640042665...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49871134020618557\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47891662640042665 and Equi_Max = 0.48147587924788426\n",
      "Game 194\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9948717948717949\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4789251840644124...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4987179487179487\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4789251840644124 and Equi_Max = 0.48147128902459896\n",
      "Game 195\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9948979591836735\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4789336543542624...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4987244897959184\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4789336543542624 and Equi_Max = 0.4814667458288353\n",
      "Game 196\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9949238578680203\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.4789420386013204...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49873096446700504\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.4789420386013204 and Equi_Max = 0.4814622489415694\n",
      "Game 197\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9949494949494949\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47895033811001897...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4987373737373737\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47895033811001897 and Equi_Max = 0.4814577976583611\n",
      "Game 198\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9949748743718593\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47895855415855565...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.4987437185929648\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47895855415855565 and Equi_Max = 0.4814533912889861\n",
      "Game 199\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.995\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47896668799954933...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49874999999999997\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47896668799954933 and Equi_Max = 0.48144902915707866\n",
      "Game 200\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.9950248756218906\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.47897474086067743...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.49875621890547267\n",
      "fla_max_fre\n",
      "(array([0, 2], dtype=int64),)\n",
      "[0.005 0.995]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.47897474086067743 and Equi_Max = 0.48144471059978633\n",
      "Game 201\n",
      "_____________________\n",
      "min_recent_1_touched\n",
      "[2]\n",
      "max_recent_1_touched\n",
      "[1]\n",
      "Min last 100 action\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "Experiment = 10\n",
    "\n",
    "Experiment_note = str('Note: This experiement has initial condition. Game round:'+str(Game_rounds)+'.')\n",
    "(First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, fla_max_fre, max_history_last_100) = all_fre_limited_touch(s, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_distribution\n",
      "[1.]\n",
      "(array([1], dtype=int64), array([0], dtype=int64))\n",
      "Min_distribution_last_100\n",
      "Counter({2: 100})\n",
      "fla_min_fre\n",
      "[1.]\n",
      "Counter({2: 100})\n",
      "dict_keys([(2, 0), (2, 0.375), (2, 0.41666666666666663), (2, 0.4375), (2, 0.45), (2, 0.45833333333333337), (2, 0.46428571428571425), (2, 0.46875), (2, 0.4722222222222222), (2, 0.47500000000000003), (2, 0.47727272727272724), (2, 0.47916666666666663), (2, 0.4807692307692308), (2, 0.48214285714285715), (2, 0.48333333333333334), (2, 0.484375), (2, 0.4852941176470588), (2, 0.4861111111111111), (2, 0.48684210526315785), (2, 0.4875), (2, 0.4880952380952381), (2, 0.48863636363636365), (2, 0.4891304347826087), (2, 0.48958333333333337), (2, 0.49), (2, 0.4903846153846154), (2, 0.4907407407407407), (2, 0.4910714285714286), (2, 0.4913793103448276), (2, 0.4916666666666667), (2, 0.49193548387096775), (2, 0.4921875), (2, 0.49242424242424243), (2, 0.4926470588235294), (2, 0.4928571428571428), (2, 0.4930555555555555), (2, 0.49324324324324326), (2, 0.493421052631579), (2, 0.49358974358974356), (2, 0.49374999999999997), (2, 0.49390243902439024), (2, 0.494047619047619), (2, 0.4941860465116279), (2, 0.4943181818181818), (2, 0.4944444444444444), (2, 0.4945652173913044), (2, 0.49468085106382975), (2, 0.49479166666666663), (2, 0.49489795918367346), (2, 0.495), (2, 0.49509803921568624), (2, 0.49519230769230765), (2, 0.49528301886792453), (2, 0.4953703703703704), (2, 0.4954545454545454), (2, 0.49553571428571425), (2, 0.4956140350877193), (2, 0.49568965517241376), (2, 0.4957627118644068), (2, 0.4958333333333333), (2, 0.4959016393442623), (2, 0.4959677419354839), (2, 0.496031746031746), (2, 0.49609375), (2, 0.49615384615384617), (2, 0.4962121212121212), (2, 0.4962686567164179), (2, 0.4963235294117647), (2, 0.49637681159420294), (2, 0.49642857142857144), (2, 0.4964788732394366), (2, 0.4965277777777778), (2, 0.4965753424657534), (2, 0.49662162162162166), (2, 0.4966666666666667), (2, 0.49671052631578944), (2, 0.4967532467532467), (2, 0.4967948717948718), (2, 0.4968354430379747), (2, 0.496875), (2, 0.49691358024691357), (2, 0.4969512195121951), (2, 0.4969879518072289), (2, 0.49702380952380953), (2, 0.4970588235294118), (2, 0.49709302325581395), (2, 0.4971264367816092), (2, 0.49715909090909094), (2, 0.49719101123595505), (2, 0.49722222222222223), (2, 0.49725274725274726), (2, 0.49728260869565216), (2, 0.49731182795698925), (2, 0.49734042553191493), (2, 0.49736842105263157), (2, 0.49739583333333337), (2, 0.49742268041237114), (2, 0.4974489795918367), (2, 0.4974747474747475), (2, 0.4975), (2, 0.4975247524752475), (2, 0.49754901960784315), (2, 0.4975728155339806), (2, 0.49759615384615385), (2, 0.4976190476190476), (2, 0.4976415094339623), (2, 0.49766355140186913), (2, 0.4976851851851852), (2, 0.49770642201834864), (2, 0.49772727272727274), (2, 0.4977477477477477), (2, 0.49776785714285715), (2, 0.497787610619469), (2, 0.49780701754385964), (2, 0.49782608695652175), (2, 0.4978448275862069), (2, 0.49786324786324787), (2, 0.4978813559322034), (2, 0.49789915966386555), (2, 0.4979166666666667), (2, 0.49793388429752067), (2, 0.4979508196721311), (2, 0.4979674796747967), (2, 0.49798387096774194), (2, 0.498), (2, 0.498015873015873), (2, 0.49803149606299213), (2, 0.498046875), (2, 0.49806201550387597), (2, 0.4980769230769231), (2, 0.4980916030534351), (2, 0.4981060606060606), (2, 0.4981203007518797), (2, 0.4981343283582089), (2, 0.4981481481481481), (2, 0.49816176470588236), (2, 0.49817518248175185), (2, 0.49818840579710144), (2, 0.4982014388489209), (2, 0.4982142857142857), (2, 0.4982269503546099), (2, 0.4982394366197183), (2, 0.4982517482517483), (2, 0.4982638888888889), (2, 0.4982758620689655), (2, 0.4982876712328767), (2, 0.49829931972789115), (2, 0.4983108108108108), (2, 0.49832214765100674), (2, 0.4983333333333333), (2, 0.49834437086092714), (2, 0.49835526315789475), (2, 0.49836601307189543), (2, 0.4983766233766234), (2, 0.4983870967741935), (2, 0.4983974358974359), (2, 0.4984076433121019), (2, 0.49841772151898733), (2, 0.49842767295597484), (2, 0.49843750000000003), (2, 0.49844720496894407), (2, 0.4984567901234568), (2, 0.49846625766871167), (2, 0.49847560975609756), (2, 0.4984848484848485), (2, 0.4984939759036145), (2, 0.498502994011976), (2, 0.49851190476190477), (2, 0.4985207100591716), (2, 0.4985294117647059), (2, 0.4985380116959064), (2, 0.498546511627907), (2, 0.4985549132947977), (2, 0.4985632183908046), (2, 0.4985714285714286), (2, 0.4985795454545454), (2, 0.49858757062146897), (2, 0.4985955056179775), (2, 0.49860335195530725), (2, 0.4986111111111111), (2, 0.49861878453038677), (2, 0.49862637362637363), (2, 0.49863387978142076), (2, 0.49864130434782605), (2, 0.49864864864864866), (2, 0.4986559139784946), (2, 0.49866310160427807), (2, 0.4986702127659574), (2, 0.49867724867724866), (2, 0.4986842105263158), (2, 0.4986910994764398), (2, 0.49869791666666663), (2, 0.49870466321243523), (2, 0.49871134020618557), (2, 0.4987179487179487), (2, 0.4987244897959184), (2, 0.49873096446700504), (2, 0.4987373737373737), (2, 0.4987437185929648), (2, 0.49874999999999997), (2, 0.49875621890547267)])\n",
      "Min_distribution_all\n",
      "[0.995]\n",
      "Max_distribution_all\n",
      "[0.005 0.995]\n",
      "[(array([0, 1], dtype=int64), array([0, 0], dtype=int64))]\n"
     ]
    }
   ],
   "source": [
    "# MAXimizer's distribution of LAST 100 iteration \n",
    "print('Max_distribution')  \n",
    "max_l100_fre = max_history_last_100/100\n",
    "print(max_l100_fre [np.nonzero(max_l100_fre)])\n",
    "# print for small network\n",
    "#print(max_history_last_100)\n",
    "# # Print for Large Network\n",
    "print(np.nonzero(max_l100_fre))\n",
    "\n",
    "# MINimizer's Strategy in the last 100 round\n",
    "print('Min_distribution_last_100')\n",
    "counter_h=collections.Counter(min_touched_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter_h)\n",
    "counter=collections.Counter(min_touched_last_100)\n",
    "fla_min_fre = np.array(list(counter.values()))/(100) #return only frequency of all min options in order\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)\n",
    "print(counter_h)\n",
    "# print(min_touched_last_100)\n",
    "\n",
    "\n",
    "counter_1h=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter_1h.keys())\n",
    "counter_1=collections.Counter(min_touched_all)  #return a dictionary include {'min_option': count of this choice}\n",
    "# print(counter_1)\n",
    "fla_min_fre_1 = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "print('Min_distribution_all')\n",
    "print(fla_min_fre_1)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "max_fre = max_history/Game_rounds\n",
    "print('Max_distribution_all')\n",
    "print(max_fre[np.nonzero(max_fre)])\n",
    "print([np.nonzero(max_fre)])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.000e+02 -1.000e+02  1.131e-01  4.602e-01  1.294e-01  1.745e-01]\n",
      " [ 4.539e-01  1.572e-01  6.289e-01  0.000e+00 -1.000e+02 -1.000e+02]\n",
      " [ 3.220e-01  1.243e-01  5.465e-01  1.648e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.388e-02  4.973e-01  1.133e-01  2.533e-01]\n",
      " [-1.000e+02 -1.000e+02  2.808e-02  5.166e-01  1.151e-01  2.864e-01]\n",
      " [ 2.904e-01  1.209e-01  5.290e-01  2.724e-02 -1.000e+02 -1.000e+02]\n",
      " [ 3.079e-01  1.225e-01  5.385e-01  2.086e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.854e-01  1.206e-01  5.264e-01  2.930e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  3.627e-02  5.056e-01  1.137e-01  2.679e-01]\n",
      " [-1.000e+02 -1.000e+02  3.047e-02  5.131e-01  1.146e-01  2.806e-01]\n",
      " [-1.000e+02 -1.000e+02  3.739e-02  5.043e-01  1.136e-01  2.656e-01]\n",
      " [ 2.785e-01  1.203e-01  5.228e-01  3.230e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.876e-01  1.207e-01  5.275e-01  2.839e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.958e-01  1.213e-01  5.319e-01  2.515e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.764e-01  1.202e-01  5.217e-01  3.328e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  3.980e-02  5.016e-01  1.134e-01  2.609e-01]\n",
      " [-1.000e+02 -1.000e+02  4.015e-02  5.012e-01  1.134e-01  2.602e-01]\n",
      " [ 2.745e-01  1.202e-01  5.207e-01  3.418e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.801e-01  1.204e-01  5.236e-01  3.160e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.793e-01  1.203e-01  5.232e-01  3.197e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.736e-01  1.202e-01  5.203e-01  3.457e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.110e-02  5.001e-01  1.134e-01  2.584e-01]\n",
      " [-1.000e+02 -1.000e+02  3.857e-02  5.029e-01  1.135e-01  2.633e-01]\n",
      " [-1.000e+02 -1.000e+02  4.126e-02  5.000e-01  1.134e-01  2.581e-01]\n",
      " [ 2.727e-01  1.202e-01  5.198e-01  3.499e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.768e-01  1.203e-01  5.219e-01  3.308e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.807e-01  1.204e-01  5.239e-01  3.131e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.723e-01  1.202e-01  5.196e-01  3.521e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.177e-02  4.994e-01  1.134e-01  2.571e-01]\n",
      " [-1.000e+02 -1.000e+02  3.797e-02  5.036e-01  1.136e-01  2.644e-01]\n",
      " [-1.000e+02 -1.000e+02  3.998e-02  5.014e-01  1.134e-01  2.605e-01]\n",
      " [-1.000e+02 -1.000e+02  4.195e-02  4.992e-01  1.134e-01  2.568e-01]\n",
      " [ 2.717e-01  1.202e-01  5.193e-01  3.552e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.747e-01  1.202e-01  5.208e-01  3.406e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.777e-01  1.203e-01  5.223e-01  3.268e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.806e-01  1.204e-01  5.238e-01  3.139e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.773e-01  1.203e-01  5.222e-01  3.285e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.742e-01  1.202e-01  5.206e-01  3.429e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.713e-01  1.202e-01  5.191e-01  3.570e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.230e-02  4.989e-01  1.134e-01  2.562e-01]\n",
      " [-1.000e+02 -1.000e+02  4.080e-02  5.005e-01  1.134e-01  2.589e-01]\n",
      " [-1.000e+02 -1.000e+02  3.938e-02  5.020e-01  1.135e-01  2.617e-01]\n",
      " [-1.000e+02 -1.000e+02  4.091e-02  5.004e-01  1.134e-01  2.587e-01]\n",
      " [-1.000e+02 -1.000e+02  4.241e-02  4.988e-01  1.134e-01  2.560e-01]\n",
      " [ 2.709e-01  1.202e-01  5.189e-01  3.588e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.733e-01  1.202e-01  5.201e-01  3.473e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.756e-01  1.202e-01  5.213e-01  3.364e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.779e-01  1.203e-01  5.224e-01  3.259e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.754e-01  1.202e-01  5.212e-01  3.374e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.730e-01  1.202e-01  5.199e-01  3.487e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.707e-01  1.202e-01  5.188e-01  3.598e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.262e-02  4.986e-01  1.134e-01  2.556e-01]\n",
      " [-1.000e+02 -1.000e+02  4.141e-02  4.998e-01  1.134e-01  2.578e-01]\n",
      " [-1.000e+02 -1.000e+02  4.025e-02  5.011e-01  1.134e-01  2.600e-01]\n",
      " [-1.000e+02 -1.000e+02  4.148e-02  4.997e-01  1.134e-01  2.577e-01]\n",
      " [-1.000e+02 -1.000e+02  4.269e-02  4.985e-01  1.134e-01  2.554e-01]\n",
      " [-1.000e+02 -1.000e+02  4.506e-02  4.961e-01  1.134e-01  2.512e-01]\n",
      " [ 2.724e-01  1.202e-01  5.196e-01  3.516e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.743e-01  1.202e-01  5.206e-01  3.425e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.762e-01  1.202e-01  5.216e-01  3.338e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.780e-01  1.203e-01  5.225e-01  3.254e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.760e-01  1.202e-01  5.215e-01  3.347e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.740e-01  1.202e-01  5.205e-01  3.439e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.721e-01  1.202e-01  5.195e-01  3.530e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.495e-02  4.962e-01  1.134e-01  2.514e-01]\n",
      " [-1.000e+02 -1.000e+02  4.286e-02  4.983e-01  1.134e-01  2.551e-01]\n",
      " [-1.000e+02 -1.000e+02  4.186e-02  4.993e-01  1.134e-01  2.570e-01]\n",
      " [-1.000e+02 -1.000e+02  4.288e-02  4.983e-01  1.134e-01  2.551e-01]\n",
      " [-1.000e+02 -1.000e+02  4.487e-02  4.963e-01  1.134e-01  2.515e-01]\n",
      " [ 2.718e-01  1.202e-01  5.193e-01  3.545e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.734e-01  1.202e-01  5.202e-01  3.468e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.750e-01  1.202e-01  5.210e-01  3.393e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.766e-01  1.203e-01  5.218e-01  3.320e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.749e-01  1.202e-01  5.209e-01  3.399e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.732e-01  1.202e-01  5.200e-01  3.478e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.716e-01  1.202e-01  5.192e-01  3.555e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.479e-02  4.964e-01  1.134e-01  2.517e-01]\n",
      " [-1.000e+02 -1.000e+02  4.300e-02  4.982e-01  1.134e-01  2.549e-01]\n",
      " [-1.000e+02 -1.000e+02  4.214e-02  4.990e-01  1.134e-01  2.564e-01]\n",
      " [-1.000e+02 -1.000e+02  4.302e-02  4.981e-01  1.134e-01  2.548e-01]\n",
      " [-1.000e+02 -1.000e+02  4.474e-02  4.964e-01  1.134e-01  2.518e-01]\n",
      " [ 2.713e-01  1.202e-01  5.191e-01  3.567e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.741e-01  1.202e-01  5.205e-01  3.434e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.727e-01  1.202e-01  5.198e-01  3.503e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.712e-01  1.202e-01  5.191e-01  3.572e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.469e-02  4.965e-01  1.134e-01  2.519e-01]\n",
      " [-1.000e+02 -1.000e+02  4.309e-02  4.981e-01  1.134e-01  2.547e-01]\n",
      " [-1.000e+02 -1.000e+02  4.466e-02  4.965e-01  1.134e-01  2.519e-01]\n",
      " [ 2.711e-01  1.202e-01  5.190e-01  3.579e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.724e-01  1.202e-01  5.196e-01  3.517e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.710e-01  1.202e-01  5.189e-01  3.584e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.462e-02  4.965e-01  1.134e-01  2.520e-01]\n",
      " [-1.000e+02 -1.000e+02  4.316e-02  4.980e-01  1.134e-01  2.546e-01]\n",
      " [-1.000e+02 -1.000e+02  4.460e-02  4.966e-01  1.134e-01  2.520e-01]\n",
      " [ 2.709e-01  1.202e-01  5.189e-01  3.590e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.721e-01  1.202e-01  5.195e-01  3.532e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.732e-01  1.202e-01  5.201e-01  3.477e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.720e-01  1.202e-01  5.194e-01  3.535e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.708e-01  1.202e-01  5.188e-01  3.593e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.456e-02  4.966e-01  1.134e-01  2.521e-01]\n",
      " [-1.000e+02 -1.000e+02  4.321e-02  4.979e-01  1.134e-01  2.545e-01]\n",
      " [-1.000e+02 -1.000e+02  4.454e-02  4.966e-01  1.134e-01  2.521e-01]\n",
      " [ 2.707e-01  1.202e-01  5.188e-01  3.600e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.452e-02  4.966e-01  1.134e-01  2.521e-01]\n",
      " [-1.000e+02 -1.000e+02  4.452e-02  4.966e-01  1.134e-01  2.522e-01]\n",
      " [-1.000e+02 -1.000e+02  4.514e-02  4.960e-01  1.134e-01  2.511e-01]\n",
      " [ 2.727e-01  1.202e-01  5.198e-01  3.502e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.737e-01  1.202e-01  5.203e-01  3.454e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.726e-01  1.202e-01  5.198e-01  3.505e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.716e-01  1.202e-01  5.192e-01  3.556e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.509e-02  4.961e-01  1.134e-01  2.511e-01]\n",
      " [-1.000e+02 -1.000e+02  4.448e-02  4.967e-01  1.133e-01  2.522e-01]\n",
      " [-1.000e+02 -1.000e+02  4.447e-02  4.967e-01  1.133e-01  2.522e-01]\n",
      " [ 2.714e-01  1.202e-01  5.192e-01  3.562e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.734e-01  1.202e-01  5.201e-01  3.470e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.724e-01  1.202e-01  5.196e-01  3.518e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.714e-01  1.202e-01  5.191e-01  3.566e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.501e-02  4.962e-01  1.134e-01  2.513e-01]\n",
      " [-1.000e+02 -1.000e+02  4.444e-02  4.967e-01  1.133e-01  2.523e-01]\n",
      " [-1.000e+02 -1.000e+02  4.444e-02  4.967e-01  1.133e-01  2.523e-01]\n",
      " [-1.000e+02 -1.000e+02  4.499e-02  4.962e-01  1.134e-01  2.513e-01]\n",
      " [ 2.713e-01  1.202e-01  5.191e-01  3.571e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.722e-01  1.202e-01  5.195e-01  3.527e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.731e-01  1.202e-01  5.200e-01  3.484e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.712e-01  1.202e-01  5.190e-01  3.575e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.441e-02  4.967e-01  1.133e-01  2.524e-01]\n",
      " [-1.000e+02 -1.000e+02  4.336e-02  4.978e-01  1.133e-01  2.542e-01]\n",
      " [-1.000e+02 -1.000e+02  4.440e-02  4.968e-01  1.133e-01  2.524e-01]\n",
      " [-1.000e+02 -1.000e+02  4.491e-02  4.963e-01  1.134e-01  2.515e-01]\n",
      " [ 2.711e-01  1.202e-01  5.190e-01  3.581e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.719e-01  1.202e-01  5.194e-01  3.540e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.736e-01  1.202e-01  5.202e-01  3.460e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.727e-01  1.202e-01  5.198e-01  3.502e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.718e-01  1.202e-01  5.194e-01  3.544e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.710e-01  1.202e-01  5.189e-01  3.585e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.486e-02  4.963e-01  1.134e-01  2.516e-01]\n",
      " [-1.000e+02 -1.000e+02  4.437e-02  4.968e-01  1.133e-01  2.524e-01]\n",
      " [-1.000e+02 -1.000e+02  4.340e-02  4.978e-01  1.133e-01  2.541e-01]\n",
      " [-1.000e+02 -1.000e+02  4.436e-02  4.968e-01  1.133e-01  2.524e-01]\n",
      " [-1.000e+02 -1.000e+02  4.483e-02  4.963e-01  1.134e-01  2.516e-01]\n",
      " [ 2.709e-01  1.202e-01  5.189e-01  3.590e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.724e-01  1.202e-01  5.197e-01  3.515e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.724e-01  1.202e-01  5.196e-01  3.517e-02 -1.000e+02 -1.000e+02]\n",
      " [ 2.708e-01  1.202e-01  5.188e-01  3.594e-02 -1.000e+02 -1.000e+02]\n",
      " [-1.000e+02 -1.000e+02  4.433e-02  4.968e-01  1.133e-01  2.525e-01]\n",
      " [-1.000e+02 -1.000e+02  4.344e-02  4.977e-01  1.133e-01  2.541e-01]\n",
      " [-1.000e+02 -1.000e+02  4.344e-02  4.977e-01  1.133e-01  2.541e-01]\n",
      " [-1.000e+02 -1.000e+02  4.432e-02  4.968e-01  1.133e-01  2.525e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = 'Extr.1'\n",
    "Experiment = 'Pure_1'\n",
    "pd.DataFrame(payoff_matrix).to_csv('Extr.1 Payoff Matrix'+ str(Experiment)+'.csv')\n",
    "with open('Result'+str(Network)+'.'+str(Experiment)+'.txt', \"a\") as f:\n",
    "#     print(Experiment_note, file=f)\n",
    "    print('Initial Condition -(agent, opinion, pol)', file=f)\n",
    "    print('Innate op'+str(s),file=f)\n",
    "    print('Adjacency matrix'+ str(G), file=f)\n",
    "    print('Max:'+ str(First_max), file=f)\n",
    "    print('Min' + str(First_min), file=f)\n",
    "\n",
    "    print(\"In the Last 100 Rounds\", file=f) \n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # MAX distribution of LAST 100 iteration \n",
    "    print('Max_distribution', file=f)  \n",
    "    max_l100_fre = max_history_last_100/100\n",
    "    print(max_l100_fre [np.nonzero(max_l100_fre)], file=f)\n",
    "    # print for small network\n",
    "    #print(max_history_last_100, file=f)\n",
    "    # # Print for Large Network\n",
    "    print(np.nonzero(max_l100_fre),file=f)\n",
    "\n",
    "    # MIN Strategy in the last 100 round\n",
    "    counter=collections.Counter(min_touched_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    # print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/100 #return only frequency of all min options in order\n",
    "#     print('Min_frequency', file=f)\n",
    "#     print(list(counter.keys()), file=f)\n",
    "    print('Min_distribution_last_100', file=f)\n",
    "    print(fla_min_fre, file=f)\n",
    "    counter_h=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_h, file=f)\n",
    "    \n",
    "    print('min_recent_'+str(memory)+'_touched', file=f)# then stop at Game 202\n",
    "    print(min_touched, file=f)\n",
    "    print('max_recent_'+str(memory)+'_touched', file=f)\n",
    "    print(max_touched, file=f)\n",
    "    \n",
    "    print('In Overall'+str(Game_rounds)+' Rounds', file=f)\n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # Max action Overall \n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    max_fre = max_history/Game_rounds\n",
    "#     print('Max_frequency', file=f)\n",
    "#     print(max_history, file=f)\n",
    "    print('Max_distribution', file=f)\n",
    "    print(max_fre [np.nonzero(max_fre)], file=f)\n",
    "    print(np.nonzero(max_fre),file=f)\n",
    "\n",
    "\n",
    "    # Min Strategy in the Overall    \n",
    "    counter_1=collections.Counter(min_touched_all)  #return a dictionary include {'min_option': count of this choice}\n",
    "    fla_min_fre_all = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "    print('Min_dist_all', file=f)\n",
    "    print(fla_min_fre_all, file=f)\n",
    "    print('Min_distribution', file=f)\n",
    "    counter_a=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_a, file=f)\n",
    "#     print(payoff_matrix, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(6, 0.0): 358, (29, 1.0): 37, (29, 0.9999999999999999): 5, (13, 1): 1})\n",
      "fla_min_fre\n",
      "[0.002 0.893 0.092 0.012]\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(min_history) \n",
    "print(counter)\n",
    "fla_min_fre = np.array(list(counter.values()))/Game_rounds\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
