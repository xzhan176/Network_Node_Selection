{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real datanetwork with small sythetic network\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n",
    "import scipy.optimize\n",
    "import os\n",
    "import os.path\n",
    "from math import comb\n",
    "from game import Game, GameResult\n",
    "\n",
    "# sys.path.append('../gameutils')\n",
    "from utils import *\n",
    "\n",
    "save_path = os.getcwd()\n",
    "\n",
    "calculate_polarization = calculate_polarization1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Fixed initial condition + memeory = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #################### Import Sythetic Network Data\n",
    "df = pd.read_csv (save_path+'/data/5Adj_matrix.csv', header=None)\n",
    "G = np.array(df[df.columns[:]])\n",
    "# # # print(G)\n",
    "df1 = pd.read_csv(save_path+'/data/5innate_op.csv', header = None)\n",
    "s = np.array(df1[df1.columns[:]])\n",
    "\n",
    "# print(s)\n",
    "# print(G.shape)\n",
    "\n",
    "# # Set n according to the data\n",
    "n = len(s[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[[0.231]\n",
      " [0.887]\n",
      " [0.641]\n",
      " [0.218]\n",
      " [0.162]]\n"
     ]
    }
   ],
   "source": [
    "################## Process the Network Data\n",
    "L = scipy.sparse.csgraph.laplacian(G, normed=False)  # Return the Laplacian matrix\n",
    "A = np.linalg.inv(np.identity(n) + L)  # A = (I + L)^(-1)\\n  Stanford paper theory\n",
    "m = num_edges(L, n)                    # call the function to calculate the number of edges\n",
    "# what the twitter graph looks like\n",
    "nxG = nx.from_numpy_array(G)\n",
    "#plt.figure(figsize=(20, 20))\n",
    "# nx.draw(nxG).\n",
    "columnsum_ij = np.sum(A, axis=0)\n",
    "print(columnsum_ij)\n",
    "# s = [0.5]*n\n",
    "# s = np.reshape(s, (n, 1))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x2000 with 0 Axes>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# what the twitter graph looks like\n",
    "s_use = s.flatten()   # Convert array to a list for later operation\n",
    "s_use = s_use.tolist()\n",
    "new_s = [i * 30 for i in s_use]\n",
    "df = pd.DataFrame(new_s, columns=['Opinion']) #create a dataframe with index at column 1, opinion at column 2\n",
    "#_______________________________________________________\n",
    "\n",
    "nxG = nx.from_numpy_array(G)\n",
    "# nxG = nx.relabel_nodes(nxG, mapping)\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "\n",
    "def node_edge(G, n):\n",
    "    edges =[]\n",
    "    for v in range(n):\n",
    "        a = np.array(np.nonzero(G[v])[0])\n",
    "        edge = len(a)\n",
    "#         print(edge)\n",
    "        edges.append(edge)\n",
    "\n",
    "    return edges\n",
    "\n",
    "node_edges = node_edge(G, n)\n",
    "# print(node_edges)\n",
    "\n",
    "node_sizes =[]\n",
    "for i in node_edges:\n",
    "    node_size = 1/i*20000\n",
    "    node_sizes.append(node_size)\n",
    "\n",
    "min_equi = []\n",
    "max_equi = []\n",
    "# Create node color\n",
    "color_map = []\n",
    "for node in nxG:\n",
    "    if node in min_equi:\n",
    "        color_map.append('orange')\n",
    "    elif node in max_equi:\n",
    "        color_map.append('red')\n",
    "    else:\n",
    "        color_map.append('grey')\n",
    "# print(node_sizes)\n",
    "#-_______________________________________\n",
    "# what the twitter graph looks like\n",
    "nxG = nx.from_numpy_array(G)\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# # Fix seed - fix network shape\n",
    "# my_pos = nx.spring_layout(nxG, seed = 2)\n",
    "# nx.draw(nxG, pos= my_pos, with_labels= True, node_color=df['Opinion'].astype(int),cmap=plt.cm.Blues, node_size= node_sizes, edge_color='black', width=0.8, font_color='black',font_size=26, font_weight='bold', alpha=0.8)\n",
    "# #nx.draw(nxG, pos = my_pos, with_labels=False, node_color=color_map, node_size= node_sizes, edge_color='grey', width=0.5, font_color='white',font_size=9, font_weight='bold')\n",
    "# sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin = 0, vmax=1))\n",
    "# cbar = plt.colorbar(sm, shrink = 0.5)\n",
    "# tick_font_size = 24\n",
    "# cbar.ax.tick_params(labelsize=tick_font_size)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Equilibrium & Polarization \n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\t0.41022891110361115\n",
      "Equi_polarization:\t0.04411213470472514\n",
      "Difference:\t\t-0.366116776398886\n"
     ]
    }
   ],
   "source": [
    "# Calculate network polarization\n",
    "calculate_polarization(s, n, A, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Parameters\n",
    "game_rounds = 3\n",
    "memory = 10\n",
    "k = 2\n",
    "experiment = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes, opinions\n",
      "[0, 3] (1, 1)\n",
      "Nodes, opinions\n",
      "[1, 4] (0.5, 0.5)\n",
      "min_history [((1, 4), (0.5, 0.5))]\n",
      "--------------------\n",
      "Game 1\n",
      "----------\n",
      "min_history [((1, 4), (0.5, 0.5))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "fla_min_fre [1.]\n",
      "Max_por 0.033797021050447185\n",
      "Max_por 0.033797021050447185\n",
      "Max_por 0.033797021050447185\n",
      "all_por [0.    0.    0.    0.    0.028 0.022 0.022 0.034 0.016 0.019 0.024 0.016\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.019 0.028 0.02  0.018 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.   ]\n",
      "column - best action: 7\n",
      "Maximizer found its target 2 agent: [0, 2] op: (1, 1)\n",
      "fre_max at spot 0.5\n",
      "M\n",
      "[[ 0.159 -0.041  0.159 -0.038 -0.241]]\n",
      "M\n",
      "[[ 0.139 -0.061  0.068  0.079 -0.225]]\n",
      "champion:  [1, 4] [0.834 0.877] 0.008232769862068768\n",
      "Minimizer finds its target agents: [1, 4]\n",
      "fla_min_fre: [0.5 0.5]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.008232769862068768 and Equi_Max = 0.033797021050447185\n",
      "--------------------\n",
      "Game 2\n",
      "----------\n",
      "min_history [((1, 4), (0.5, 0.5)), ((1, 4), (0.8341762737020206, 0.8768075818888889))]\n",
      "max_history [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.5 0.5]\n",
      "Max_por 0.0571393114565627\n",
      "Max_por 0.0571393114565627\n",
      "Max_por 0.0571393114565627\n",
      "all_por [0.    0.    0.    0.    0.057 0.031 0.031 0.023 0.033 0.033 0.021 0.01\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.045 0.05  0.026 0.019 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.   ]\n",
      "column - best action: 4\n",
      "Maximizer found its target 2 agent: [0, 2] op: (0, 0)\n",
      "fre_max at spot 0.3333333333333333\n",
      "M\n",
      "[[-0.007 -0.007 -0.007  0.029 -0.007]]\n",
      "M\n",
      "[[ 0.159 -0.041  0.159 -0.038 -0.241]]\n",
      "M\n",
      "[[ 0.139 -0.061  0.068  0.079 -0.225]]\n",
      "champion:  [1, 4] [0.574 0.593] 0.024873854271851153\n",
      "Minimizer finds its target agents: [1, 4]\n",
      "fla_min_fre: [0.333 0.333 0.333]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.024873854271851153 and Equi_Max = 0.0571393114565627\n",
      "--------------------\n",
      "Game 3\n",
      "----------\n",
      "min_history [((1, 4), (0.5, 0.5)), ((1, 4), (0.8341762737020206, 0.8768075818888889)), ((1, 4), (0.5744406402121215, 0.5926005626666666))]\n",
      "max_history [0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.333 0.333 0.333]\n",
      "Max_por 0.051122796437529566\n",
      "Max_por 0.051122796437529566\n",
      "Max_por 0.051122796437529566\n",
      "all_por [0.    0.    0.    0.    0.051 0.029 0.029 0.024 0.029 0.03  0.02  0.01\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.04  0.045 0.024 0.018 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.   ]\n",
      "column - best action: 4\n",
      "Maximizer found its target 2 agent: [0, 2] op: (0, 0)\n",
      "fre_max at spot 0.5\n",
      "M\n",
      "[[-0.007 -0.007 -0.007  0.029 -0.007]]\n",
      "M\n",
      "[[ 0.159 -0.041  0.159 -0.038 -0.241]]\n",
      "M\n",
      "[[ 0.139 -0.061  0.068  0.079 -0.225]]\n",
      "champion:  [1, 4] [0.445 0.45 ] 0.02604455021726012\n",
      "Minimizer finds its target agents: [1, 4]\n",
      "fla_min_fre: [0.25 0.25 0.25 0.25]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.02604455021726012 and Equi_Max = 0.051122796437529566\n",
      "MAX_last_100,  all\n",
      "[0.02 0.01] [0.667 0.333 0.333]\n",
      "[4 7] [ 4  7 11]\n",
      "Max Nodes: [0, 2] | Opinion: (0, 0)\n",
      "Max Nodes: [0, 2] | Opinion: (1, 1)\n",
      "MIN_last_100,  all\n",
      "[0.03] [1.333]\n",
      "Counter({(1, 4): 3}) Counter({(1, 4): 4})\n",
      "Max Pol: 0.051122796437529566 Min Pol: 0.02604455021726012\n"
     ]
    }
   ],
   "source": [
    "game = Game(s, A, L, k)\n",
    "result = game.run(game_rounds, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(result: GameResult, k, experiment):\n",
    "    pd.DataFrame(result.payoff_matrix).to_csv(\n",
    "        f'results/Payoff-Matrix-k-{k}-experiment-{experiment}.csv')\n",
    "\n",
    "    # Save the original standard output\n",
    "    original_stdout = sys.stdout\n",
    "\n",
    "    with open(f'results/Result-k-{k}-experiment-{experiment}.txt', \"w\") as f:\n",
    "        # Change the standard output to the file we created.\n",
    "        sys.stdout = f\n",
    "\n",
    "        print('Initial Condition -(agent, opinion, pol)')\n",
    "        # print(f'Innate op {s}')\n",
    "        # print(f'Adjacency matrix {G}')\n",
    "        # print('Selected Nodeset, k_Opinions, Steady-state polarization')\n",
    "        print(f'Max:\\t{result.first_max}')\n",
    "        print(f'Min:\\t{result.first_min}')\n",
    "\n",
    "        print('_____________________')\n",
    "        print(f'Max Pol:\\t{result.equi_max}')\n",
    "        print(f'Min Pol:\\t{result.equi_min}')\n",
    "\n",
    "        # MAXimizer's distribution of LAST 100 iteration\n",
    "        print('Max_distribution_last_100')\n",
    "        max_l100_fre = result.max_history_last_100/100\n",
    "        print(max_l100_fre[np.nonzero(max_l100_fre)])\n",
    "        # print for small network\n",
    "        # print(max_history_last_100)\n",
    "        # # Print for Large Network\n",
    "        print(np.nonzero(max_l100_fre))\n",
    "\n",
    "        columns = np.nonzero(max_l100_fre)\n",
    "        columns = list(columns[0])\n",
    "        for column in columns:\n",
    "            (k_nodes, opinions) = game.map_action(column)\n",
    "            print(f'Max Nodes: {k_nodes}\\tOpinion: {opinions}')\n",
    "\n",
    "        print('Max_distribution_all')\n",
    "        max_fre = result.max_history/result.game_rounds\n",
    "        print(max_fre[np.nonzero(max_fre)])\n",
    "        print([np.nonzero(max_fre)])\n",
    "\n",
    "        # TODO confirm isn't this block the same as the one above\n",
    "        columns_all = np.nonzero(max_l100_fre)\n",
    "        columns_all = list(columns_all[0])\n",
    "        for column in columns_all:\n",
    "            (k_nodes, opinions) = game.map_action(column)\n",
    "            print(f'Max Nodes: {k_nodes}\\tOpinion: {opinions}')\n",
    "\n",
    "        # MINimizer's Strategy in the last 100 round\n",
    "        counter = collections.Counter(result.min_touched_last_100)\n",
    "        # frequency of all min options in order\n",
    "        fla_min_fre = np.array(list(counter.values()))/(100)\n",
    "        print('Min_distribution_last_100')\n",
    "        print(fla_min_fre)\n",
    "        print(counter)\n",
    "        # print(min_touched_last_100)\n",
    "\n",
    "        # a dictionary include {'min_option': count of this choice}\n",
    "        counter_1 = collections.Counter(result.min_touched_all)\n",
    "        # frequency of all min options in order\n",
    "        fla_min_fre_1 = np.array(list(counter_1.values()))/result.game_rounds\n",
    "        print('Min_distribution_all')\n",
    "        print(fla_min_fre_1)\n",
    "        print(counter_1)\n",
    "        np.set_printoptions(precision=3)\n",
    "\n",
    "        # a dictionary include {'min_option': count of this choice}\n",
    "        counter_a = collections.Counter(result.min_history)\n",
    "        print(counter_a)\n",
    "\n",
    "        print(f'min_recent_{memory}_touched')\n",
    "        print(result.min_touched)\n",
    "        print(f'max_recent_{memory}_touched')\n",
    "        print(result.max_touched)\n",
    "\n",
    "        # Reset the standard output to its original value\n",
    "        sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(result, k, experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
