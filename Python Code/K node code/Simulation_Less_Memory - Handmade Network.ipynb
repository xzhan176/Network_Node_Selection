{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real datanetwork with small sythetic network\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "# %run Check_Derivation_of_Two_Opinions.ipynb\n",
    "#%run pure_strategy_selection.ipynb  #include simple selection algorithm\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n",
    "from itertools import product\n",
    "import scipy.optimize\n",
    "import csv\n",
    "import os\n",
    "import os.path\n",
    "from math import comb\n",
    "\n",
    "save_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Fixed initial condition + memeory = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers the opinion vector around 0\\n\",\n",
    "def mean_center(op, n):\n",
    "    ones = np.ones((n, 1))\n",
    "    x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "    return x\n",
    "\n",
    "# compute number of edges, m\\n\n",
    "def num_edges(L, n):\n",
    "    m = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i > j and L[i,j] < 0:\n",
    "                m += 1\n",
    "    return m\n",
    "\n",
    "# Compute number of possible actions\n",
    "def len_actions(k, n):\n",
    "    # create all combination of K opinions\n",
    "    max_option = [0, 1]\n",
    "\n",
    "    k_opinions =list(product(max_option, repeat=k))  # All k opinion combinations\n",
    "    len_kops = len(k_opinions) # - number of combinations exist\n",
    "    # Horizontal length of all possible actions\n",
    "    h = comb(n,k) * len_kops\n",
    "    return h\n",
    "\n",
    "\n",
    "# maximizing polarization only: \\\\bar{z}^T \\\\bar{z}\n",
    "def obj_polarization(A, L, op, n):\n",
    "    op_mean = mean_center(op, n)\n",
    "    z_mean = np.dot(A, op_mean)\n",
    "    return np.dot(np.transpose(z_mean), z_mean)[0,0]\n",
    "\n",
    "# def obj_polarization_1(A, L, op, n):  #z_mean is the same as s_mean - according to Stanford paper theory\n",
    "#     z = np.dot(A, op)\n",
    "#     z_mean = mean\n",
    "#     return np.dot(np.transpose(z_mean), z_mean)[0,0]\n",
    "\n",
    "# Calculate innate polarization\n",
    "def obj_innate_polarization(s, n):\n",
    "#     np.set_printoptions(precision=5)\n",
    "    op_mean = mean_center(s, n)\n",
    "    return np.dot(np.transpose(op_mean), op_mean)[0,0]\n",
    "\n",
    "np.set_printoptions(precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3\n",
    "# n = 20\n",
    "# # #comb(n,k)\n",
    "# h = len_actions(k, n)\n",
    "# print(h)\n",
    "def cgen(i,n,k):\n",
    "    \"\"\"\n",
    "    returns the i-th combination of k numbers chosen from 1,2,...,n\n",
    "    \"\"\"\n",
    "    c = []\n",
    "    r = i+0\n",
    "    j = -1\n",
    "    for s in range(1,k+1):\n",
    "        j = j+1\n",
    "        while r-comb(n-1-j,k-s)>=0:\n",
    "            r -= comb(n-1-j,k-s)\n",
    "            j += 1\n",
    "        c.append(j)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all combination\n",
    "def cgen(i, n, k):\n",
    "    \"\"\"\n",
    "    returns the i-th combination of k numbers chosen from 1,2,...,n\n",
    "    \"\"\"\n",
    "    c = []\n",
    "    r = i+0\n",
    "    j = -1\n",
    "    for s in range(1, k+1):\n",
    "        cs = j+1\n",
    "        while r-comb(n-1-cs, k-s) >= 0:\n",
    "            r -= comb(n-1-cs, k-s)\n",
    "            cs += 1\n",
    "        c.append(cs)\n",
    "        j = cs\n",
    "    return c\n",
    "\n",
    "\n",
    "# input representative k_nodes, nodes needs to be excluded, return the real k nodes that are available\n",
    "def convert_available(k_nodes, touched, k):\n",
    "    # touched = list(set(touched)) #[2,4,6,8] unqie values of touched\n",
    "    touched.sort()\n",
    "    for i in touched:\n",
    "        for j in range(k):  # [2,3,4,5,6,7,8,9]\n",
    "            if k_nodes[j] >= i:\n",
    "                k_nodes[j] = k_nodes[j] + 1\n",
    "    return k_nodes\n",
    "\n",
    "# create available combination of K nodes\n",
    "\n",
    "\n",
    "def creat_available_comb(i_th, n, k, touched):\n",
    "    a = len(set(touched))  # number of unqiue touched nodes\n",
    "    # len_nodesets = comb(n-a,k) #  number of available combination of k nodes\n",
    "\n",
    "    k_fake = cgen(i_th, n-a, k)  # generate the i-th list from total n-a agents\n",
    "    # convert the i-th list to real k nodes\n",
    "    k_nodes = convert_available(k_fake, touched, k)\n",
    "    print(\n",
    "        f'DEBUG2 k_fake={k_fake} k_nodes={k_nodes} touched={touched} n={n} k={k} a={a} i_th={i_th}')\n",
    "    return k_nodes\n",
    "\n",
    "# def creat_all_comb(i_th, n, k):\n",
    "# ########### create all combination of K opinions\n",
    "#     max_option = [0, 1]\n",
    "#     k_opinions =[]\n",
    "#     k_opinions = product(max_option, repeat=k)  # - all k opinion combinations\n",
    "\n",
    "#     k_nodes = cgen(i_th,n,k)\n",
    "\n",
    "#     return(k_nodes, k_opinions)\n",
    "\n",
    "\n",
    "def creat_all_comb(n, k):\n",
    "    # create all combination of K opinions\n",
    "    max_option = [0, 1]\n",
    "    k_opinions = []\n",
    "    k_opinions = product(max_option, repeat=k)  # - all k opinion combinations\n",
    "    return list(k_opinions)\n",
    "\n",
    "\n",
    "# node_set - 1 set  k_opinion- 1 set\n",
    "def change_k_innate_opinion(s, node_set, k_opinion):\n",
    "    op = copy.copy(s)  # make a copy of the innate opinion array\n",
    "\n",
    "    for j in range(k):\n",
    "        b = node_set[j]  # b - agent index\n",
    "        op[b] = k_opinion[j]   # j - index of which opinion combination\n",
    "\n",
    "    return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_erdos_renyi_network(n, p, u, v):\n",
    "    A = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "#         for j in range(i+1,n):\n",
    "#             r = np.random.rand()\n",
    "#             if r < p:\n",
    "            if j<i:\n",
    "                A[i,j] = A[j,i] = 1\n",
    "#             if i==u or i==v:\n",
    "#                 A[i,j] = A[j,i] = 1\n",
    "    return A\n",
    "\n",
    "\n",
    "# Create a synthetic Erdős-Rényi network with 10 nodes and edge probability 0.3\n",
    "# p = 0.3\n",
    "# u=1\n",
    "# v=2\n",
    "\n",
    "# print(s)\n",
    "# # Print the adjacency matrix\n",
    "# G = create_erdos_renyi_network(n, p, u, v)\n",
    "# print(G)\n",
    "\n",
    "def make_innat_opinions(n): # Make opinion for agents only - no info source is involved\n",
    "\n",
    "    # Make list of ind innate opinion to define info source opinion\n",
    "    innat_s = np.random.uniform(low=0, high=1, size=int(n))   #individual's innate opinion\n",
    "\n",
    "    s = np.zeros((n, 1))\n",
    "\n",
    "    idx1 = 0\n",
    "    for i in range(len(s)):\n",
    "        s[i] = innat_s[idx1]  #set innate opinion for ind.\n",
    "        idx1 += 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #################### Import Sythetic Network Data\n",
    "df = pd.read_csv (save_path+'/data/5Adj_matrix.csv', header=None)\n",
    "G = np.array(df[df.columns[:]])\n",
    "# # # print(G)\n",
    "df1 = pd.read_csv(save_path+'/data/5innate_op.csv', header = None)\n",
    "s = np.array(df1[df1.columns[:]])\n",
    "\n",
    "# print(s)\n",
    "# print(G.shape)\n",
    "\n",
    "# # Set n according to the data\n",
    "n = len(s[:])\n",
    "# # print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#n = 20 # Number of nodes\n",
    "\n",
    "# # Create empty graph\n",
    "# nxG = nx.Graph()\n",
    "\n",
    "# # Add nodes to graph\n",
    "# nxG.add_nodes_from(range(n))\n",
    "\n",
    "# # Add edges based on node index\n",
    "# for i in range(n):\n",
    "#     for j in range(i):\n",
    "#         p = (n-i)/(n-j) # Probability of edge existing\n",
    "#         if np.random.random() < p:\n",
    "#             nxG.add_edge(i, j)\n",
    "\n",
    "# # Draw graph\n",
    "# nx.draw(nxG, with_labels=True)\n",
    "# plt.show()\n",
    "# G = nx.adjacency_matrix(nxG).todense()\n",
    "# print(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[[0.23068]\n",
      " [0.8871 ]\n",
      " [0.64121]\n",
      " [0.21768]\n",
      " [0.16186]]\n"
     ]
    }
   ],
   "source": [
    "################## Process the Network Data\n",
    "L = scipy.sparse.csgraph.laplacian(G, normed=False)  # Return the Laplacian matrix\n",
    "A = np.linalg.inv(np.identity(n) + L)  # A = (I + L)^(-1)\\n  Stanford paper theory\n",
    "m = num_edges(L, n)                    # call the function to calculate the number of edges\n",
    "# what the twitter graph looks like\n",
    "nxG = nx.from_numpy_array(G)\n",
    "#plt.figure(figsize=(20, 20))\n",
    "# nx.draw(nxG).\n",
    "columnsum_ij = np.sum(A, axis=0)\n",
    "print(columnsum_ij)\n",
    "# s = [0.5]*n\n",
    "# s = np.reshape(s, (n, 1))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x2000 with 0 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x2000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# what the twitter graph looks like\n",
    "s_use = s.flatten()   # Convert array to a list for later operation\n",
    "s_use = s_use.tolist()\n",
    "new_s = [i * 30 for i in s_use]\n",
    "df = pd.DataFrame(new_s, columns=['Opinion']) #create a datafram with index at column 1, opinion at column 2\n",
    "#_______________________________________________________\n",
    "\n",
    "nxG = nx.from_numpy_array(G)\n",
    "# nxG = nx.relabel_nodes(nxG, mapping)\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "\n",
    "def node_edge(G, n):\n",
    "    edges =[]\n",
    "    for v in range(n):\n",
    "        a = np.array(np.nonzero(G[v])[0])\n",
    "        edge = len(a)\n",
    "#         print(edge)\n",
    "        edges.append(edge)\n",
    "\n",
    "    return edges\n",
    "\n",
    "node_edges = node_edge(G, n)\n",
    "# print(node_edges)\n",
    "\n",
    "node_sizes =[]\n",
    "for i in node_edges:\n",
    "    node_size = 1/i*20000\n",
    "    node_sizes.append(node_size)\n",
    "\n",
    "min_equi = []\n",
    "max_equi = []\n",
    "# Create node color\n",
    "color_map = []\n",
    "for node in nxG:\n",
    "    if node in min_equi:\n",
    "        color_map.append('orange')\n",
    "    elif node in max_equi:\n",
    "        color_map.append('red')\n",
    "    else:\n",
    "        color_map.append('grey')\n",
    "# print(node_sizes)\n",
    "#-_______________________________________\n",
    "# what the twitter graph looks like\n",
    "nxG = nx.from_numpy_array(G)\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# # Fix seed - fix network shape\n",
    "# my_pos = nx.spring_layout(nxG, seed = 2)\n",
    "# nx.draw(nxG, pos= my_pos, with_labels= True, node_color=df['Opinion'].astype(int),cmap=plt.cm.Blues, node_size= node_sizes, edge_color='black', width=0.8, font_color='black',font_size=26, font_weight='bold', alpha=0.8)\n",
    "# #nx.draw(nxG, pos = my_pos, with_labels=False, node_color=color_map, node_size= node_sizes, edge_color='grey', width=0.5, font_color='white',font_size=9, font_weight='bold')\n",
    "# sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin = 0, vmax=1))\n",
    "# cbar = plt.colorbar(sm, shrink = 0.5)\n",
    "# tick_font_size = 24\n",
    "# cbar.ax.tick_params(labelsize=tick_font_size)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23068]\n",
      " [0.8871 ]\n",
      " [0.64121]\n",
      " [0.21768]\n",
      " [0.16186]]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Equilibrium & Polarization \n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op = s\n",
    "# y = mean_center(s,n)\n",
    "# # print(y)\n",
    "# innat_pol = np.dot(np.transpose(y), y)[0,0]\n",
    "# print('Innate_polarization:')\n",
    "# print(innat_pol)\n",
    "\n",
    "# # Test equilibrium polarization\n",
    "# equ_pol = obj_polarization(A, L, s, n)\n",
    "# print('Equi_polarization:')\n",
    "# print(equ_pol)\n",
    "\n",
    "# di = equ_pol-innat_pol\n",
    "# print(\"Difference:\")\n",
    "# print(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_random_play(s,n,k):  # player randomly choose an agent and randomly change the agent\n",
    "\n",
    "    op = copy.copy(s)\n",
    "\n",
    "    ########### create all combination of K nodes\n",
    "    k_opinions = creat_all_comb(n, k)\n",
    "    len_nodesets = comb(n,k)\n",
    "\n",
    "    i_th = random.randint(0,len_nodesets-1)  # randomly select an agent index\n",
    "#     v_list = node_sets[v_index]\n",
    "    v_list = cgen(i_th,n,k)\n",
    "\n",
    "    ########### create all combination of K opinions\n",
    "    len_kops = len(k_opinions) # - number of combinations exist\n",
    "    op_index = random.randint(0,len_kops-1) # randomly select index for an OPINION list\n",
    "    new_op = k_opinions[op_index]  # randomly select an opininon list(0 and 1) to update the opinion array\n",
    "    print('Nodes, opinions')\n",
    "    print(v_list,new_op)\n",
    "#     print(new_op)\n",
    "    op = change_k_innate_opinion(s, v_list, new_op)\n",
    "\n",
    "    #print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "\n",
    "    column = len_kops*i_th + op_index\n",
    "#     print(\"Network reaches stead_state Polarization: \" + str(por))\n",
    "\n",
    "    return (v_list, new_op, por, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_random_play_1(s,n,k,max_touched):  # player randomly choose an agent and randomly change the agent\n",
    "\n",
    "    op = copy.copy(s)\n",
    "#     max_opi_option = random.uniform(0, 1)   # options that maximizer have\n",
    "\n",
    "    a = len(set(max_touched))  # number of unqiue touched nodes\n",
    "    len_nodesets = comb(n-a,k) #  number of available combination of k nodes\n",
    "\n",
    "    i_th = random.randint(0,len_nodesets-1)  # randomly select an action index\n",
    "    v_list = creat_available_comb(i_th, n,k,max_touched)\n",
    "\n",
    "    new_op_list = []\n",
    "    for i in range(k):\n",
    "        #new_op = random.uniform(0, 1)  # randomly select an opininon between 0 and 1\n",
    "        new_op = 0.5\n",
    "        new_op_list.append(new_op)\n",
    "\n",
    "    new_op_list = tuple(new_op_list)\n",
    "    print('Nodes, opinions')\n",
    "    print(v_list,new_op_list)\n",
    "    op = change_k_innate_opinion(s, v_list, new_op_list)\n",
    "   # print(\"    \"+\"Agent\" + str(v_list) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "#     print(\"Network reaches steady-state Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v_list, new_op_list, por)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Strategy Payoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "def make_k_payoff_row(op1, h, v2): #op1 here is only changed by Min\n",
    "    payoff_row = np.zeros(h)\n",
    "\n",
    "    column = 0\n",
    "    i = 1\n",
    "    for i in range(0, comb(n,k)):# i - which set of nodes option\n",
    "#         print('nodeset'+ str(i))\n",
    "        nodes = cgen(i,n,k)\n",
    "#         print(nodes)\n",
    "        k_opinions = creat_all_comb(n, k)\n",
    "\n",
    "        for ops in k_opinions: # tuple index - select one combinatio of opinions\n",
    "#             print('opset'+ str(j))\n",
    "#             j = j + 1\n",
    "            op2 = change_k_innate_opinion(op1, nodes, ops) # op2 has changed by both min and max now\n",
    "            check =  any(node in nodes for node in v2)\n",
    "\n",
    " # when v1 == v2, the polarization should be negative for max, infinet for min.\n",
    " # Replace the the column_index of agent v2 with 0 for max\n",
    "            if check is False:    #if v1 != v2\n",
    "                # calculate the payofflarization\n",
    "                payoff = obj_polarization(A, L, op2, n) # calculate the payofflarization\n",
    "                payoff_row[column] = payoff\n",
    "                column = column + 1\n",
    "#                 print(payoff_row)\n",
    "            else:                #if v1 == v2\n",
    "                payoff_row[column] = 10000  # use to avoid min and max choose the same agent\n",
    "                column = column + 1\n",
    "#                 print(payoff_row)\n",
    "\n",
    "#     print(payoff_row)\n",
    "\n",
    "\n",
    "    return payoff_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate polarization of minimizer's Mixed Strategy\n",
    "def mixed_K_min_polarization(s,v2,k_opinion,fla_max_fre):\n",
    "\n",
    "    op1 = change_k_innate_opinion(s, v2, k_opinion) # only updated by minimizer's current change\n",
    "    # calculate the polarization with both min(did above) and max's action(in make_payoff_row)\n",
    "    payoff_row = make_k_payoff_row(op1, h, v2)  # the vector list out 2*n payoffs after min's action combine with 2*n possible max's actions\n",
    "#     print('payoff_row')\n",
    "#     print(payoff_row)\n",
    "    #calculate fictitious payoff - equi_min\n",
    "    payoff_cal = payoff_row * fla_max_fre # fla_max_fre recorded the frequency of each maximizer's action, frequency sum = 1\n",
    "                                             # payoff (2*n array) * maximizer_action_frequency (2*n array)\n",
    "#     print('payoff_cal')\n",
    "#     print(payoff_cal)\n",
    "    mixed_pol = np.sum(payoff_cal) # add up all, calculate average/expected payoff\n",
    "#     print('Mixed_pol')\n",
    "#     print(mixed_pol)\n",
    "\n",
    "    # Replace the the column_index of agent v2 with -100 for max\n",
    "\n",
    "    payoff_row = [-10000 if ele == 10000 else ele for ele in payoff_row]\n",
    "\n",
    "\n",
    "    return (mixed_pol,payoff_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sum_term - exclude selected nodes\n",
    "# M_1= sum_term - term_out    # numerator\n",
    "def sum_rest(n, op, v2):\n",
    "    # Reshape opinion array\n",
    "    op = np.reshape(op, (n, 1))\n",
    "\n",
    "    E_new = np.array([1/n] * n * n)\n",
    "    # create a n*n matrix with all elements 1\n",
    "    E_new = np.reshape(E_new, (n, n))\n",
    "    # A_new = np.reshape(A, (n,n))\n",
    "    A_new = copy.copy(A)\n",
    "    A_temp = A_new-E_new\n",
    "    M_new_temp = A_temp@op\n",
    "\n",
    "    def sumFunction(x):\n",
    "        s_i = op[x]*A_temp[x]\n",
    "        return s_i\n",
    "    np.sum(A, axis=0)\n",
    "    Out_term = np.sum([sumFunction(x) for x in v2], axis=0)\n",
    "    Out_term = np.reshape(Out_term, (n, 1))\n",
    "    M_rest = np.transpose(M_new_temp-Out_term)\n",
    "    print('M')\n",
    "    print(M_rest)\n",
    "    return M_rest\n",
    "\n",
    "############## Derivate min_opinions - using above result#####################################\n",
    "def k_derivate_s(v2, k, n, M):\n",
    "    # k - # of selected nodes\n",
    "    # V2 - selection list(k nodes) of minimier\n",
    "    #  take the node index from selection list\n",
    "    #  it's also the column index for these two nodes\n",
    "    # op - n*1 innate opinion array that updated by maximizer\n",
    "    # A - n*n adjacency matrix\n",
    "\n",
    "    # create a parameter array with all 1/n\n",
    "    c = np.array([1/n] * n)\n",
    "    # c = np.reshape(c, (n,1))\n",
    "\n",
    "    # Create left side of '=' matrix\n",
    "    def leftFunction(x, y):\n",
    "        a_i = np.transpose(A[x]-c)@(A[y]-c)\n",
    "        return [a_i]\n",
    "    a = np.concatenate([leftFunction(x, y) for y in v2 for x in v2])\n",
    "    a = np.reshape(a, (k, k))\n",
    "    # Create right side of '=' matrix\n",
    "\n",
    "    def rightFunction(x, M):\n",
    "        Mi = np.dot(M, (A[x]-c))\n",
    "        return -Mi\n",
    "    b = np.concatenate([rightFunction(x, M) for x in v2])\n",
    "    result = np.linalg.solve(a, b)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_k_mixed_opinion(s, n, v2, fla_max_fre):\n",
    "    weight_op = 0\n",
    "    weight_M = 0\n",
    "    # loop for each max_action(in total 2*n)\n",
    "    k_opinions = creat_all_comb(n, k)\n",
    "    len_kops = len(k_opinions)\n",
    "\n",
    "    for column in range(h):\n",
    "        if fla_max_fre[column] != 0:\n",
    "            if column > k:\n",
    "                nodeset_index = int(column/len_kops)\n",
    "                opset_index = column % len_kops\n",
    "            else:\n",
    "                nodeset_index = 0\n",
    "                opset_index = column\n",
    "\n",
    "            # Calculating Max's action at this column\n",
    "            v1 = cgen(nodeset_index, n, k)\n",
    "            max_opinion = k_opinions[opset_index]\n",
    "\n",
    "            # change innate opinion by max action\n",
    "            op1 = change_k_innate_opinion(s, v1, max_opinion)\n",
    "\n",
    "            # Derivate optimal Min's opinion for nodeset v2\n",
    "            # {sum}{j}(s_j(h_j -c))  - rest of terms\n",
    "            print(f'DEBUG0 op1 {op1} v2 {v2}')\n",
    "            M_rest = sum_rest(n, op1, v2)\n",
    "\n",
    "            # {sum}{v} p_v * M\n",
    "            weight_M += fla_max_fre[column]*M_rest\n",
    "\n",
    "    # Got optimal Min's opinion for v2\n",
    "    # give a set of k weighted opinions\n",
    "    k_opinion = k_derivate_s(v2, k, n, weight_M)\n",
    "    # check if the result is matching hand derivative or not.\n",
    "    # Game 1 results should be the same because weight it's pure strategy, same result as \"check_\"\n",
    "    # print('result')\n",
    "    # print(k_opinion)\n",
    "    # #%run Check_Derivation_of_Two_Opinions.ipynb\n",
    "    # (x,y) = py_pack(A, op1, n, v2)\n",
    "    # (si,sl) = deriv_sty(A, op1, n, v2)\n",
    "\n",
    "    # print('Weighted polarization')\n",
    "    # print(mixed_por)\n",
    "\n",
    "    # When the min_op is out of range(0,1), replace it with the boundary that close to min_op\n",
    "    if any(x < 0 for x in k_opinion):\n",
    "        # min_opinion should be in the range (0,1)\n",
    "        print('Min_opinion less than 0')\n",
    "        print(k_opinion)\n",
    "        print('.')\n",
    "        print('.')\n",
    "        print('.')\n",
    "        print('.')\n",
    "    elif any(x > 1 for x in k_opinion):\n",
    "        print('Min_opinion more than 1')\n",
    "        print(k_opinion)\n",
    "        print('.')\n",
    "        print('.')\n",
    "        print('.')\n",
    "        print('.')\n",
    "        # sys.exit()\n",
    "    arry = np.array([])\n",
    "    for i in k_opinion:\n",
    "        if i < 0:\n",
    "            print(i)\n",
    "            i = 0\n",
    "            print(i)\n",
    "            arry = np.append(arry, i)\n",
    "            print(arry)\n",
    "        elif i > 1:\n",
    "            i = 1\n",
    "            arry = np.append(arry, i)\n",
    "        else:\n",
    "            arry = np.append(arry, i)\n",
    "\n",
    "    (mixed_por, payoff_row) = mixed_K_min_polarization(\n",
    "        s, v2, k_opinion, fla_max_fre)\n",
    "\n",
    "    return (arry, payoff_row, mixed_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizer search: Go through each agent\n",
    "def mixed_choose_min_vertex(s, n, max_touched, fla_max_fre):\n",
    "    min_por = 1000 # store innate max updated polarization\n",
    "    champion = (None, None, 0, None)  # assume the best action is champion\n",
    "\n",
    "    a = len(set(max_touched))\n",
    "    len_nodesets = comb(n-a, k)\n",
    "\n",
    "    for i_th in range(len_nodesets):  #---- now v2 is a set of nodes\n",
    "        v2 = creat_available_comb(i_th, n, k, max_touched)\n",
    "        print(f'DEBUG 1 {v2}')\n",
    "        (changed_opinion, payoff_row, por) =  min_k_mixed_opinion(s, n, v2, fla_max_fre) # find the best new_op option\n",
    "\n",
    "        if por < min_por:  # if the recent polarization is smaller than the minimum polarization in the history\n",
    "            min_por = por\n",
    "            champion = (v2, changed_opinion[:], payoff_row, min_por)\n",
    "            print(\"champion: \",champion[0],champion[1],champion[3])\n",
    "\n",
    "            # update the recent option as champion\n",
    "        # else:\n",
    "        #     print('Innate polarization is smaller than Min action')\n",
    "\n",
    "    return (champion)  # find the best minimizer's action after going through every new_op option of every agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Op has been updated by maximizer, fla_max_fre includes max's hisotry, so minimizer react to the innate op after that\n",
    "def mixed_min_play(s, n, max_touched, fla_max_fre):\n",
    "    min_champion = mixed_choose_min_vertex(s, n, max_touched, fla_max_fre)\n",
    "    (v2, min_opinion, payoff_row, min_pol) = min_champion\n",
    "\n",
    "    if v2 == None:    # if minimizer cannot find a action to minimize polarization after maximizer's action\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"Minimizer finds its target agents:\")\n",
    "        print(v2)\n",
    "\n",
    "        # Store innate_op of the min_selected k vertex\n",
    "        old_opinion_min = [s[i] for i in v2]\n",
    "\n",
    "        # print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "        # print('fla_max_fre')\n",
    "        # print(fla_max_fre[np.nonzero(fla_max_fre)])\n",
    "\n",
    "        # print(\"Payoff row\")\n",
    "        # print(payoff_row)\n",
    "        # print(\"Network reaches steady-state Polarization: \" + str(min_pol))\n",
    "\n",
    "    return (tuple(v2), payoff_row, min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by minimizer, fla_min_fre includes min's hisotry, so maxmizer react to the innate op after that\n",
    "def k_max_polarization(payoff_matrix,column,fla_min_fre):\n",
    "\n",
    "    # create payoff matrix for maxmizer\n",
    "    payoff_vector = payoff_matrix[:,column]\n",
    "    print('payoff_vector',payoff_vector [np.nonzero(payoff_vector)])\n",
    "#     print(fla_min_fre [np.nonzero(fla_min_fre)])\n",
    "    if any(i> 10 for i in payoff_vector) >10:\n",
    "        print('Error in Payoff Matrix')\n",
    "        sys.exit\n",
    "    #calculate fictitious payoff - equi_max\n",
    "#     print('fla_min_fre')\n",
    "#     print(fla_min_fre)\n",
    "    payoff_cal = payoff_vector * fla_min_fre #payoff * frequency\n",
    "\n",
    "    mixed_pol = np.sum(payoff_cal) # add up\n",
    "    #print('mixed_pol',mixed_pol)\n",
    "    return mixed_pol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx(k_nodes, n):\n",
    "    latter = 0\n",
    "    index = 0\n",
    "    k = len(k_nodes)\n",
    "\n",
    "    for i in range(k):\n",
    "        before = k_nodes[i] + 1\n",
    "        L_min = latter + 1\n",
    "        L_max = before - 1\n",
    "\n",
    "        M = L_max - L_min\n",
    "\n",
    "        for m in range(1, M+2):\n",
    "            P = n - latter - m\n",
    "            L = k - 1 - i\n",
    "            index = index + comb(P, L)\n",
    "        latter = before\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pass on the innate opinion that has been changed by minimizer\n",
    "# - op1 - innate opinion that has been changed by minimizer\n",
    "def max_k_play(payoff_matrix, n, k, fla_min_fre, min_touched):\n",
    "    k_opinions = creat_all_comb(n, k)\n",
    "    len_kops = len(k_opinions)\n",
    "\n",
    "    ############ start producing changes ###########\n",
    "    all_por = np.zeros(h)\n",
    "    print('fla_min_fre', fla_min_fre[np.nonzero(fla_min_fre)])\n",
    "    a = len(set(min_touched))  # number of unique touched agent\n",
    "    len_avsets = comb(n-a, k)  # length of available k_nodes combinations\n",
    "\n",
    "    for i_th in range(len_avsets):  # for each available k nodes\n",
    "        # for i in node_sets:  # for each available k nodes\n",
    "        v1 = creat_available_comb(i_th, n, k, min_touched)\n",
    "        # map this node set to its index located in all lists\n",
    "        k_nodes_index = find_idx(v1, n)\n",
    "\n",
    "        for f in range(len_kops):         # for each opinion combination\n",
    "            # locate the column in payoff row- all combinations\n",
    "            column = k_nodes_index*len_kops + f\n",
    "            # por = obj_polarization(A, L, s, n)\n",
    "            # calculate mixed polarization\n",
    "            por = k_max_polarization(payoff_matrix, column, fla_min_fre)\n",
    "            all_por[column] = por\n",
    "\n",
    "        print('Max_por', max(all_por))\n",
    "\n",
    "    print('all_por', all_por)\n",
    "    # Index of maximum polarization - in all actions\n",
    "    column = np.argmax(all_por)\n",
    "    print('column - best action')\n",
    "    print(column)\n",
    "\n",
    "    (v1, max_opinion) = map_action(n, k, column)\n",
    "    print(f\"Maximizer found its target {k} agent: {v1} op: {max_opinion}\")\n",
    "\n",
    "    # old_opinion_max = [s[i] for i in v1]\n",
    "    # check if agent's opinionis is changed or not\n",
    "#     print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "#     print(\"Network reaches steady-state Polarization: \" + str(np.max(all_por)))\n",
    "\n",
    "    return (v1, max_opinion, np.max(all_por), column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# op = s\n",
    "# y = mean_center(s,n)\n",
    "# # print(y)\n",
    "# innat_pol = np.dot(np.transpose(y), y)[0,0]\n",
    "# print('Innate_polarization:')\n",
    "# print(innat_pol)\n",
    "\n",
    "# # Test equilibrium polarization\n",
    "# equ_pol = obj_polarization(A, L, op, n)\n",
    "# print('Equi_polarization:')\n",
    "# print(equ_pol)\n",
    "\n",
    "# di = equ_pol-innat_pol\n",
    "# print(\"Difference:\")\n",
    "# print(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Game Parameters\n",
    "# Game_rounds =200 # Rounds + 1- use for printing data\n",
    "# memory = 50\n",
    "\n",
    "# Game Preparation\n",
    "def push(obj, element):\n",
    "    if len(obj) >= memory:\n",
    "        dif = len(obj) - memory\n",
    "        obj.pop(dif)\n",
    "    obj.extend(list(element))\n",
    "    obj = list(set(obj))\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_action(n,k,column):\n",
    "    k_opinions = creat_all_comb(n, k)\n",
    "    len_kops = len(k_opinions)\n",
    "    nodeset_index = int(column/len_kops)\n",
    "    opset_index = column%len_kops\n",
    "    k_nodes = cgen(nodeset_index, n, k)\n",
    "    opinions = k_opinions[opset_index]\n",
    "\n",
    "    return (k_nodes, opinions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=40 k=2 n=5\n"
     ]
    }
   ],
   "source": [
    "# Game Parameters\n",
    "# Game_rounds =200 # Rounds + 1- use for printing data\n",
    "Game_rounds = 3\n",
    "memory = 10\n",
    "k = 2\n",
    "experiment = 1\n",
    "h = len_actions(k, n)\n",
    "print(f'h={h} k={k} n={n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes, opinions\n",
      "[1, 2] (0, 1)\n",
      "First_max ([1, 2], (0, 1), 0.033878134298830756, 17)\n",
      "DEBUG2 k_fake=[0, 4] k_nodes=[0, 4] touched=[1, 2] n=5 k=2 a=2 i_th=1\n",
      "Nodes, opinions\n",
      "[0, 4] (0.5, 0.5)\n",
      "First_min: ([0, 4], (0.5, 0.5), 0.012700877654037955)\n",
      "min_history [((0, 4), (0.5, 0.5))]\n",
      "Game 1\n",
      "_____________________\n",
      "min_history [((0, 4), (0.5, 0.5))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "fla_min_fre [1.]\n",
      "DEBUG2 k_fake=[1, 2] k_nodes=[1, 2] touched=[0, 4] n=5 k=2 a=2 i_th=0\n",
      "payoff_vector [0.028]\n",
      "payoff_vector [0.022]\n",
      "payoff_vector [0.022]\n",
      "payoff_vector [0.034]\n",
      "Max_por 0.033797021050447185\n",
      "DEBUG2 k_fake=[1, 3] k_nodes=[1, 3] touched=[0, 4] n=5 k=2 a=2 i_th=1\n",
      "payoff_vector [0.016]\n",
      "payoff_vector [0.019]\n",
      "payoff_vector [0.024]\n",
      "payoff_vector [0.016]\n",
      "Max_por 0.033797021050447185\n",
      "DEBUG2 k_fake=[2, 3] k_nodes=[2, 3] touched=[0, 4] n=5 k=2 a=2 i_th=2\n",
      "payoff_vector [0.021]\n",
      "payoff_vector [0.022]\n",
      "payoff_vector [0.033]\n",
      "payoff_vector [0.024]\n",
      "Max_por 0.033797021050447185\n",
      "all_por [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.028 0.022 0.022 0.034 0.016 0.019 0.024 0.016\n",
      " 0.    0.    0.    0.    0.021 0.022 0.033 0.024 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.   ]\n",
      "column - best action\n",
      "19\n",
      "Maximizer found its target 2 agent: [1, 2] op: (1, 1)\n",
      "fre_max at spot 0.5\n",
      "DEBUG2 k_fake=[0, 3] k_nodes=[0, 3] touched=[1, 2] n=5 k=2 a=2 i_th=0\n",
      "DEBUG 1 [0, 3]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [0.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [0, 3]\n",
      "M\n",
      "[[-0.036 -0.036  0.164 -0.039 -0.055]]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [1.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [0, 3]\n",
      "M\n",
      "[[-0.052  0.148  0.148 -0.072 -0.171]]\n",
      "Min_opinion less than 0\n",
      "[-0.024  0.41 ]\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "-0.023863305263157764\n",
      "0\n",
      "[0.]\n",
      "champion:  [0, 3] [0.   0.41] 0.05363317866837766\n",
      "DEBUG2 k_fake=[0, 4] k_nodes=[0, 4] touched=[1, 2] n=5 k=2 a=2 i_th=1\n",
      "DEBUG 1 [0, 4]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [0.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [0, 4]\n",
      "M\n",
      "[[-0.024 -0.024  0.176 -0.004 -0.124]]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [1.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [0, 4]\n",
      "M\n",
      "[[-0.041  0.159  0.159 -0.038 -0.241]]\n",
      "champion:  [0, 4] [0.616 0.691] 0.022946786813537845\n",
      "DEBUG2 k_fake=[3, 4] k_nodes=[3, 4] touched=[1, 2] n=5 k=2 a=2 i_th=2\n",
      "DEBUG 1 [3, 4]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [0.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [3, 4]\n",
      "M\n",
      "[[ 0.026 -0.021  0.179 -0.041 -0.144]]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [1.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [3, 4]\n",
      "M\n",
      "[[ 0.009  0.163  0.163 -0.074 -0.26 ]]\n",
      "Minimizer finds its target agents:\n",
      "[0, 4]\n",
      "min_history\n",
      "[((0, 4), (0.5, 0.5)), ((0, 4), (0.6155754338383838, 0.6908531908888887))]\n",
      "Counter({((0, 4), (0.5, 0.5)): 1, ((0, 4), (0.6155754338383838, 0.6908531908888887)): 1})\n",
      "fla_min_fre: [0.5 0.5]\n",
      "Game 2\n",
      "_____________________\n",
      "min_history [((0, 4), (0.5, 0.5)), ((0, 4), (0.6155754338383838, 0.6908531908888887))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.5 0.5]\n",
      "DEBUG2 k_fake=[1, 2] k_nodes=[1, 2] touched=[0, 4] n=5 k=2 a=2 i_th=0\n",
      "payoff_vector [0.028 0.055]\n",
      "payoff_vector [0.022 0.028]\n",
      "payoff_vector [0.022 0.028]\n",
      "payoff_vector [0.034 0.018]\n",
      "Max_por 0.041058415717851085\n",
      "DEBUG2 k_fake=[1, 3] k_nodes=[1, 3] touched=[0, 4] n=5 k=2 a=2 i_th=1\n",
      "payoff_vector [0.016 0.03 ]\n",
      "payoff_vector [0.019 0.03 ]\n",
      "payoff_vector [0.024 0.016]\n",
      "payoff_vector [0.016 0.006]\n",
      "Max_por 0.041058415717851085\n",
      "DEBUG2 k_fake=[2, 3] k_nodes=[2, 3] touched=[0, 4] n=5 k=2 a=2 i_th=2\n",
      "payoff_vector [0.021 0.03 ]\n",
      "payoff_vector [0.022 0.028]\n",
      "payoff_vector [0.033 0.021]\n",
      "payoff_vector [0.024 0.008]\n",
      "Max_por 0.041058415717851085\n",
      "all_por [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.041 0.025 0.025 0.026 0.023 0.025 0.02  0.011\n",
      " 0.    0.    0.    0.    0.026 0.025 0.027 0.016 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.   ]\n",
      "column - best action\n",
      "16\n",
      "Maximizer found its target 2 agent: [1, 2] op: (0, 0)\n",
      "fre_max at spot 0.3333333333333333\n",
      "DEBUG2 k_fake=[0, 3] k_nodes=[0, 3] touched=[1, 2] n=5 k=2 a=2 i_th=0\n",
      "DEBUG 1 [0, 3]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [0.   ]\n",
      " [0.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [0, 3]\n",
      "M\n",
      "[[-0.019 -0.019 -0.019 -0.005  0.062]]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [0.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [0, 3]\n",
      "M\n",
      "[[-0.036 -0.036  0.164 -0.039 -0.055]]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [1.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [0, 3]\n",
      "M\n",
      "[[-0.052  0.148  0.148 -0.072 -0.171]]\n",
      "champion:  [0, 3] [0.055 0.304] 0.03796129752997609\n",
      "DEBUG2 k_fake=[0, 4] k_nodes=[0, 4] touched=[1, 2] n=5 k=2 a=2 i_th=1\n",
      "DEBUG 1 [0, 4]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [0.   ]\n",
      " [0.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [0, 4]\n",
      "M\n",
      "[[-0.007 -0.007 -0.007  0.029 -0.007]]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [0.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [0, 4]\n",
      "M\n",
      "[[-0.024 -0.024  0.176 -0.004 -0.124]]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [1.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [0, 4]\n",
      "M\n",
      "[[-0.041  0.159  0.159 -0.038 -0.241]]\n",
      "champion:  [0, 4] [0.429 0.469] 0.027536134133403164\n",
      "DEBUG2 k_fake=[3, 4] k_nodes=[3, 4] touched=[1, 2] n=5 k=2 a=2 i_th=2\n",
      "DEBUG 1 [3, 4]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [0.   ]\n",
      " [0.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [3, 4]\n",
      "M\n",
      "[[ 0.042 -0.004 -0.004 -0.008 -0.027]]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [0.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [3, 4]\n",
      "M\n",
      "[[ 0.026 -0.021  0.179 -0.041 -0.144]]\n",
      "DEBUG0 op1 [[0.231]\n",
      " [1.   ]\n",
      " [1.   ]\n",
      " [0.218]\n",
      " [0.162]] v2 [3, 4]\n",
      "M\n",
      "[[ 0.009  0.163  0.163 -0.074 -0.26 ]]\n",
      "Minimizer finds its target agents:\n",
      "[0, 4]\n",
      "min_history\n",
      "[((0, 4), (0.5, 0.5)), ((0, 4), (0.6155754338383838, 0.6908531908888887)), ((0, 4), (0.42870674696969713, 0.46863096866666665))]\n",
      "Counter({((0, 4), (0.5, 0.5)): 1, ((0, 4), (0.6155754338383838, 0.6908531908888887)): 1, ((0, 4), (0.42870674696969713, 0.46863096866666665)): 1})\n",
      "fla_min_fre: [0.333 0.333 0.333]\n",
      "Game 3\n",
      "_____________________\n",
      "min_history [((0, 4), (0.5, 0.5)), ((0, 4), (0.6155754338383838, 0.6908531908888887)), ((0, 4), (0.42870674696969713, 0.46863096866666665))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "MAX_last_100,  all\n",
      "[0.01 0.01] [0.333 0.333 0.333]\n",
      "[16 19] [16 17 19]\n",
      "Max Nodes:[1, 2] Opinion: (0, 0)\n",
      "Max Nodes:[1, 2] Opinion: (1, 1)\n",
      "MIN_last_100,  all\n",
      "[0.02] [1.]\n",
      "Counter({(0, 4): 2}) Counter({(0, 4): 3})\n",
      "Max Pol: 0.041058415717851085 Min Pol: 0.027536134133403164\n"
     ]
    }
   ],
   "source": [
    "# def excute(h, k, Game_rounds):\n",
    "# Preparation for the game\n",
    "payoff_matrix = np.empty((0, h), float)\n",
    "max_history = np.zeros(h, int)  # n*2 matrix, agent i & opinion options\n",
    "# append a list of (agent i, min_opinion), min_opinion can be any value\n",
    "min_history = []\n",
    "\n",
    "max_history_last_100 = np.zeros(h, int)\n",
    "min_history_last_100 = []\n",
    "\n",
    "max_touched = []\n",
    "min_touched = []\n",
    "min_touched_all = []\n",
    "min_touched_last_100 = []\n",
    "\n",
    "\n",
    "# Game start from maximizer random play\n",
    "#     print('Maximizer first selection')\n",
    "# (v1, max_opinion, max_pol) = random_play(op,n)   # Maximizer does random action\n",
    "# (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "\n",
    "# (v1, max_opinion, max_pol)  = ((6,19),(1,0),0)\n",
    "\n",
    "# (v1, max_opinion, max_pol, column) = ([0, 2], (1, 1), 0.1270803458329454, 7)\n",
    "(v1, max_opinion, max_pol, column) = k_random_play(s, n, k)\n",
    "First_max = (v1, max_opinion, max_pol, column)\n",
    "print('First_max', First_max)\n",
    "\n",
    "\n",
    "# Maximizer start with greedy play\n",
    "# (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)   # Maximizer choose action greedily\n",
    "\n",
    "max_touched.extend(tuple(v1))\n",
    "# store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "max_history[column] = max_history[column] + 1\n",
    "\n",
    "fla_max_fre = max_history/1  # its frequency, only played  1 time so far, divided by 1\n",
    "\n",
    "# fla_max_fre = max_frequency.flatten()   # flatten the n*2 matrix to a 2n*1 matrix\n",
    "# so we can multiply the freuency (2n*1)with payoff array (1*2n)\n",
    "# to get average payoff of fictitious play\n",
    "\n",
    "# if game start from minimizer random play - make sure two random play are not same agent!!!\n",
    "#     print('Minimizer first selection')\n",
    "(v2, min_opinion, min_pol) = k_random_play_1(s, n, k, max_touched)\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,min_touched)\n",
    "\n",
    "# (v2, min_opinion, min_pol) = ([1, 3], (0.5, 0.5), 0.020147309098439665)\n",
    "# (v2, min_opinion, min_pol) = ((13,15),(0.5,0.5),1)\n",
    "# (v2, payoff_row, min_opinion, min_pol) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "First_min = (v2, min_opinion, min_pol)\n",
    "print('First_min:', First_min)\n",
    "\n",
    "if any(x in v1 for x in v2):  # if Max and Min randomly selected the same agent, then we need to restart - cannot choose same agent\n",
    "    sys.exit()\n",
    "\n",
    "# Minimizer start with greedy play\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "min_touched.extend(v2)\n",
    "min_touched_all.append(tuple(v2))\n",
    "# store minimizer play history\n",
    "#############################################################\n",
    "min_history.append((tuple(v2), tuple(min_opinion)))\n",
    "print(f'min_history {min_history}')\n",
    "# return a dictionary include {'min_option': count of this choice}\n",
    "counter = collections.Counter(min_history)\n",
    "# print(counter)\n",
    "# return only frequency of all min options in order\n",
    "fla_min_fre = np.array(list(counter.values()))/1\n",
    "# print(f'fla_min_fre {fla_min_fre}')\n",
    "\n",
    "(_, payoff_row) = mixed_K_min_polarization(s, v2, min_opinion, fla_max_fre)\n",
    "payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "\n",
    "equi_min = min_pol\n",
    "equi_max = max_pol\n",
    "\n",
    "Flag = 0\n",
    "i = 0\n",
    "while Flag == 0:\n",
    "    i = i + 1\n",
    "    print(\"Game \" + str(i))\n",
    "    print(\"_____________________\")\n",
    "    print('min_history', min_history)\n",
    "    print('max_history', max_history)\n",
    "\n",
    "\n",
    "#     if max_pol == min_pol:\n",
    "    if i == Game_rounds:            # i == # of iterations you want to run + 2\n",
    "        # because Game 101 is skipped for collecting data, to get 200 game result, we need to run 201 iteration\n",
    "        print('MAX_last_100,  all')\n",
    "        max_l100_fre = max_history_last_100/100\n",
    "        max_fre = max_history/Game_rounds\n",
    "        print(max_l100_fre[np.nonzero(max_l100_fre)],\n",
    "              max_fre[np.nonzero(max_fre)])\n",
    "        print(np.nonzero(max_l100_fre)[0], np.nonzero(max_fre)[0])\n",
    "        columns = list(np.nonzero(max_l100_fre)[0])\n",
    "        for column in list(columns):\n",
    "            k_opinions = creat_all_comb(n, k)\n",
    "            len_kops = len(k_opinions)\n",
    "            nodeset_index = int(column/len_kops)\n",
    "            opset_index = column % len_kops\n",
    "            k_nodes = cgen(nodeset_index, n, k)\n",
    "\n",
    "            opinions = k_opinions[opset_index]\n",
    "            print('Max Nodes:' + str(k_nodes)+' Opinion: ' + str(opinions))\n",
    "\n",
    "        # MINimizer's Strategy in the last 100 round\n",
    "        counter = collections.Counter(min_touched_last_100)\n",
    "        # return only frequency of all min options in order\n",
    "        fla_min_fre = np.array(list(counter.values()))/(100)\n",
    "        print('MIN_last_100,  all')\n",
    "        # return a dictionary include {'min_option': count of this choice}\n",
    "        counter_1 = collections.Counter(min_touched_all)\n",
    "        # return only frequency of all min options in order\n",
    "        fla_min_fre_1 = np.array(list(counter_1.values()))/Game_rounds\n",
    "        print(fla_min_fre, fla_min_fre_1)\n",
    "        print(counter, counter_1)\n",
    "        print(f'Max Pol: {equi_max} Min Pol: {equi_min}')\n",
    "\n",
    "        break\n",
    "\n",
    "    elif equi_min == equi_max:\n",
    "        print(f\"Reached Nash Equilibrium at game {i} and Equi_Por = {equi_min}\")\n",
    "        # print(f'max_distribution {max_frequency}')\n",
    "        # print(f'min_distribution {fla_min_fre}')\n",
    "        Flag = 1\n",
    "        break\n",
    "# ________________________________________________________________\n",
    "    # maximizer play\n",
    "    else:\n",
    "        if i == Game_rounds-100:  # if Game_round = 200, after 100 iteration, Game 101 print previous historical result\n",
    "\n",
    "            # Remove max frequency less than 0.1--\n",
    "            max_history_last_100 = np.zeros(h)\n",
    "            min_history_last_100 = []\n",
    "            min_touched_last_100 = []\n",
    "\n",
    "        (v1, max_opinion, equi_max, column) = max_k_play(\n",
    "            payoff_matrix, n, k, fla_min_fre, min_touched)\n",
    "        max_touched = push(max_touched, v1)\n",
    "\n",
    "        # cumulate strategy\n",
    "        max_history[column] = max_history[column] + 1\n",
    "        max_history_last_100[column] = max_history_last_100[column] + 1\n",
    "\n",
    "        # max_frequency to calculate average payoff\n",
    "        fla_max_fre = max_history/(i+1)\n",
    "        print(f'fre_max at spot {fla_max_fre[column]}')\n",
    "\n",
    "        # MINImizer play\n",
    "        (v2, payoff_row, min_opinion, equi_min) = mixed_min_play(\n",
    "            s, n, max_touched, fla_max_fre)\n",
    "        min_touched = push(min_touched, v2)\n",
    "        min_touched_all.append(v2)\n",
    "        min_touched_last_100.append(v2)\n",
    "\n",
    "        # print(v2, min_opinion, min_pol)\n",
    "        if tuple(tuple(v2)+min_opinion) in counter.keys():\n",
    "            # if this min_option is in min_history, no need to update paryoff matrix, only update frequency\n",
    "            payoff_matrix = payoff_matrix\n",
    "            # print(\"Same history\")\n",
    "            # print((str(v2),str(min_opinion)))\n",
    "        else:\n",
    "            # if this is a new option, append to previous matrix\n",
    "            payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "            # print(f'payoff_row {payoff_row}')\n",
    "\n",
    "        min_history.append((tuple(v2), tuple(min_opinion)))\n",
    "        min_history_last_100.append(tuple(v2 + min_opinion))\n",
    "        print('min_history')\n",
    "        print(min_history)\n",
    "        # return a dictionary include {'min_option': count of this choice}\n",
    "        counter = collections.Counter(min_history)\n",
    "        print(counter)\n",
    "        # print('counter.keys:', counter.keys())\n",
    "        # print(counter.keys())\n",
    "        # return only frequency of all min options in order\n",
    "        fla_min_fre = np.array(list(counter.values()))/(i+1)\n",
    "        print('fla_min_fre:', fla_min_fre)\n",
    "        # print(fla_min_fre)\n",
    "        # print(\"Not Reached Nash Equilibrium at Equi_Min = \" + str(equi_min) + \" and Equi_Max = \"+ str(equi_max))\n",
    "\n",
    "result = (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100,\n",
    "          min_touched_last_100, min_touched_all, max_history, fla_max_fre, max_history_last_100, equi_max, equi_min)\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(s)\n",
    "# np.savetxt('Nt1_Innate_opinion.csv', s, delimiter=\",\")\n",
    "# np.savetxt('Nt1_Adjacency_matrix.csv', G, delimiter=\",\")\n",
    "# save_(result, k, experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_(result, k, experiment):\n",
    "    (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, fla_max_fre, max_history_last_100, equi_max, equi_min) = result\n",
    "    pd.DataFrame(payoff_matrix).to_csv('Payoff Matrix'+ str(k) +'.'+str(experiment)+'.csv')\n",
    "\n",
    "    with open('Result'+ str(k) +'.'+str(experiment)+'.txt', \"a\") as f:\n",
    "        print('Initial Condition -(agent, opinion, pol)', file=f)\n",
    "#         print('Innate op'+str(s),file=f)\n",
    "#         print('Adjacency matrix'+ str(G), file=f)\n",
    "#         print('Selected Nodeset, k_Opinions, Steady-state polarization',file=f)\n",
    "        print('Max:'+ str(First_max), file=f)\n",
    "        print('Min' + str(First_min), file=f)\n",
    "\n",
    "        print('_____________________', file=f)\n",
    "        print('Max Pol: '+str(equi_max)+\"  Min Pol: \"+str(equi_min))\n",
    "        # MAXimizer's distribution of LAST 100 iteration\n",
    "        print('Max_distribution_last_100',file = f)\n",
    "        max_l100_fre = max_history_last_100/100\n",
    "        print(max_l100_fre [np.nonzero(max_l100_fre)],file = f)\n",
    "        # print for small network\n",
    "        #print(max_history_last_100)\n",
    "        # # Print for Large Network\n",
    "        print(np.nonzero(max_l100_fre),file = f)\n",
    "        columns = np.nonzero(max_l100_fre)\n",
    "        columns = list(columns[0])\n",
    "        for column in columns:\n",
    "            (k_nodes, opinions) = map_action(n,k,column)\n",
    "            print('  Max Nodes:'+ str(k_nodes)+' Opinion: '+ str(opinions), file = f)\n",
    "\n",
    "\n",
    "        print('Max_distribution_all',file = f)\n",
    "        max_fre = max_history/Game_rounds\n",
    "        print(max_fre[np.nonzero(max_fre)],file = f)\n",
    "        print([np.nonzero(max_fre)],file = f)\n",
    "        columns_all = np.nonzero(max_l100_fre)\n",
    "        columns_all = list(columns_all[0])\n",
    "        for column in columns_all:\n",
    "            (k_nodes, opinions) =  map_action(n,k,column)\n",
    "            print('  Max Nodes:'+ str(k_nodes)+' Opinion: '+ str(opinions), file = f)\n",
    "\n",
    "\n",
    "        # MINimizer's Strategy in the last 100 round\n",
    "        counter=collections.Counter(min_touched_last_100)\n",
    "        fla_min_fre = np.array(list(counter.values()))/(100) #return only frequency of all min options in order\n",
    "        print('Min_distribution_last_100',file = f)\n",
    "        print(fla_min_fre,file = f)\n",
    "        print(counter,file = f)\n",
    "        # print(min_touched_last_100)\n",
    "\n",
    "        counter_1=collections.Counter(min_touched_all)  #return a dictionary include {'min_option': count of this choice}\n",
    "        fla_min_fre_1 = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "        print('Min_distribution_all',file = f)\n",
    "        print(fla_min_fre_1,file = f)\n",
    "        print(counter_1,file = f)\n",
    "        np.set_printoptions(precision=3)\n",
    "\n",
    "        counter_a=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "        print(counter_a, file=f)\n",
    "\n",
    "        print('min_recent_'+str(memory)+'_touched', file=f)# then stop at Game 202\n",
    "        print(min_touched, file=f)\n",
    "        print('max_recent_'+str(memory)+'_touched', file=f)\n",
    "        print(max_touched, file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
