{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real datanetwork with small sythetic network\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "# %run Check_Derivation_of_Two_Opinions.ipynb\n",
    "#%run pure_strategy_selection.ipynb  #include simple selection algorithm\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n",
    "from itertools import count\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "import scipy.optimize\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "# add parent directory to path so that utils is available\n",
    "sys.path.append('..')\n",
    "from utils import *\n",
    "\n",
    "from game import *\n",
    "\n",
    "# Game Parameters\n",
    "game_rounds = 5\n",
    "memory = 50\n",
    "\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# Create the 'results' directory if it doesn't exist\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "np.set_printoptions(precision=5)\n",
    "\n",
    "calculate_polarization = calculate_polarization1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Fixed initial condition + memeory = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Synthetic Network Data\n",
    "df = pd.read_csv(f'{current_path}/data/Adjacency Matrix.csv', header=None)\n",
    "G = np.array(df[df.columns[:]])\n",
    "# print(G)\n",
    "df1 = pd.read_csv(f'{current_path}/data/Innate Opinion.csv', header=None)\n",
    "s = np.array(df1[df1.columns[:]])\n",
    "# print(G.shape)\n",
    "\n",
    "# Set n according to the data\n",
    "n = len(s[:])\n",
    "# print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Doesn't work with network_anl\n",
    "\n",
    "# Import Synthetic Network Data\n",
    "df = pd.read_csv(f'{current_path}/data/5Adj_matrix.csv', header=None)\n",
    "G = np.array(df[df.columns[:]])\n",
    "# # # print(G)\n",
    "df1 = pd.read_csv(f'{current_path}/data/5innate_op.csv', header=None)\n",
    "s = np.array(df1[df1.columns[:]])\n",
    "\n",
    "# print(s)\n",
    "# print(G.shape)\n",
    "\n",
    "# # Set n according to the data\n",
    "n = len(s[:])\n",
    "# # print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Process the Network Data\n",
    "L = scipy.sparse.csgraph.laplacian(G, normed=False)  # Return the Laplacian matrix\n",
    "A = np.linalg.inv(np.identity(n) + L)  # A = (I + L)^(-1)\\n  Stanford paper theory\n",
    "m = num_edges(L, n)                    # call the function to calculate the number of edges\n",
    "# what the twitter graph looks like\n",
    "nxG = nx.from_numpy_array(G)\n",
    "#plt.figure(figsize=(20, 20))\n",
    "# nx.draw(nxG)\n",
    "columnsum_ij = np.sum(A, axis=0)\n",
    "# print(columnsum_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________Degree Centrality_____________________________\n",
      "{4: 0.25, 0: 0.75, 1: 0.75, 2: 0.75, 3: 1.0}\n",
      "                           \n",
      "_______________Closeness Rank_____________________________\n",
      "{4: 0.5714285714285714, 0: 0.8, 1: 0.8, 2: 0.8, 3: 1.0}\n",
      "                           \n",
      "_______________Page Rank_____________________________\n",
      "{4: 0.16965100981173117, 0: 0.4820442559301269, 1: 0.4820442559301269, 2: 0.4820442559301269, 3: 0.5235633112090369}\n",
      "                           \n",
      "{0: array([0.19703]), 3: array([0.21003]), 2: array([0.21351]), 4: array([0.26584]), 1: array([0.45939])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def network_anl2(s,n,G,agent):\n",
    "    print(str(agent)+' opinion: ' + str(s[agent]))\n",
    "    print(str(agent)+' neighbors: '+ str(np.nonzero(G[agent])))\n",
    "\n",
    "    s_aa = s[:, 0]\n",
    "    my_dict = {index: value for index, value in enumerate(s_aa)}\n",
    "    sorting_s = sorted(my_dict.items(), key=lambda x:x[1])\n",
    "    sorted_S = dict(sorting_s)\n",
    "\n",
    "    temp = list(sorted_S.items())\n",
    "    res = [idx for idx, key in enumerate(temp) if key[0]==agent]\n",
    "    # printing result\n",
    "    print(\"Opinion rank of this agent is : \" + str(res))\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"___________________Max Analyze__________________________________________\")\n",
    "    nxG = nx.from_numpy_array(G)\n",
    "    # G = nx.karate_club_graph()\n",
    "    print(\"_______________Degree Centrality___________________\")\n",
    "    deg_centrality = nx.degree_centrality(nxG)\n",
    "    sortedDict = sorted(deg_centrality.items(), key=lambda x:x[1])\n",
    "    converted_dict = dict(sortedDict)\n",
    "    temp1 = list(converted_dict.items())\n",
    "    res1 = [idx for idx, key in enumerate(temp1) if key[0]==agent]\n",
    "    print(\"rank of this agent is : \" + str(res1))\n",
    "    print(converted_dict[agent])\n",
    "\n",
    "    # print(converted_dict)\n",
    "    print(\"                           \")\n",
    "    print(\"_______________Closeness Rank________________________\")\n",
    "    close_centrality = nx.closeness_centrality(nxG)\n",
    "    sortedDict1 = sorted(close_centrality.items(), key=lambda x:x[1])\n",
    "    converted_dict1 = dict(sortedDict1)\n",
    "    temp2 = list(converted_dict1.items())\n",
    "    res2 = [idx for idx, key in enumerate(temp2) if key[0]==agent]\n",
    "    print(\"rank of this agent is : \" + str(res2))\n",
    "    print(converted_dict1[agent])\n",
    "    # print(converted_dict1)\n",
    "    print(\"                           \")\n",
    "    print(\"_______________Page Rank_____________________________\")\n",
    "    pr = nx.eigenvector_centrality(nxG)\n",
    "    sortedDict3 = sorted(pr.items(), key=lambda x:x[1])\n",
    "    converted_dict3 = dict(sortedDict3)\n",
    "    temp3 = list(converted_dict3.items())\n",
    "    res3 = [idx for idx, key in enumerate(temp3) if key[0]==agent]\n",
    "    print(\"rank of this agent is : \" + str(res3))\n",
    "    print(converted_dict3[agent])\n",
    "    # print(converted_dict3)\n",
    "\n",
    "    print(\"                           \")\n",
    "\n",
    "    def gap(op, n):\n",
    "        ones = np.ones((n, 1))\n",
    "        x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "        return abs(x)\n",
    "\n",
    "    gap = gap(s,n)\n",
    "    my_gap = {index: value for index, value in enumerate(gap)}\n",
    "    sorting_gap = sorted(my_gap.items(), key=lambda x:x[1])\n",
    "    sorted_gap = dict(sorting_gap)\n",
    "    #print(sorted_gap)\n",
    "    temp4 = list(sorted_gap.items())\n",
    "    res4 = [idx for idx, key in enumerate(temp4) if key[0]==agent]\n",
    "    print(\"Agent's opinion gap to mean opinion is ranked as: \" + str(res4))\n",
    "\n",
    "\n",
    "    print(\"___________________Max Analyze__________________________________________\")\n",
    "nxG = nx.from_numpy_array(G)\n",
    "# G = nx.karate_club_graph()\n",
    "print(\"_______________Degree Centrality_____________________________\")\n",
    "plt.figure(figsize =(15, 15))\n",
    "deg_centrality = nx.degree_centrality(nxG)\n",
    "sortedDict = sorted(deg_centrality.items(), key=lambda x:x[1])\n",
    "converted_dict = dict(sortedDict)\n",
    "print(converted_dict)\n",
    "print(\"                           \")\n",
    "print(\"_______________Closeness Rank_____________________________\")\n",
    "close_centrality = nx.closeness_centrality(nxG)\n",
    "sortedDict1 = sorted(close_centrality.items(), key=lambda x:x[1])\n",
    "converted_dict1 = dict(sortedDict1)\n",
    "print(converted_dict1)\n",
    "print(\"                           \")\n",
    "print(\"_______________Page Rank_____________________________\")\n",
    "pr = nx.eigenvector_centrality(nxG)\n",
    "sortedDict3 = sorted(pr.items(), key=lambda x:x[1])\n",
    "converted_dict3 = dict(sortedDict3)\n",
    "print(converted_dict3)\n",
    "\n",
    "print(\"                           \")\n",
    "\n",
    "def gap(op, n):\n",
    "    ones = np.ones((n, 1))\n",
    "    x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "    return abs(x)\n",
    "\n",
    "gap = gap(s,n)\n",
    "my_gap = {index: value for index, value in enumerate(gap)}\n",
    "sorting_gap = sorted(my_gap.items(), key=lambda x:x[1])\n",
    "sorted_gap = dict(sorting_gap)\n",
    "print(sorted_gap)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO resolve network_anl (from this file) and network_anl2 (from utils) produce different results\n",
    "# agent = 6\n",
    "# # network_anl(s,n,G,agent)\n",
    "# print('_' * 10)\n",
    "# network_anl2(s,n,G,agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Equilibrium & Polarization \n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\t0.41022891110361115\n",
      "Equi_polarization:\t0.04411213470472514\n",
      "Difference:\t\t-0.366116776398886\n"
     ]
    }
   ],
   "source": [
    "calculate_polarization1(s, n, A, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_(result: GameResult, k: int, experiment: int):\n",
    "    pd.DataFrame(result.payoff_matrix).to_csv(\n",
    "        f'results/Payoff-Matrix-k-{k}-experiment-{experiment}.csv')\n",
    "\n",
    "    # Save the original standard output\n",
    "    original_stdout = sys.stdout\n",
    "\n",
    "    with open(f'results/Result-k-{k}-experiment-{experiment}.txt', \"w\") as f:\n",
    "        # Change the standard output to the file we created.\n",
    "        sys.stdout = f\n",
    "\n",
    "        print('Initial Condition - (agent, opinion, pol)')\n",
    "        # print(f'Innate op {s}')\n",
    "        # print(f'Adjacency matrix {G}')\n",
    "        # print('Selected Nodeset, k_Opinions, Steady-state polarization')\n",
    "        print(f'Max:{result.first_max}')\n",
    "        print(f'Min{result.first_min}')\n",
    "\n",
    "        print('_____________________')\n",
    "        print(f'Max Pol: {result.equi_max} Min Pol: {result.equi_min}')\n",
    "        # MAXimizer's distribution of LAST 100 iteration\n",
    "        print('Max_distribution_last_100')\n",
    "        max_l100_fre = result.max_history_last_100/100\n",
    "        print(max_l100_fre[np.nonzero(max_l100_fre)])\n",
    "        # print for small network\n",
    "        # print(max_history_last_100)\n",
    "        # # Print for Large Network\n",
    "        print(np.nonzero(max_l100_fre))\n",
    "\n",
    "        print('Max_distribution_all')\n",
    "        max_fre = result.max_history/result.game_rounds\n",
    "        print(max_fre[np.nonzero(max_fre)])\n",
    "        print([np.nonzero(max_fre)])\n",
    "\n",
    "        # MINimizer's Strategy in the last 100 round\n",
    "        counter = collections.Counter(result.min_touched_last_100)\n",
    "        # frequency of all min options in order\n",
    "        fla_min_fre = np.array(list(counter.values()))/(100)\n",
    "        print('Min_distribution_last_100')\n",
    "        print(fla_min_fre)\n",
    "        print(counter)\n",
    "        # print(min_touched_last_100)\n",
    "\n",
    "        # return a dictionary include {'min_option': count of this choice}\n",
    "        counter_1 = collections.Counter(result.min_touched_all)\n",
    "        # frequency of all min options in order\n",
    "        fla_min_fre_1 = np.array(list(counter_1.values()))/result.game_rounds\n",
    "        print('Min_distribution_all')\n",
    "        print(fla_min_fre_1)\n",
    "        print(counter_1)\n",
    "        np.set_printoptions(precision=3)\n",
    "\n",
    "        # a dictionary include {'min_option': count of this choice}\n",
    "        counter_a = collections.Counter(result.min_history)\n",
    "        print(counter_a)\n",
    "\n",
    "        print(f'min_recent_{memory}_touched')  # then stop at Game 202\n",
    "        print(result.min_touched)\n",
    "        print('max_recent_{memory}_touched')\n",
    "        print(result.max_touched)\n",
    "\n",
    "        # Reset the standard output to its original value\n",
    "        sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=40 k=2 n=5\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "h = len_actions(k, n)\n",
    "print(f'h={h} k={k} n={n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes, opinions\n",
      "[2, 4] (0, 1)\n",
      "DEBUG2 k_fake=[1, 3] k_nodes=[1, 3] touched=[2, 4] n=5 k=2 a=2 i_th=2\n",
      "Nodes, opinions\n",
      "[1, 3] (0.5, 0.5)\n",
      "min_history [((1, 3), (0.5, 0.5))]\n",
      "--------------------\n",
      "Game 1\n",
      "----------\n",
      "min_history [((1, 3), (0.5, 0.5))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0]\n",
      "fla_min_fre [1.]\n",
      "DEBUG2 k_fake=[0, 2] k_nodes=[0, 2] touched=[1, 3] n=5 k=2 a=2 i_th=0\n",
      "Max_por 0.09181344162282333\n",
      "DEBUG2 k_fake=[0, 4] k_nodes=[0, 4] touched=[1, 3] n=5 k=2 a=2 i_th=1\n",
      "Max_por 0.10085992937712412\n",
      "DEBUG2 k_fake=[2, 4] k_nodes=[2, 4] touched=[1, 3] n=5 k=2 a=2 i_th=2\n",
      "Max_por 0.11239874284511504\n",
      "all_por [0.      0.      0.      0.      0.00916 0.0416  0.0416  0.09181 0.\n",
      " 0.      0.      0.      0.03724 0.08109 0.10086 0.02249 0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.01837 0.1124  0.07469 0.0465\n",
      " 0.      0.      0.      0.     ]\n",
      "column - best action: 33\n",
      "Maximizer found its target 2 agent: [2, 4] op: (0, 1)\n",
      "fre_max at spot 1.0\n",
      "DEBUG2 k_fake=[0, 1] k_nodes=[0, 1] touched=[2, 4] n=5 k=2 a=2 i_th=0\n",
      "M\n",
      "[[-0.12392 -0.12392 -0.12392 -0.00431  0.37608]]\n",
      "Min_opinion more than 1\n",
      "[1.07862 1.07862]\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "champion:  [0, 1] [1.07862 1.07862] 0.053082706428791065\n",
      "DEBUG2 k_fake=[0, 3] k_nodes=[0, 3] touched=[2, 4] n=5 k=2 a=2 i_th=1\n",
      "M\n",
      "[[-0.13145  0.04597 -0.13145 -0.0629   0.27984]]\n",
      "Min_opinion more than 1\n",
      "[1.17572 0.7657 ]\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "champion:  [0, 3] [1.17572 0.7657 ] 0.048332346145553\n",
      "DEBUG2 k_fake=[1, 3] k_nodes=[1, 3] touched=[2, 4] n=5 k=2 a=2 i_th=2\n",
      "M\n",
      "[[-0.07438 -0.12051 -0.12051 -0.04102  0.35642]]\n",
      "Min_opinion more than 1\n",
      "[1.27937 0.62751]\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Minimizer finds its target agents: [0, 3]\n",
      "fla_min_fre: [0.5 0.5]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.048332346145553 and Equi_Max = 0.11239874284511504\n",
      "--------------------\n",
      "Game 2\n",
      "----------\n",
      "min_history [((1, 3), (0.5, 0.5)), ((0, 3), (1.1757216774210524, 0.765704430105263))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.5 0.5]\n",
      "DEBUG2 k_fake=[2, 4] k_nodes=[2, 4] touched=[0, 1, 3] n=5 k=2 a=3 i_th=0\n",
      "Max_por 0.13591018710480768\n",
      "all_por [0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.06922 0.08037 0.13591 0.02483\n",
      " 0.      0.      0.      0.     ]\n",
      "column - best action: 34\n",
      "Maximizer found its target 2 agent: [2, 4] op: (1, 0)\n",
      "fre_max at spot 0.3333333333333333\n",
      "DEBUG2 k_fake=[0, 1] k_nodes=[0, 1] touched=[2, 4] n=5 k=2 a=2 i_th=0\n",
      "M\n",
      "[[-0.12392 -0.12392 -0.12392 -0.00431  0.37608]]\n",
      "M\n",
      "[[-0.02392 -0.02392  0.17608 -0.00431 -0.12392]]\n",
      "champion:  [0, 1] [0.67478 0.67478] 0.08824136159032953\n",
      "DEBUG2 k_fake=[0, 3] k_nodes=[0, 3] touched=[2, 4] n=5 k=2 a=2 i_th=1\n",
      "M\n",
      "[[-0.13145  0.04597 -0.13145 -0.0629   0.27984]]\n",
      "M\n",
      "[[-0.03145  0.14597  0.16855 -0.0629  -0.22016]]\n",
      "champion:  [0, 3] [0.68449 0.6429 ] 0.0864685181277752\n",
      "DEBUG2 k_fake=[1, 3] k_nodes=[1, 3] touched=[2, 4] n=5 k=2 a=2 i_th=2\n",
      "M\n",
      "[[-0.07438 -0.12051 -0.12051 -0.04102  0.35642]]\n",
      "M\n",
      "[[ 0.02562 -0.02051  0.17949 -0.04102 -0.14358]]\n",
      "Minimizer finds its target agents: [0, 3]\n",
      "fla_min_fre: [0.33333 0.33333 0.33333]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.0864685181277752 and Equi_Max = 0.13591018710480768\n",
      "--------------------\n",
      "Game 3\n",
      "----------\n",
      "min_history [((1, 3), (0.5, 0.5)), ((0, 3), (1.1757216774210524, 0.765704430105263)), ((0, 3), (0.6844936072456138, 0.6428974125614034))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.33333 0.33333 0.33333]\n",
      "DEBUG2 k_fake=[2, 4] k_nodes=[2, 4] touched=[0, 1, 3] n=5 k=2 a=3 i_th=0\n",
      "Max_por 0.1372124198978826\n",
      "all_por [0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.06953 0.07351 0.13721 0.01897\n",
      " 0.      0.      0.      0.     ]\n",
      "column - best action: 34\n",
      "Maximizer found its target 2 agent: [2, 4] op: (1, 0)\n",
      "fre_max at spot 0.5\n",
      "DEBUG2 k_fake=[0, 1] k_nodes=[0, 1] touched=[2, 4] n=5 k=2 a=2 i_th=0\n",
      "M\n",
      "[[-0.12392 -0.12392 -0.12392 -0.00431  0.37608]]\n",
      "M\n",
      "[[-0.02392 -0.02392  0.17608 -0.00431 -0.12392]]\n",
      "champion:  [0, 1] [0.47285 0.47285] 0.09168607378648336\n",
      "DEBUG2 k_fake=[0, 3] k_nodes=[0, 3] touched=[2, 4] n=5 k=2 a=2 i_th=1\n",
      "M\n",
      "[[-0.13145  0.04597 -0.13145 -0.0629   0.27984]]\n",
      "M\n",
      "[[-0.03145  0.14597  0.16855 -0.0629  -0.22016]]\n",
      "DEBUG2 k_fake=[1, 3] k_nodes=[1, 3] touched=[2, 4] n=5 k=2 a=2 i_th=2\n",
      "M\n",
      "[[-0.07438 -0.12051 -0.12051 -0.04102  0.35642]]\n",
      "M\n",
      "[[ 0.02562 -0.02051  0.17949 -0.04102 -0.14358]]\n",
      "Minimizer finds its target agents: [0, 1]\n",
      "fla_min_fre: [0.25 0.25 0.25 0.25]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.09168607378648336 and Equi_Max = 0.1372124198978826\n",
      "--------------------\n",
      "Game 4\n",
      "----------\n",
      "min_history [((1, 3), (0.5, 0.5)), ((0, 3), (1.1757216774210524, 0.765704430105263)), ((0, 3), (0.6844936072456138, 0.6428974125614034)), ((0, 1), (0.47285372288461525, 0.47285372288461547))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.25 0.25 0.25 0.25]\n",
      "DEBUG2 k_fake=[2, 4] k_nodes=[2, 4] touched=[0, 1, 3] n=5 k=2 a=3 i_th=0\n",
      "Max_por 0.12488071367099433\n",
      "all_por [0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.05829 0.079   0.12488 0.02337\n",
      " 0.      0.      0.      0.     ]\n",
      "column - best action: 34\n",
      "Maximizer found its target 2 agent: [2, 4] op: (1, 0)\n",
      "fre_max at spot 0.6\n",
      "DEBUG2 k_fake=[0, 1] k_nodes=[0, 1] touched=[2, 4] n=5 k=2 a=2 i_th=0\n",
      "M\n",
      "[[-0.12392 -0.12392 -0.12392 -0.00431  0.37608]]\n",
      "M\n",
      "[[-0.02392 -0.02392  0.17608 -0.00431 -0.12392]]\n",
      "champion:  [0, 1] [0.3517 0.3517] 0.08922982418109876\n",
      "DEBUG2 k_fake=[0, 3] k_nodes=[0, 3] touched=[2, 4] n=5 k=2 a=2 i_th=1\n",
      "M\n",
      "[[-0.13145  0.04597 -0.13145 -0.0629   0.27984]]\n",
      "M\n",
      "[[-0.03145  0.14597  0.16855 -0.0629  -0.22016]]\n",
      "DEBUG2 k_fake=[1, 3] k_nodes=[1, 3] touched=[2, 4] n=5 k=2 a=2 i_th=2\n",
      "M\n",
      "[[-0.07438 -0.12051 -0.12051 -0.04102  0.35642]]\n",
      "M\n",
      "[[ 0.02562 -0.02051  0.17949 -0.04102 -0.14358]]\n",
      "champion:  [1, 3] [0.39516 0.40646] 0.08915275249621427\n",
      "Minimizer finds its target agents: [1, 3]\n",
      "fla_min_fre: [0.2 0.2 0.2 0.2 0.2]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.08915275249621427 and Equi_Max = 0.12488071367099433\n",
      "--------------------\n",
      "Game 5\n",
      "----------\n",
      "min_history [((1, 3), (0.5, 0.5)), ((0, 3), (1.1757216774210524, 0.765704430105263)), ((0, 3), (0.6844936072456138, 0.6428974125614034)), ((0, 1), (0.47285372288461525, 0.47285372288461547)), ((1, 3), (0.39515623568421077, 0.40645835242105266))]\n",
      "max_history [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0 0\n",
      " 0 0 0]\n",
      "fla_min_fre [0.2 0.2 0.2 0.2 0.2]\n",
      "DEBUG2 k_fake=[2, 4] k_nodes=[2, 4] touched=[0, 1, 3] n=5 k=2 a=3 i_th=0\n",
      "Max_por 0.1135444556245015\n",
      "all_por [0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.04918 0.08732 0.11354 0.02947\n",
      " 0.      0.      0.      0.     ]\n",
      "column - best action: 34\n",
      "Maximizer found its target 2 agent: [2, 4] op: (1, 0)\n",
      "fre_max at spot 0.6666666666666666\n",
      "DEBUG2 k_fake=[0, 1] k_nodes=[0, 1] touched=[2, 4] n=5 k=2 a=2 i_th=0\n",
      "M\n",
      "[[-0.12392 -0.12392 -0.12392 -0.00431  0.37608]]\n",
      "M\n",
      "[[-0.02392 -0.02392  0.17608 -0.00431 -0.12392]]\n",
      "champion:  [0, 1] [0.27093 0.27093] 0.0857077090595603\n",
      "DEBUG2 k_fake=[0, 3] k_nodes=[0, 3] touched=[2, 4] n=5 k=2 a=2 i_th=1\n",
      "M\n",
      "[[-0.13145  0.04597 -0.13145 -0.0629   0.27984]]\n",
      "M\n",
      "[[-0.03145  0.14597  0.16855 -0.0629  -0.22016]]\n",
      "DEBUG2 k_fake=[1, 3] k_nodes=[1, 3] touched=[2, 4] n=5 k=2 a=2 i_th=2\n",
      "M\n",
      "[[-0.07438 -0.12051 -0.12051 -0.04102  0.35642]]\n",
      "M\n",
      "[[ 0.02562 -0.02051  0.17949 -0.04102 -0.14358]]\n",
      "champion:  [1, 3] [0.29691 0.3819 ] 0.08520205145443646\n",
      "Minimizer finds its target agents: [1, 3]\n",
      "fla_min_fre: [0.16667 0.16667 0.16667 0.16667 0.16667 0.16667]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.08520205145443646 and Equi_Max = 0.1135444556245015\n",
      "MAX_last_100,  all\n",
      "[0.01 0.04] [0.4 0.8]\n",
      "[33 34] [33 34]\n",
      "Max Nodes: [2, 4] | Opinion: (0, 1)\n",
      "Max Nodes: [2, 4] | Opinion: (1, 0)\n",
      "MIN_last_100,  all\n",
      "[0.02 0.01 0.02] [0.6 0.4 0.2]\n",
      "Counter({(0, 3): 2, (1, 3): 2, (0, 1): 1}) Counter({(1, 3): 3, (0, 3): 2, (0, 1): 1})\n",
      "Max Pol: 0.1135444556245015 Min Pol: 0.08520205145443646\n"
     ]
    }
   ],
   "source": [
    "game = Game(s, A, L, k)\n",
    "result = game.run(game_rounds, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 1\n",
    "save_(result, k, experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
