{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real datanetwork with small sythetic network \n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "#%run pure_strategy_selection.ipynb  #include simple selection algorithm\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n",
    "from itertools import count\n",
    "import os.path\n",
    "\n",
    "save_path = 'C:/Users/xzhan/OneDrive/08102022 - updated Mixed Opinion/Karate Result'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Fixed initial condition + memeory = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers the opinion vector around 0\\n\",\n",
    "def mean_center(op, n):\n",
    "    ones = np.ones((n, 1))\n",
    "    x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "    return x\n",
    "    \n",
    "# compute number of edges, m\\n\n",
    "def num_edges(L, n):\n",
    "    m = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i > j and L[i,j] < 0:\n",
    "                m += 1            \n",
    "    return m\n",
    "\n",
    "# maximizing polarization only: \\\\bar{z}^T \\\\bar{z}   \n",
    "def obj_polarization(A, L, op, n):\n",
    "    op_mean = mean_center(op, n)\n",
    "    z_mean = np.dot(A, op_mean) \n",
    "    return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "# def obj_polarization_1(A, L, op, n):  #z_mean is the same as s_mean - according to Stanford paper theory\n",
    "#     z = np.dot(A, op) \n",
    "#     z_mean = mean_center(z, n)\n",
    "#     return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "# Calculate innate polarization\n",
    "def obj_innate_polarization(s, n):  \n",
    "#     np.set_printoptions(precision=5)\n",
    "    op_mean = mean_center(s, n)\n",
    "    return np.dot(np.transpose(op_mean), op_mean)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the network\n",
    "n = 3\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Network\n",
    "### 1. Make Random Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "weight1\n",
      "[[0.02]\n",
      " [0.02]\n",
      " [0.02]]\n",
      "G for agents completed!\n",
      "[[0.   0.02 0.02]\n",
      " [0.02 0.   0.02]\n",
      " [0.02 0.02 0.  ]]\n",
      "[[0.9623 0.0189 0.0189]\n",
      " [0.0189 0.9623 0.0189]\n",
      " [0.0189 0.0189 0.9623]]\n",
      "Column Sum\n",
      "[1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAG+CAYAAADsjWHpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA1w0lEQVR4nO3deZzO9f7/8ec1m5kwSCVFzTBxjWVoEI19LUpki0IIyTK7UenUoVSOQfomFZEOWU4pyZYjJDvDMMuFMTOk7PtgzHb9/ujwO0uKmWvmc13X53H/q9tNc3mek9v19H6/X5/P22K32+0CAMAkPIwOAABASaL4AACmQvEBAEyF4gMAmArFBwAwFYoPAGAqFB8AwFQoPgCAqVB8AABTofgAAKZC8QEATIXiAwCYCsUHADAVig8AYCoUHwDAVCg+AICpUHwAAFOh+AAApkLxAQBMheIDAJgKxQcAMBWKDwBgKhQfAMBUKD4AgKlQfAAAU6H4AACmQvEBAEyF4gMAmArFBwAwFYoPAGAqFB8AwFQoPgCAqVB8AABTofgAAKbiZXQAAEDxOZ11TV/uOirb8Yu6mJ0nf18vWe/1V88GVVSxTCmj4xnCYrfb7UaHAAA4VuLP5zV9fZo2HDglSbqWV3Dj13y9PGSX1Krm3RreMkj1qpY3JqRBKD4AcDPztmZqwgqbsvPy9Uff8BaL5OvlqbGdrOrbJKDE8hmNrU4AcCO/lV6qruYW/Om/a7dLV3PzNWFFqiSZpvwYbgEAN5H483lNWGG7pdL7d1dzCzRhhU17j54vnmBOhuIDADcxfX2asvPyC/Wz2Xn5+nB9moMTOSeKDwDcwOmsa9pw4NQfnun9EbtdWrf/lM5kXXNsMCdE8QGAG/hy19Eif4ZF0pcJRf8cZ0fxAYAbsB2/+B+PLBRGdl6BbMcuOSiR86L4AMANXMzOc9Dn5Drkc5wZxQcAbsDf1zFPp/n7ejvkc5wZxQcAbsB6r79KeRXtK93Xy0PWymUdlMh5UXwA4AZ6NKhS5M+wS+oRWvTPcXYUHwC4gStnT+iO8xmyFxRuwMVikVrXvNsUL66m+ADAhV25ckXjxo1TaGiomla4LL9ShTvr8/Xy1PBWQQ5O55woPgBwQXa7XYsXL1ZwcLBSUlKUkJCgD8aP1mudguXnfXtf7X7eHhrbyaqQKuWLJ6yT4SXVAOBi9uzZo4iICF24cEF///vf1aJFixu/dv1F09zOcHNcSwQALuLUqVP6y1/+om+++Ubjxo3T4MGD5enp+bv/7t6j5/Xh+jSt239KFv32cPp11+/ja13zbg1vFWSald51FB8AOLnc3Fx9+OGHeuutt/Tcc8/pjTfeUIUKFW7pZ89kXdOXCUdlO3ZJCUmpUu5VPduppXqEmvcGdrY6AcCJff/994qMjFTVqlW1YcMG1apV67Z+vmKZUnqxRXVJ0oyzW5SYmKgXWwwqjqgug+IDACeUlpam6OhopaSkaMqUKercubMsFkuRPrNcuXK6cOGCgxK6LqY6AcCJXLp0SWPGjFGTJk3UtGlTJScn66mnnipy6UlS+fLlKT5RfADgFAoKCjR37lxZrVadOHFC+/bt05gxY1SqlOPO4Vjx/YatTgAw2LZt2xQeHi5JWrJkiRo3blwsv0+5cuV0/vz5YvlsV8KKDwAM8uuvv6p///7q1q2bRowYoS1bthRb6Ums+K6j+ACghGVnZ+vdd99VSEiI7r//ftlsNvXv318eHsX7lUzx/YatTgAoIXa7Xd9++62io6NVt25dbd26VUFBJfd+zDJlyujq1avKy8uTl5d5v/7N+78cAEpQSkqKIiMjdfToUX300Udq3759iWfw8PBQ2bJldfHiRd15550l/vs7C7Y6AaAYnTt3ThEREWrZsqWefPJJJSYmGlJ617HdSfEBQLHIz8/XRx99JKvVqpycHKWkpCg8PFze3t6G5qL42OoEAIfbsGGDwsPDVb58ea1evVr169c3OtINPMRO8QGAwxw+fFijR4/Wtm3bFB8frx49ejjkjSuOxIqPrU4AKLIrV67ojTfeUGhoqOrUqaPU1FT17NnT6UpP4iF2iRUfABSa3W7XokWLFBcXp7CwMO3evVsPPPCA0bH+ECs+ig8ACmX37t0KDw9XVlaW5s+fr+bNmxsd6ZZQfGx1AsBtOXXqlIYOHaqOHTuqf//+2rlzp8uUnsRwi0TxAcAtyc3N1dSpU1WrVi2VLl1aNptNQ4YMkaenp9HRbgtnfGx1AsCfWrVqlaKiovTggw/qxx9/VHBwsNGRCo2tTooPAG7q4MGDio6Ols1m09SpU/XEE0845aTm7aD42OoEgP9x8eJFxcXF6dFHH1Xz5s2VlJSkJ5980uVLT+KMT6L4AOCGgoICzZkzR1arVadOnVJSUpLi4uIcegu60VjxsdUJAJKkrVu3Kjw8XJ6enlq6dKkaNWpkdKRiwXALKz4AJvfrr7+qX79+6t69u0aNGqVNmza5belJrPgkig+ASWVnZ+vtt99W3bp1VbVqVe3fv1/9+vUr9lvQjXbHHXcoNzdXubm5RkcxDFudAEzFbrdr6dKliomJUUhIiLZv367q1asbHavEWCyWG6u+u+66y+g4hqD4AJhGcnKyIiIidOzYMX388cdq166d0ZEMcf2cz6zF595regCQdPbsWY0aNUqtW7dWly5dlJiYaNrSkzjno/gAuK28vDzNmDFDwcHBys/PV0pKikaNGiUvL3Nvdpm9+Mz9Xx+A21q/fr0iIiJUoUIFff/996pXr57RkZwGxQcAbiQzM1OjR4/Wjh07FB8fr+7du7vFG1ccyexvb2GrE4BbuHz5sl5//XU1aNBAISEhSk1NVY8ePSi932H2h9hZ8QFwaXa7XQsXLtSYMWPUrFkz7dmzR1WrVjU6llNjqxMAXFRCQoLCw8N15coVffHFF2rWrJnRkVxCuXLl9MsvvxgdwzBsdQJwOSdPntSQIUP0xBNPaMCAAdqxYweldxs44wMAF5GTk6MpU6aodu3a8vf3l81m0+DBg13uFnSjsdUJAC5g5cqVioqKUrVq1fTTTz+pZs2aRkdyWQy3AIATO3DggKKjo3XgwIEbt6CjaMy+4mOrE4BTunDhgmJjYxUWFqZWrVopKSmJ0nMQig8AnEhBQYFmz54tq9Wqc+fOKSkpSbGxsfLx8TE6mtsw+3ALW50AnMbmzZsVHh4uHx8fLVu2TA0bNjQ6klvijA8ADHb06FGNGTNGGzZs0MSJE/Xss8/yxpVi5OvrK+m3y3iv/7OZsNUJwDDZ2dmaMGGC6tevr8DAQNlsNj333HOUXgkw8zkfKz4AJc5ut+vrr79WbGysHn74YW3fvl3VqlUzOpapXD/nq1SpktFRShzFB6BE7du3T5GRkTp58qRmzpyptm3bGh3JlMy84mOrE0CJOHPmjEaOHKm2bduqW7du2r17N6VnIDMPuFB8AIpVXl6epk+fruDgYElSamqqRowYYfpb0I1m5hUff/IAFJsffvhBERERuvvuu7V27VrVrVvX6Ej4F4oPABwoIyNDsbGxSkhI0OTJk/X0008zqelkzPwQO1udABzm8uXLeu2119SoUSOFhoYqJSVF3bp1o/ScEGd8AFAEdrtd8+fPl9VqVUZGhvbs2aOxY8fKz8/P6Gi4CbY6AaCQdu7cqYiICF27dk0LFy5U06ZNjY6EW2Dm4mPFB6BQTpw4oRdeeEGdO3fWCy+8oO3bt1N6LoQzPgC4RTk5OYqPj1ft2rVVoUIF2Ww2DRo0SB4efJ24EjOv+NjqBHDLli9frqioKD300EPatGkTt6C7MDMPt1B8AP7U/v37FRUVpUOHDum9995Tp06djI6EIjLzio+9CQA3deHCBcXExKhp06Zq27at9u3bR+m5CYoPAP5Nfn6+Pv30U1mtVl24cEHJycmKiYnhFnQ3cr347Ha70VFKHFudAP7Dpk2bFB4eLl9fX3333Xdq0KCB0ZFQDHx8fOTt7a0rV66odOnSRscpURQfAEm/3YIeFxenjRs3auLEierTpw9vXHFz11d9Zis+tjoBk7t69areeust1atXT9WrV5fNZtOzzz5L6ZmAWc/5WPEBJmW327VkyRLFxsaqQYMG2rlzpwIDA42OhRJk1ofYKT7AhPbu3avIyEidOnVKn376qdq0aWN0JBjArCs+tjoBEzlz5oxGjBihdu3aqUePHtq9ezelZ2JmfYid4gNMIC8vTx988IGCg4Pl4eEhm82m4cOHcwu6yZl1xcefesDNrV27VhEREapUqZJ++OEH1alTx+hIcBIUHwC3kp6ertjYWO3Zs0eTJ09W165dmdTEfzDrcAtbnYCbycrK0tixY/XII4+oYcOGSklJ0dNPP03p4X9wxgfApdntds2bN09Wq1WHDx9WYmKiXn31Vfn6+hodDU6KrU4ALmvHjh2KiIhQbm6uFi9erLCwMKMjwQWYtfhY8QEu7Pjx4xo0aJC6dOmiIUOGaNu2bZQebhlnfABcRk5OjiZNmqQ6deqoYsWKstlsGjhwILeg47aYdcXHVifgQux2u5YvX67o6GjVqFFDmzdvVo0aNYyOBRdl1uEWig9wETabTVFRUcrIyNC0adPUsWNHoyPBxZl1xce+CODkzp8/r+joaDVv3lzt27fX3r17KT04hL+/vy5dumS6y2gpPsBJ5efna+bMmbJarbp06ZKSk5MVHR3NLehwGC8vL/n5+SkrK8voKCWKrU7ACW3cuFEREREqXbq0VqxYodDQUKMjwU1dP+crW7as0VFKDCs+wIn8/PPP6tOnj5577jnFxcXpxx9/pPRQrMx4zkfxAU7g6tWrGj9+vOrXr68aNWooNTVVvXv35jVjKHZmLD62OgED2e12ffnllxo9erQeeeQR7dq1SwEBAUbHgolQfABKTGJioiIiInTu3Dl99tlnatWqldGRYEJmfHsLW51ACTt9+rReeukldejQQb1799auXbsoPRjGjA+xU3xACcnNzdX777+v4OBgeXt7KzU1VcOGDeMWdBiKrU4AxWLNmjWKjIzUfffdp/Xr16t27dpGRwIkUXwAHOzQoUOKiYnRvn37NGXKFD311FNMasKplC9fXkePHjU6RoliqxMoBpcuXdIrr7yixo0bq3HjxkpOTlaXLl0oPTgdzvgAFElBQYE+//xzWa1W/fLLL9q7d69eeeUVbkGH02KrE0Chbd++XeHh4SooKNBXX32lJk2aGB0J+FNmLD5WfEARHTt2TAMGDFDXrl01bNgwbd26ldKDy6D4ANyya9euaeLEiapbt64qVaokm82mAQMGcAs6XIoZH2BnqxO4TXa7Xd99952io6MVHBysLVu26KGHHjI6FlAoZhxusdjNdgMhUASpqamKjIzUkSNH9N577+mxxx4zOhJQJAUFBfL29lZOTo48PT2NjlMi2JMBbsH58+cVGRmpFi1aqGPHjtq7dy+lB7fg4eGhMmXK6NKlS0ZHKTEUH/AH8vPz9cknn8hqterq1atKSUlRZGSkvL29jY4GOIzZzvk44wNu4scff1RERITKli2rlStX6uGHHzY6ElAszDbZSfEB/+XIkSMaPXq0tmzZokmTJqlXr168cQVuzWwDLmx1Av9y5coVjRs3Tg8//LCCg4Nls9n0zDPPUHpwe6z4AJOx2+36xz/+odGjR6tJkyZKSEjQgw8+aHQsoMRQfICJ7NmzRxEREbpw4YI+//xztWzZ0uhIQIkz23ALW50wpVOnTmnYsGF67LHH9Oyzz2rXrl2UHkyLMz7AjeXm5mratGmqVauWfH19ZbPZ9OKLL5rmwV3g97DVCbip77//XpGRkapSpYo2bNigWrVqGR0JcArlypVTenq60TFKDMUHt5eWlqaYmBglJydrypQp6ty5M5OawL/hjA9wE5cuXdLLL7+sJk2aKCwsTMnJyXrqqacoPeC/mG2rk+KD2ykoKNDcuXNltVp17Ngx7d27V2PGjFGpUqWMjgY4JbMNt7DVCbeybds2hYeHS5KWLFmixo0bG5wIcH6s+AAXdOzYMT3//PPq1q2bRowYoS1btlB6wC2i+AAXcu3aNb377ruqW7euKleuLJvNpv79+3MLOnAbzDbcwlYnXJLdbte3336rmJgY1a5dW1u3blVQUJDRsQCXVLp0aWVnZys3N9cUV25RfHA51+/EO3r0qD788EN16NDB6EiAS7NYLPL399fFixdVsWJFo+MUO/aD4DLOnTuniIgItWzZUk888YQSExMpPcBBzHTOR/HB6eXn5+ujjz6S1WrVtWvXlJKSooiICFNsyQAlxUznfGx1wqlt2LBBERERKleunFavXq369esbHQlwS2Za8VF8cEqHDx/W6NGjtW3bNk2aNEk9e/bkjStAMTLTQ+xsdcKpXLlyRW+88YZCQ0NVu3ZtpaamqlevXpQeUMxY8QElzG63a9GiRYqLi1NYWJh2796tBx54wOhYgGlQfEAJ2r17t8LDw5WVlaV58+apRYsWRkcCTMdMwy1sdcIwp06d0tChQ9WxY0f169dPO3fupPQAg3DGBxSj3NxcTZ06VbVq1VLp0qWVmpqqoUOHcgs6YCC2OoFismrVKkVFRemBBx7Qjz/+qODgYKMjARDFBzjcwYMHFR0dLZvNpilTpujJJ59kUhNwIpzxAQ5y8eJFxcXF6dFHH1Xz5s2VlJSkzp07U3qAkzHTio/iQ7EoKCjQnDlzZLVaderUKSUlJSkuLo5b0AEnZabhFrY64XBbt25VeHi4PD09tXTpUjVq1MjoSAD+BCs+oBB+/fVX9evXT927d9eoUaO0adMmSg9wERQfcBuys7P19ttvq27duqpatar279+vfv36cQs64EL8/PyUn5+va9euGR2l2LHViUKz2+1aunSpYmJiFBISou3bt6t69epGxwJQCBaL5caq75577jE6TrGi+FAoycnJioiI0LFjx/Txxx+rXbt2RkcCUERmKT72onBbzp49q1GjRql169bq0qWLEhMTKT3ATZjlnI/iwy3Jy8vTjBkzFBwcrPz8fKWkpGjUqFHy8mLTAHAXZnmInW8t/Kn169crIiJCFSpU0Pfff6969eoZHQlAMTDLio/iw01lZmZq9OjR2rFjh+Lj49W9e3feuAK4MbM8xM5WJ/7H5cuX9frrr6tBgwYKCQlRamqqevToQekBbo4VH0zHbrdr4cKFGjNmjJo1a6Y9e/aoatWqRscCUEIoPphKQkKCwsPDdeXKFX3xxRdq1qyZ0ZEAlLDy5cvryJEjRscodmx1mtzJkyc1ZMgQderUSQMGDNCOHTsoPcCkOOODW8vJydGUKVNUu3Zt+fv7y2azafDgwdyCDpgYW51wWytXrlRUVJQCAwO1ceNGWa1WoyMBcAIUH9zOgQMHFB0drQMHDmjq1Knq1KkTk5oAbjBL8bHVaQIXL17U6NGjFRYWplatWikpKUlPPPEEpQfgP5jlzS0UnxsrKCjQ7NmzVbNmTZ05c0ZJSUmKjY2Vj4+P0dEAOCGzDLew1emmNm/erPDwcPn4+Ojbb7/lQlgAf+r6VqfdbnfrHSFWfG7ml19+Ud++fdWrVy9FRkZyCzqAW1aqVCl5eHgoOzvb6CjFiuJzE9nZ2ZowYYJCQkL04IMPymazqW/fvm79tzYAjmeGcz62Ol2c3W7XN998o5iYGNWvX187duxQtWrVjI4FwEVdP+e79957jY5SbCg+F5aUlKSIiAidOHFCM2fOVNu2bY2OBMDFmeGRBrY6XdDZs2c1cuRItWnTRk8//bT27NlD6QFwCIoPTiUvL08ffvihgoODZbfblZqaqpEjR3ILOgCHMUPx8Y3pItatW6eIiAhVrFhRa9asUUhIiNGRALghhltguIyMDMXGxiohIUHx8fHq1q0bk5oAio0ZHmJnq9NJXb58Wa+99poaNmyohx9+WCkpKerevTulB6BYmWGrk+JzMna7XfPnz5fValVGRoYSExP12muvyc/Pz+hoAEzADMXHVqcT2blzpyIiIpSdna2FCxeqadOmRkcCYDJmOONjxecETpw4oRdeeEGdO3fWoEGDtH37dkoPgCHMsOKj+AyUk5Oj+Ph41a5dWxUqVJDNZtMLL7zALegADGOG4Ra2Og2yfPlyRUVFKSgoSJs2bVLNmjWNjgQApljxUXwlbP/+/YqKitKhQ4f03nvvqVOnTkZHAoAbzFB8bHWWkAsXLigmJkZNmzZV27ZttW/fPkoPgNNhuAVFlp+fr08//VRWq1Xnz59XcnKyYmJiuAUdgFPy9/e/cRmtu2Krsxht2rRJ4eHh8vX11bJly9SwYUOjIwHAH/L29lapUqV0+fJllSlTxug4xYLiKwZHjx5VXFycNm7cqIkTJ6pPnz68cQWAy7h+zueuxcdWpwNdvXpVb731lurVq6dq1aopNTVVzz77LKUHwKW4+zkfKz4HsNvtWrJkiWJjYxUaGqqdO3cqMDDQ6FgAUCjuPtlJ8RXR3r17FRkZqVOnTunTTz9VmzZtjI4EAEXi7g+xs9VZSGfOnNGIESPUrl079ejRQ7t376b0ALgFd1/xUXy3KS8vTx988IGCg4Pl4eEhm82m4cOHcws6ALfh7sXHt/VtWLt2rSIiIlSpUiX98MMPqlOnjtGRAMDhGG6B0tPTFRsbqz179mjy5Mnq2rUrk5oA3BZnfCaWlZWlsWPH6pFHHlHDhg2VkpKip59+mtID4NbcfauT4vsddrtd8+bNk9Vq1eHDh5WYmKhXX31Vvr6+RkcDgGLn7sXHVud/2bFjhyIiIpSbm6vFixcrLCzM6EgAUKLc/YyPFd+/HD9+XIMGDVKXLl00ZMgQbdu2jdIDYEruvuIzffHl5ORo0qRJqlOnjipWrCibzaaBAwfKw8P0/9cAMCl3H24x7Van3W7X8uXLFR0drRo1amjz5s2qUaOG0bEAwHDuvuIzZfHZbDZFRUUpIyND06ZNU8eOHY2OBABOw92Lz1T7eefPn1d0dLSaN2+u9u3ba+/evZQeAPwXf39/ZWVlqaCgwOgoxcJlVnyns67py11HZTt+URez8+Tv6yXrvf7q2aCKKpYp9Yc/m5+fr9mzZ+svf/mLOnfurOTkZN1zzz0llBwAXIunp6fuuOMOXbp0SeXKlTM6jsM5ffEl/nxe09enacOBU5Kka3n//28gvl7HNfWfB9Sq5t0a3jJI9aqW/5+f/+mnnxQeHq477rhDK1asUGhoaElFBwCXdX27k+IrYfO2ZmrCCpuy8/Jlt//vr2f/qwS/TzmhHw+c1thOVvVtEiBJ+vnnnxUXF6effvpJf/vb39S7d2/euAIAt8idz/mctvh+K71UXc398z1mu126mpuvCStSlZObq6PrFmjatGkaMWKEZs2apdKlS5dAYgBwH+78ELtTFl/iz+c1YYXtlkrv313NLdD4b/ep1rFftWvXLgUEBBRPQABwc+684nPKqc7p69OUnZdfqJ+1eJXSA48PpvQAoAjc+SF2pyu+01nXtOHAqd8907sVdknr9p/SmaxrDs0FAGbCiq8EfbnraJE/wyLpy4Sifw4AmBXFV4Jsxy/+xyMLhZGdVyDbsUsOSgQA5uPOwy1OV3wXs/Mc9Dm5DvkcADAjzvhKkL+vYwZN/X29HfI5AGBGbHWWIOu9/irlVbRYvl4eslYu66BEAGA+FF8J6tGgSpE/Iy8/X13rVXZAGgAwJ4qvBN1VppRa1rhbhX27mEV2eZ8+oMb1aum9997TpUsMuQDA7WK4pYSNaBUkXy/PQv2sr7eXFr0+SIsWLdLmzZsVGBioV155Rb/++quDUwKA+2K4pYTVq1peYztZ5ed9e/H8vD00tpNVIVXKq3Hjxlq8eLG2b9+uy5cvq06dOho4cKCSk5OLKTUAuA+2Og3Qt0mAxnYKlp+3559ue1oskp+3p8Z2Cr5xO8N11apV0/vvv6+0tDQFBQWpXbt26tSpk3744QfZC/t6GABwc2XKlNGVK1eUn1+410c6M4vdyb/99x49rw/Xp2nd/lOy6P9fRST9Nr1pl9S65t0a3ipIIVXK/+nnZWdna/78+YqPj5efn59iY2PVs2dPeXvz+AMA/LsKFSooPT1dFSpUMDqKQzl98V13Juuavkw4KtuxS7qYnSt/X29ZK5dVj9A/v4H99xQUFGjFihWKj49XRkaGIiMjNXjwYJUty2MQACBJAQEBWrdunQIDA42O4lAuU3zFaceOHZo8ebLWrFmjwYMHKzw8XPfff7/RsQDAUPXq1dPcuXNVv359o6M4lNOe8ZWkRo0aaeHChdq5c6eys7NVt25dDRgwQPv27TM6GgAYxl0HXCi+fxMYGKhp06YpLS1NNWvW1GOPPabHH39ca9euZRAGgOlQfCZy55136pVXXlFGRoZ69eqlUaNGKTQ0VPPnz1duLi+/BmAO7voQO8X3B0qVKqVBgwYpKSlJEyZM0KxZs1S9enVNnjxZFy9eNDoeABQrd32IneK7BR4eHurUqZPWrVunJUuWaOfOnQoMDFRcXJyOHuXCWwDuia1OSJIaNmyoBQsWaNeuXcrNzVVISIj69++vvXv3Gh0NAByK4sN/CAgI0NSpU3Xo0CHVqlVLjz/+uB577DGtWbOGQRgAboEzPvyuChUq6OWXX1ZGRob69OmjqKgoPfzww5o3bx6DMABcGis+/KFSpUrdePbvnXfe0Zw5c1StWjXFx8e75R8cAO6P4RbcEovFoo4dO2rt2rX65ptvlJCQoGrVqmn06NH6+eefjY4HALeMFR9uW4MGDfTFF18oISFB+fn5qlevnvr166fExESjowHAn6L4UGgPPvigpkyZovT0dNWpU0edOnVShw4d9P333zMIA8BpuetwCy+pNkBOTo4WLFig+Ph4WSwWxcbGqnfv3vLx8TE6GgDccPnyZd199926cuWK0VEciuIzkN1u1+rVqxUfHy+bzaaIiAgNHTpU5cqVMzoaAMhut8vHx0dXrlxxqztL2eo0kMVi0eOPP65//vOfWrZsmRITExUYGKiYmBgdOXLE6HgATM5iscjf39/ttjspPidx/dm/PXv2SJLq16+vvn37avfu3cYGA2Bq7njOR/E5mQceeECTJ09Wenq66tWrp86dO6tdu3ZatWoVgzAASpw7TnZSfE6qfPnyGj16tNLT09W/f3/FxcUpJCREc+fOVU5OjtHxAJiEOz7ETvE5OR8fH/Xv31+JiYmaPHmy5s2bp8DAQE2cONHt/jACcD6s+GAYi8WiDh06aM2aNVq+fLmSkpJUrVo1RUdH6/Dhw0bHA+CmKD44hfr16+vvf/+7EhMT5enpqdDQUD377LNKSEgwOhoAN8NwC5xK1apVNWnSJKWnpys0NFRdunRR27ZttXLlSgZhADgEZ3xwSuXKlVNsbKwOHTqkgQMH6pVXXlHdunX12Wef6dq1a0bHA+DC2OqEU/Px8bnx7N/UqVO1YMECVatWTe+++67OnTtndDwALojig0uwWCxq3769Vq9erRUrViglJUXVq1dXZGSkMjMzjY4HwIVwxgeXU69ePX3++efau3evfHx81KBBA/Xp00e7du0yOhoAF8CKDy6rSpUq+tvf/qaMjAw1atRITz/9tFq3bq3ly5eroKDA6HgAnBTDLXB5/v7+io6O1qFDhzR48GC99tprqlu3rmbPns0gDID/wYoPbsPb21vPPfecEhIS9P7772vx4sUKDAzUO++8wyAMgBsoPrgdi8Witm3batWqVVq1apX279+v6tWrKyIiQhkZGUbHA2Awhlvg1kJCQvTZZ59p37598vX1VcOGDfXMM89o586dRkcDYBBfX1/Z7XZlZ2cbHcVhKD78j/vvv18TJ05URkaGmjRpou7du6tVq1b67rvvGIQBTMjdtjspPtyUv7+/oqKilJaWphdffFGvv/666tSpo08//dSt/vYH4I9RfDAdb2/vG8/+ffDBB/rqq68UGBioCRMm6OzZs0bHA1DM3O2cj+LDLbNYLGrTpo1WrFihNWvWKC0tTUFBQQoPD1d6errR8QAUE1Z8gKQ6depozpw5SkpKUunSpfXII4+oV69e2r59u9HRADiYuz3ETvGhSO677z698847ysjIUFhYmHr27KkWLVpo2bJlDMIAboIVH/A7ypYtq8jISB06dEjDhw/XX//6V9WqVUszZ85kEAZwcRQf8Ae8vLzUu3dv7dy5UzNmzNA333yjgIAAvfXWWzpz5ozR8QAUAsMtwC2wWCw3XoK9du1aZWRkKCgoSCNHjtShQ4eMjgfgNnDGB9ym2rVr69NPP1VKSor8/f3VuHFj9ezZU9u2bTM6GoBbwFYnUEiVK1fW22+/rczMTDVv3ly9e/dW8+bNtXTpUgZhACdG8QFFVKZMGYWHh+vgwYMaOXKk3nzzTQUHB+uTTz7R1atXjY4H4L9QfICDeHl56ZlnntGOHTv0ySefaNmyZQoICND48eN1+vRpo+MB+BeGWwAHs1gsatmypZYtW6Z169bpyJEjeuihhzRixAilpaUZHQ8wPYZbgGJUq1YtzZo1S6mpqSpfvvyN2yG2bt1qdDTAtNxtq9Nit9vtRocAbiYrK0tz5szRlClTdP/99ys2NladO3eWp6en0dEA08jJyVHp0qWVk5Mji8VidJwio/jgEvLy8vT1119r0qRJOn/+vKKjo/X888/Lz8/P6GiAKdxxxx06ffq07rjjDqOjFBlbnXAJXl5eN579mzVrllasWKGAgACNGzdOp06dMjoe4Pbc6ZyP4oNLsVgsatGihb799ltt2LBBR48eVY0aNfTSSy/p4MGDRscD3JY7nfNRfHBZVqtVM2fOVGpqqu666y6FhYWpW7du2rx5s9HRALdD8QFO5N5779Wbb76pzMxMtWnTRn379lVYWJi+/vpr5efnGx0PcAsUH+CESpcurZEjR+rgwYOKjo7Wu+++K6vVqhkzZujKlStGxwNcmjs9xE7xwe14enqqR48e2rp1q+bMmaNVq1YpICBAb7zxhk6ePGl0PMAlMdwCuACLxaJmzZpp6dKl2rhxo44fP66aNWtq2LBhOnDggNHxAJfCVifgYmrWrKmPP/5YNptN99xzj5o1a6ann35amzZtEo+yAn+O4gNcVKVKlTR+/HhlZGSoffv2ev755xUWFqavvvqKQRjgD3DGB7i40qVLa/jw4dq/f79iY2M1adIk1axZUx9++CGDMMDv4IwPcBOenp7q3r27tmzZorlz52rNmjUKCAjQ66+/rhMnThgdD3AabHUCbsZisahp06b6+uuvtXHjRp08eVJWq1VDhw7V/v37jY4HGI7iA9xYzZo19dFHH2n//v2qXLmymjdvri5dumjjxo0MwsC0KD7ABO655x6NGzdOmZmZevzxxzVo0CA9+uij+vLLLxmEgem403AL1xIBtyg/P1/ffvutJk2apBMnTigqKkoDBw5U6dKljY4GFLuzZ8+qevXqOnfunNFRioziAwph8+bNio+P18aNGzVs2DCNHDlSlSpVMjoWUGzy8vLk6+ur3Nxcl7+Mlq1OoBDCwsK0ZMkSbd68WadPn5bVatWQIUOUmppqdDSgWHh5ecnX11dZWVlGRykyig8ogoceekgzZszQgQMHVKVKFbVq1UpPPfWUfvzxRwZh4Hbc5ZyP4gMc4O6779Ybb7yhzMxMderUSYMHD1bjxo31j3/8Q3l5eUbHAxzCXSY7KT7Agfz8/DRs2DClpqbq1Vdf1bRp01SjRg393//9n1tsEcHc3OXtLRQfUAw8PT3VtWtX/fTTT5o/f77Wr1+vwMBAjR07VsePHzc6HlAorPgA3JJHH31UX331lbZs2aLz588rODhYgwcPVkpKitHRgNtC8QG4LUFBQZo+fboOHjyoBx54QK1bt9aTTz6p9evXMwgDl8BwC4BCueuuu/T6668rMzNTTz31lF588UU98sgjWrRoEYMwcGqc8QEoEj8/Pw0dOlSpqal67bXX9MEHH+ihhx7S+++/zyAMnBJbnQAcwsPD48ZLsBcsWKCNGzcqICBAr776qo4dO2Z0POAGig+AwzVp0kT/+Mc/tG3bNl28eFG1atXSoEGDlJycbHQ0gDM+AMWnevXq+uCDD5SWlqZq1aqpbdu2euKJJ7Ru3ToGYWAYVnwAil3FihX12muvKTMzU127dtVLL72khg0bauHChQzCoMQx3AKgxPj6+mrIkCFKSUnRX//6V82YMUNBQUF67733dOnSJaPjwSRY8QEocR4eHurcubM2bNigRYsWafPmzQoMDNQrr7yiX3/91eh4cHMUHwBDNW7cWIsXL9b27dt1+fJl1alTRwMHDlRSUpLR0eCmGG4B4BSqVaum999/XwcPHlRQUJDat2+vjh076ocffmAQBg5VtmxZZWVlKT8/3+goRcIN7ICbyc7O1vz58xUfHy8/Pz/FxsaqZ8+e8vb2Njoa3EC5cuV0+PBhlS9f3ugohcaKD3Azvr6+euGFF5ScnKzx48frk08+UVBQkKZOncogDIrMHc75KD7ATXl4eNx4CfaXX36pbdu2KSAgQGPGjNEvv/xidDy4KHc456P4ABNo1KiRFi5cqJ07dyo7O1t169bVgAEDtG/fPqOjwcWw4gPgUgIDAzVt2jSlpaWpRo0a6tChgx5//HH985//ZBAGt8QdHmKn+AATuvPOO/Xqq68qMzNTvXr1Unh4uEJDQzV//nzl5uYaHQ9OjBUfAJdWqlQpDRo0SElJSZowYYJmzZql6tWra/Lkybp48aLR8eCEKD4AbsHDw0OdOnXSunXrtGTJEu3cuVOBgYGKi4vT0aNHjY4HJ8JwCwC307BhQy1YsEC7du1Sbm6uQkJC1L9/fyUmJhodDU6AMz4AbisgIEBTp07VoUOHVKtWLXXs2FGPPfaY1qxZwyCMibHVCcDtVahQQS+//LIyMjLUu3dvRUZGqn79+vr73/+unJwco+OhhFF8AEyjVKlSN16C/e677+qzzz5T9erVFR8f7/JfhLh1nPEBMB2LxaKOHTtq7dq1+uabb5SQkKDAwEDFxsbq559/NjoeihkrPgCm1qBBA33xxRfavXu3CgoKVK9ePfXr10979uwxOhqKCcMtACDpwQcf1JQpU5Senq46deroiSeeUPv27fX9998zCONm3GHFx7VEABwuJydHCxYsUHx8vCwWi2JjY9W7d2/5+PgYHQ1FdOnSJVWuXFlZWVlGRyk0ig9AsbHb7Vq9erXi4+Nls9kUHh6uoUOHuvRdbmZnt9vl7e2t7OxseXl5GR2nUNjqBFBsLBbLjZdgL1u2THv37lW1atUUExOjI0eOGB0PhWCxWOTv7+/S250UH4AS8fDDD2vevHk3Bl/q16+vvn37avfu3cYGw21z9XM+ig9AiXrggQc0efJkpaenq169eurcubPatWunVatWMQjjIig+ACiE8uXLa/To0UpPT1f//v0VFxenkJAQzZ07lzfCODmKDwCKwMfH58ZLsCdPnqx58+YpMDBQEydOdPnnxdyVq7+9heID4BQsFos6dOigNWvWaPny5UpKSlK1atUUFRWlw4cPGx0P/8bVH2Kn+AA4nesvwU5MTJSXl5dCQ0P17LPPKiEhwehoEFudAFBsqlatqkmTJik9PV2hoaHq0qWL2rZtq5UrVzIIYyCKDwCKWbly5RQbG6tDhw5pwIABevnll1W3bl3NmTNH165dMzqe6XDGBwAlxMfH58ZLsKdOnaqFCxcqMDBQ7777rs6dO2d0PNPgjA8ASpjFYlH79u21evVqrVy5UikpKapevboiIyOVmZlpdDy3x1YnABioXr16+vzzz7V37175+PioQYMG6t27t3bt2mV0NLdF8QGAE6hSpYr+9re/KSMjQ40aNVLXrl3VunVrLV++XAUFBUbHcysUHwA4EX9/f8XExCg9PV2DBw/W2LFjVbduXc2ePZtBGAdhuAUAnJC3t7eee+457d69W9OmTdPixYsVEBCgt99+W2fPnjU6nktjuAUAnJjFYrnxEuzVq1frwIEDCgoKUkREhDIyMoyO55LY6gQAFxESEqLPPvtM+/btk6+vrxo2bKhnnnlGO3bsMDqaS/Hz81NeXp7Lvkyc4gNgOvfff78mTpyojIwMNWnSRN27d1fLli313XffMQhzCywWi0uf81F8AEzL399fUVFROnTokF588UX95S9/Ue3atTVr1ixlZ2cbHc+pufI5H8UHwPS8vb1vvAR7+vTp+uqrrxQYGKgJEybozJkzRsdzSq58zkfxAcC/WCwWtWnTRitXrtSaNWuUlpamoKAgjRo1Sunp6UbHcyoUHwC4mTp16mjOnDlKTk5WmTJl9Mgjj6hXr17avn270dGcAsUHAG7qvvvu0zvvvKOMjAyFhYWpZ8+eatGihb799ltTD8Iw3AIAbq5s2bKKjIzUoUOH9NJLL2ncuHGqVauWZs6cacpBGIZbAMAkvLy81KdPH+3cuVMzZszQN998o4CAAL355pumGoRhqxMATMZisdx4CfbatWuVkZGhoKAgjRw5UocOHTI6XrGj+ADAxGrXrq3Zs2crJSVF/v7+aty4sXr06KGtW7caHa3YcMYHAFDlypX19ttvKzMzUy1atFCfPn3UvHlzLV261O0GYVjxAQBuKFOmjMLDw3Xw4EGNHDlSb775poKDg/Xxxx/r6tWrRsdzCIZbAAD/w8vL68ZLsD/++GMtW7ZMAQEBGj9+vE6fPm10vCJhxQcAuCmLxaJWrVrpu+++07p163TkyBE99NBDGjFihNLS0oyOVygUHwDgltSqVUuzZs1Samqqypcvf+N2iC1bthgd7ba48nCLxW63240OAQBmlZWVpTlz5mjKlCm67777NHr0aHXu3Fmenp5GR/tD165dU9myZXXt2jVZLBaj49wWig8AnEBeXp6WLFmiSZMm6cKFC4qOjtbzzz8vPz8/o6PdlK+vr86dO+fUGX8PW50A4AS8vLxuvAR71qxZWrFihQICAjRu3DidOnXK6Hi/y1XP+Sg+AHAiFovlxkuw169fr6NHj6pGjRp66aWXdPDgQaPj/QdXPeej+ADASQUHB2vmzJlKTU3VXXfdpbCwMHXr1k2bN282OpokVnwAgGJy77336s0331RmZqbatGmjvn37KiwsTEuWLFF+fr5huVz1IXaKDwBcROnSpTVy5EgdPHhQ0dHRmjhxoqxWq2bMmKErV66UeB5WfACAEuHp6XnjJdizZ8/WqlWrFBAQoDfeeEMnT54ssRwUHwCgRFkslhsvwf7xxx91/Phx1axZU8OGDdOBAweK/fdnuAUAYBir1aqPP/5YNptN99xzj5o2baquXbvqp59+UnE9rs0ZHwDAcJUqVdL48eOVmZmpDh06aMCAAQoLC9NXX33l8EEYtjoBAE6jdOnSGj58uPbv36/Y2FhNmjRJNWrU0PTp03X58mWH/B6uWny8sgwATMBut2vTpk2Kj4/Xpk2b9NJLL2nEiBGqVKlSoT7vdNY1jft8tX7al66HmzSVv6+XrPf6q2eDKqpYppSD0zsWxQcAJrN//35NnTpVixYtUs+ePRUTE6OaNWve0s8m/nxe09enacOBUyooKFDuv10s7+vlIbukVjXv1vCWQapXtXyx5C8qig8ATOrkyZOaPn26ZsyYoUcffVSxsbFq1qzZTW9bmLc1UxNW2JSdl68/ag6LRfL18tTYTlb1bRJQPOGLgOIDAJO7cuWK5s6dqylTpujOO+9UbGysunXr9h9XI/1Weqm6+u9LvD/h5+2hsZ2Cna78KD4AgCQpPz9fS5cu1aRJk3TixAlFR0dr4MCBSjubq94zt+pq7u1Phfp5e2rR0CYKqVLe8YELieIDAPyPzZs3Kz4+Xhs3bpR18GQdVUUVpiwsFumxWpX0Ud+GDs9YWBQfAOCmtiWmqM+CNBVYCn8jfCkvD20e08Zppj15jg8AcFO7z5eSt7d3kT7DIunLhKOOCeQAFB8A4KZsxy/qWt6tD7T8nuy8AtmOXXJQoqKj+AAAN3UxO89Bn5PrkM9xBIoPAHBT/r5eDvqcom2XOhLFBwC4Keu9/irlVbSq8PXykLVyWQclKjqKDwBwUz0aVCnyZ9gl9Qgt+uc4CsUHALipu8qUUssad+smbzH7UxaL1Lrm3U7zKINE8QEA/sSIVkHy9Srcc3y+Xp4a3irIwYmKhuIDAPyhelXLa2wnq/y8b68yfntXp9WpXlcmSY4Z1wEAuLXrL5rmdgYAgKnsPXpeH65P07r9p2TRbw+nX3f9Pr7WNe/W8FZBTrfSu47iAwDctjNZ1/RlwlHZjl3Sxexc+ft6y1q5rHqEcgM7AABOheEWAICpUHwAAFOh+AAApkLxAQBMheIDAJgKxQcAMBWKDwBgKhQfAMBUKD4AgKlQfAAAU6H4AACmQvEBAEyF4gMAmArFBwAwFYoPAGAqFB8AwFQoPgCAqVB8AABTofgAAKZC8QEATIXiAwCYCsUHADAVig8AYCoUHwDAVCg+AICpUHwAAFOh+AAApkLxAQBMheIDAJgKxQcAMBWKDwBgKhQfAMBUKD4AgKlQfAAAU/l/egruph0sT8oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ############################ Make Innate Opinion ################################\n",
    "# ##create two set of weights connected with density 1) inviduals  2) individual & informaton Source\n",
    "c1 = np.sort(np.random.choice(n, n, replace=False)) #assume (1-r) are individuals\n",
    "# print('c1')\n",
    "# print(c1)\n",
    "l1 = len(c1)\n",
    "\n",
    "\n",
    "##################################Creating Adjacency Matrix ########################\n",
    "np.set_printoptions(precision=4)\n",
    "### Prepare for create adjacent matrix\n",
    "p1 = 1 # density within ind.\n",
    "p2 = 0 # density of edges between Info Source and Indivisuals\n",
    "\n",
    "#pre_weights1 = scipy.sparse.random(1, int(0.5*l1*(l1 - 1)), density=p1).A[0] \n",
    "# pre_weights1 = scipy.sparse.rand(1, int(0.5*l1*(l1 - 1)), density=p1, format='coo', dtype=None, random_state=None)\n",
    "pre_weights1  = np.full((3, 1), 1/2, dtype=float)\n",
    "# pre_weights1[0] = 0\n",
    "print(pre_weights1)\n",
    "\n",
    "\n",
    "weights1 = pre_weights1 /25\n",
    "\n",
    "\n",
    "print(\"weight1\")\n",
    "print(weights1)\n",
    "weights1.shape\n",
    "\n",
    "\n",
    "# create n x n adjacency matrix with existing init_G\n",
    "G = np.zeros((n, n))\n",
    "    \n",
    "## Assign edges between ind to ind \n",
    "idx = 0\n",
    "for i in c1:\n",
    "    for j in c1:\n",
    "            if i == j:\n",
    "                G[i][j] =0\n",
    "                continue\n",
    "            elif i < j:\n",
    "                G[i][j] = weights1[idx]\n",
    "                idx += 1\n",
    "#                 print(idx)\n",
    "#                 print (G1[i][j])\n",
    "            else:\n",
    "                G[i][j] = G[j][i]\n",
    "print(\"G for agents completed!\")\n",
    "print(G)\n",
    "\n",
    "L = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "A = np.linalg.inv(np.identity(n) + L)\n",
    "m = num_edges(L, n)\n",
    "print(A)\n",
    "columnsum_ij = np.sum(A, axis=0)\n",
    "print('Column Sum')\n",
    "print(columnsum_ij)\n",
    "##what the twitter graph looks like \n",
    "nxG = nx.from_numpy_matrix(G)\n",
    "plt.figure(figsize=(6, 6))\n",
    "nx.draw(nxG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Equilibrium & Polarization  - based on derivation\n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\n",
      "0.5\n",
      "Equi_polarization:\n",
      "0.4449982200071198\n",
      "Difference:\n",
      "-0.055001779992880195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## s =  make_innat_opinions(n, c1)\n",
    "# print('Innate Opinion')\n",
    "# print(s)\n",
    "# print('Equilibrium Opinion')\n",
    "# print(np.dot(A, s))\n",
    "# s = make_innat_opinions(n, c1)\n",
    "# op = s\n",
    "# print(s)\n",
    "n = 3\n",
    "s_0=[1]\n",
    "s_1=[0.5]\n",
    "s_2=[0]\n",
    "s = np.vstack((s_0,s_1,s_2))\n",
    "\n",
    "\n",
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, s, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing players' behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "  \n",
    "    v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "    new_op = random.randint(0, 1)  # randomly select an opininon between 0 and 1\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play1(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "#     max_opi_option = random.uniform(0, 1)   # options that maximizer have\n",
    "    \n",
    "    v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "#     v = 1\n",
    "    new_op = random.uniform(0, 1)  # randomly select an opininon between 0 and 1\n",
    "    #new_op = 0\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# min_touched =[]\n",
    "# (v1, maxmize_op, innat_equi_por, max_por) = choose_max_vertex(s, n, min_touched)\n",
    "# print(v1, maxmize_op, innat_equi_por, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# (v2, min_opinion, min_pol) = random_play(s,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizer_fir_play(s,n,min_touched):    # maxmizer first-time play, greedy algorithm\n",
    "    op = copy.copy(s)\n",
    "\n",
    "    print('Maximizer Play')\n",
    "\n",
    "    max_champion = choose_max_vertex(op, n, min_touched) # The best choice among all opinions and vertexs, function is in \"pure_strategy_selection.ipynb\"\n",
    "    (v1, max_opinion, innate_obj, max_pol) = max_champion # find agent v1, and max_opinion that can maxmize the equi_polarization(max_pol)\n",
    "\n",
    "    if v1 == None:   # if maximizer cannot find one\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "#         print('v1', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(max_champion)\n",
    "\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        ##### change the agent's opinion with best action(agent v1, max_op)\n",
    "        op[v1,0] = max_opinion\n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_touched = []\n",
    "# min_touched = []\n",
    "# (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "# print(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### minimizer first-time play, greedy algorithm\n",
    "def minimizer_fir_play(s,n,max_touched): \n",
    "    \n",
    "    op = copy.copy(s)\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = choose_min_vertex(op, n, max_touched)\n",
    "    (v2, min_opinion, innat_equi_por, min_pol) = min_champion\n",
    "    \n",
    "   #Store innate_op of the min_selected vertex\n",
    "    old_opinion_min = op[v2,0]\n",
    "    \n",
    "    if v2 == None:\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "\n",
    "        ##### change the agent's opinion\n",
    "        op[v2,0] = min_opinion   #-------------------------------------------------> store minimize strategy\n",
    "\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return (v2,min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_touched = []\n",
    "# min_touched = []\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "# print(v2, min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing above functions\n",
    "# min_touched=[]\n",
    "# max_touched=[]\n",
    "# # Game start from maximizer random play\n",
    "# print('Maximizer random selection')\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# max_touched.append(v1)\n",
    "# # print('v1, max_opinion, max_pol')\n",
    "# # print(v1, max_opinion, max_pol)\n",
    "# # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Game start from minimizer random play \n",
    "# print('Minimizer random selection')\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "# min_touched.append(v2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row are Column are depended on min and max's choice: agent v and opinion \n",
    "def row_index(v2, min_opinion):\n",
    "    row = 11*v2 + min_opinion*10 \n",
    "    return int(row)\n",
    "def column_index(v1,max_opinion):\n",
    "    column = 2*v1 + max_opinion\n",
    "    return int(column)  #the python dataframe index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Strategy Payoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_payoff_row(op1,v2):\n",
    "    payoff_row = np.zeros(2*n)\n",
    "\n",
    "#     print('one opinion changed -min')\n",
    "#     print(op1)\n",
    "    for column in range(2*n):\n",
    "#         print(column)\n",
    "        v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "        max_opinion = column%2\n",
    "#         print(v1, max_opinion)\n",
    "        # update the maximizer's change to the opinion array that has changed by minimizer(op1)\n",
    "        op2 = copy.copy(op1)\n",
    "#         temp = op1[v1]\n",
    "        op2[v1,0] = max_opinion\n",
    "\n",
    "        # calculate the polarization with both max and min's action\n",
    "        payoff_row[column] = obj_polarization(A, L, op2, n)\n",
    "#         op1[v1,0] = temp # restore\n",
    "#         print(op2,payoff_row[column])\n",
    "\n",
    "        ############# CAN DELETE \n",
    "#         if column==33:\n",
    "# #         print('max_opinion')\n",
    "# #         print(v1, max_opinion)\n",
    "#             print('_________________________Payoff row start')\n",
    "#             print('two opinion changed -min +  max')\n",
    "#             print(op2)\n",
    "        \n",
    "    # when v1 == v2, the polarization should be negative for max, infinet for min. \n",
    "    # Replace the the column_index of agent v2 with 0 for max\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "    \n",
    "    return payoff_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.445     0.1483    0.5933    0.     -100.     -100.    ]\n"
     ]
    }
   ],
   "source": [
    "# #(1,0) (2,0.3928571428571428)\n",
    "# op1=copy.copy(s)\n",
    "# print(op1)\n",
    "\n",
    "op1 = copy.copy(s)\n",
    "# print(op1)\n",
    "op1[2,0] = 1  #op1 is the opinion array that updated by minimizer\n",
    "# print(op1)\n",
    "payoff_row_1 = make_payoff_row(op1,2)\n",
    "print(payoff_row_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEEDDDDDDD UPDAE\n",
    "\n",
    "# Calculate polarization of minimizer's Mixed Strategy\n",
    "def mixed_min_polarization(s,v2,weight_op,fla_max_fre):\n",
    "\n",
    "    op1 =  copy.copy(s) # make a copy of the innate opinion array \n",
    "    op1[v2,0] = weight_op # then only updated by minimizer's current change\n",
    "#     print('Min update')\n",
    "#     print(v2, weight_op)\n",
    "    # calculate the polarization with both min(did here) and max's action(in make_payoff_row)\n",
    "    payoff_row = make_payoff_row(op1,v2)  # the vector list out 2*n payoffs after min's action combine with 2*n possible max's actions\n",
    "    #print(payoff_row)\n",
    "\n",
    "    # Replace the the column_index of agent v2 with 100 for min\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = 100\n",
    "    payoff_row[j_2] = 100\n",
    "    \n",
    "#     print('Min Payoff Row')\n",
    "#     print(payoff_row)\n",
    "    #calculate fictitious payoff - equi_min  \n",
    "    payoff_cal = payoff_row * fla_max_fre # fla_max_fre recorded the frequency of each maximizer's action, frequency sum = 1\n",
    "                                             # payoff (2*n array) * maximizer_action_frequency (2*n array)\n",
    "# can DELETE - use to check if function works as expected\n",
    "#     if v2 ==6 and v1==16:\n",
    "#         print('Payoff row')\n",
    "#         column = column_index(16,1)\n",
    "#         print(payoff_row[column],column)\n",
    "#         print('fla_max_fre')\n",
    "#         print(np.nonzero(fla_max_fre))\n",
    "#         print(fla_max_fre [np.nonzero(fla_max_fre)])\n",
    "#         print('compare to: '+str(fla_max_fre[column]))\n",
    "    \n",
    "    mixed_pol = np.sum(payoff_cal) # add up all, calculate average/expected payoff\n",
    "\n",
    "\n",
    "#     print('min_mixed_polarization')\n",
    "#     print(mixed_pol)\n",
    "        # Replace the the column_index of agent v2 with 100 for min\n",
    "\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "\n",
    "    return (mixed_pol,payoff_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # op2=op\n",
    "# # op2[0,0]=1\n",
    "# # min_opinion1 = derivate_s(op2,n,1)\n",
    "# # # print(min_opinion1)\n",
    "# # min_opinion2 = derivate_s1(op2,n,1)\n",
    "# # print(min_opinion2)\n",
    "# v2 = 254\n",
    "# min_opinion = 0\n",
    "# (mixed_pol, payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "# print(np.nonzero(fla_max_fre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_s(op,n,v2):\n",
    "               #op - opinion array that updated by maximizer\n",
    "    c = [1/n] * n\n",
    "#     print(c)\n",
    "    sum_term = 0\n",
    "    j = 0\n",
    "\n",
    "    sum_term = np.dot(np.dot((A-c),(A[v2]-c)),op)  # sum up all terms\n",
    "    \n",
    "    term_out = op[v2]*np.dot((A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "    sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "    s_star = -sum_s/np.dot((A[v2]-c),(A[v2]-c))\n",
    "    s_star = s_star[0] #take value out of array\n",
    "    min_opinion =min(max(0,s_star),1)\n",
    "    \n",
    "#     print('Min opinion-should be unique')\n",
    "#     print(min_opinion)\n",
    "    return min_opinion\n",
    "\n",
    "# def derivate_s1(op,n,v2):\n",
    "#                #op - opinion array that updated by maximizer\n",
    "#     c = [1/n] * n\n",
    "# #     print(c)\n",
    "#     sum_term = 0\n",
    "#     j = 0\n",
    "#     for j in range(0,n):\n",
    "#         term = op[j]*np.dot(np.transpose(A[j]-c),(A[v2]-c))\n",
    "# #             print(A[j])\n",
    "# #             print(A[v])\n",
    "#         sum_term = sum_term + term  # sum up all terms\n",
    "    \n",
    "#     term_out = op[v2]*np.dot(np.transpose(A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "#     sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "#     s_star = -sum_s/np.dot(np.transpose(A[v2]-c),(A[v2]-c))\n",
    "#     s_star = s_star[0] #take value out of array\n",
    "#     min_opinion =min(max(0,s_star),1)\n",
    "            \n",
    "#     return min_opinion\n",
    "\n",
    "\n",
    "\n",
    "def min_mixed_opinion(op, n, v2, fla_max_fre):\n",
    "    \n",
    "    weight_op = 0\n",
    "    \n",
    "    # loop for each max_action(in total 2*n) \n",
    "    for column in range(2*n):\n",
    "\n",
    "        if fla_max_fre[column] !=0:\n",
    "            v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "            max_opinion = column%2\n",
    "            \n",
    "##             temp = op[v1,0] \n",
    "          \n",
    "##             op[v1,0]= max_opinion #update innate opinion array with max_action  \n",
    "\n",
    "            min_opinion = derivate_s(op, n, v2)# find min_s_star for each max_action\n",
    "#             print(fla_max_fre[column],min_opinion)\n",
    "            print(min_opinion)\n",
    "            op1 = copy.copy(op)\n",
    "            op1[v2] = min_opinion\n",
    "            min_por = obj_polarization(A, L, op1, n)\n",
    "            #(min_por, row) = mixed_min_polarization(s, v2, min_opinion,fla_max_fre)\n",
    "\n",
    "    \n",
    "            weight_op = weight_op + fla_max_fre[column]*min_opinion # sum up p_i*s_i\n",
    "\n",
    "    print('Weighted opinion')\n",
    "    print(weight_op)\n",
    "    \n",
    "    (mixed_por, payoff_row) = mixed_min_polarization(s, v2, weight_op,fla_max_fre)\n",
    "    \n",
    "    print('Weighted polarization')\n",
    "    print(mixed_por)\n",
    "    \n",
    "    return(weight_op,payoff_row,mixed_por)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out weighted opinion proved that we need to do this step insetead of min_mixed_opinion - we are weighting\n",
    "# different min_opinion here\n",
    "def min_mixed_opinion_1(s, n, v2, fla_max_fre):\n",
    "    \n",
    "    weight_op = 0\n",
    "    \n",
    "    # loop for each max_action(in total 2*n) \n",
    "    for column in range(2*n):\n",
    "\n",
    "        if fla_max_fre[column] !=0:\n",
    "            v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "            max_opinion = column%2\n",
    "            op = copy.copy(s)\n",
    "            op[v1] = max_opinion\n",
    "#             print(op)\n",
    "\n",
    "#             print('Weight')\n",
    "#             print(fla_max_fre[column])\n",
    "            min_opinion = derivate_s(op, n, v2)# find min_s_star for each max_action\n",
    "\n",
    "\n",
    "            \n",
    "            op1 = copy.copy(op)\n",
    "            op1[v2] = min_opinion   #after max action, update min action on opinion array\n",
    "#             print(min_opinion)\n",
    "            min_por = obj_polarization(A, L, op1, n)\n",
    "            t = 0  \n",
    "            weight_op = weight_op + fla_max_fre[column]*min_opinion # sum up p_i*s_i\n",
    "\n",
    "#     print('Weighted opinion')\n",
    "#     print(weight_op)\n",
    "    \n",
    "  \n",
    "    (mixed_por, payoff_row) = mixed_min_polarization(s, v2, weight_op,fla_max_fre)\n",
    "\n",
    "    return(weight_op,payoff_row,mixed_por)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fla_max_fre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# op=copy.copy(s)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# op[21] = 1\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print(op)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # print(21,fla_max_fre)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m v2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m----> 6\u001b[0m (weight_op_1,payoff_row,min_por) \u001b[38;5;241m=\u001b[39m min_mixed_opinion_1(s, n, v2, \u001b[43mfla_max_fre\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fla_max_fre' is not defined"
     ]
    }
   ],
   "source": [
    "# op=copy.copy(s)\n",
    "# op[21] = 1\n",
    "# print(op)\n",
    "# # print(21,fla_max_fre)\n",
    "v2 = 6\n",
    "(weight_op_1,payoff_row,min_por) = min_mixed_opinion_1(s, n, v2, fla_max_fre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = obj_polarization(A, L, s, n) #min_por- set a standard to compare with pol after min's action\n",
    "# # maxup_por = min_por # store innate max updated polarization\n",
    "# print(a)\n",
    "# print(s[253])\n",
    "# op = copy.copy(s)\n",
    "# op[253] = 0\n",
    "# b = obj_polarization(A, L, op, n)\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizer search: Go through each agent \n",
    "\n",
    "def mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre, min_touched_all):\n",
    "    # current polarization that changed by maximizer, \"innate\" objective that min start with\n",
    "    op = copy.copy(s)\n",
    "    op[v1,0] = max_opinion\n",
    "#     print('Check if op has been updated by Maximizer')\n",
    "#     print(op)\n",
    "    min_por = obj_polarization(A, L, op, n) #min_por- set a standard to compare with pol after min's action\n",
    "    maxup_por = min_por # store innate max updated polarization\n",
    "#     print('check maxup por')\n",
    "#     print(maxup_por)\n",
    "#     payoffs = []    # create an empty list to store all polarizations   \n",
    "    champion = (None, None, 0, None)  # assume the best action is champion\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in max_touched]  # for the vertice that Maximizer has not touched\n",
    "    \n",
    "    for v2 in C1:  \n",
    "\n",
    "#         print('_________________________________')\n",
    "#         print('Min start with agent '+ str(v2) )\n",
    "        (changed_opinion, payoff_row, por) =  min_mixed_opinion_1(s, n, v2, fla_max_fre) # find the best new_op option \n",
    "        if v2 in min_touched_all:\n",
    "            print('____')\n",
    "            print('Min start with agent '+ str(v2) )\n",
    "            print('Weighted polarization')\n",
    "            print(str(por)+'...')\n",
    "            \n",
    "#         print('changed opinion, por, Maxup_por')\n",
    "#         print(changed_opinion, por, maxup_por)\n",
    "\n",
    "        if por < min_por:  # if the recent polarization is smaller than the minimum polarization in the history\n",
    "            min_por = por\n",
    "                                 # update the recent option as champion\n",
    "            champion = (v2, changed_opinion, payoff_row, min_por)  \n",
    "#         else:\n",
    "#             print('Innate polarization is smaller than Min action')\n",
    "\n",
    "    return (champion)  # find the best minimizer's action after going through every new_op option of every agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_touched' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m      5\u001b[0m max_opinion \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 6\u001b[0m champion \u001b[38;5;241m=\u001b[39m mixed_choose_min_vertex(s, n, v1, max_opinion, \u001b[43mmax_touched\u001b[49m, fla_max_fre, min_touched_all)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_touched' is not defined"
     ]
    }
   ],
   "source": [
    "# print('v1,max_opinion')\n",
    "# print(v1,max_opinion)\n",
    "min_touched_all = [6, 29]\n",
    "v1 = 16\n",
    "max_opinion = 1\n",
    "champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre, min_touched_all)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by maximizer, fla_max_fre includes max's hisotry, so minimizer react to the innate op after that\n",
    "def mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre, min_touched_all): \n",
    "\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre, min_touched_all)\n",
    "    (v2, min_opinion, payoff_row, min_pol) = min_champion\n",
    "    \n",
    "    if v2 == None:    # if minimizer cannot find a action to minimize polarization after maximizer's action\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "#         print('v2', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(v2, min_opinion, innat_equi_por, min_pol)\n",
    "\n",
    "        # Store innate_op of the min_selected vertex\n",
    "        old_opinion_min = op[v2,0]\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "        print('fla_max_fre')\n",
    "        print(np.nonzero(fla_max_fre))\n",
    "        print(fla_max_fre [np.nonzero(fla_max_fre)])\n",
    "\n",
    "\n",
    "#         print(\"Payoff row\")\n",
    "#         print(payoff_row)\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "    return (v2, payoff_row, min_opinion, min_pol)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_touched' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmax_touched\u001b[49m)\n\u001b[0;32m      2\u001b[0m (v2, payoff_row, min_opinion, polarization) \u001b[38;5;241m=\u001b[39m mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre,  min_touched_all)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_touched' is not defined"
     ]
    }
   ],
   "source": [
    "print(max_touched)\n",
    "(v2, payoff_row, min_opinion, polarization) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre,  min_touched_all)\n",
    "# print('v2, payoff_row, min_opinion, polarization')\n",
    "# print(v2, payoff_row, min_opinion, polarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by minimizer, fla_min_fre includes min's hisotry, so maxmizer react to the innate op after that\n",
    "def mixed_max_polarization(payoff_matrix,v1,max_opinion,fla_min_fre):\n",
    "\n",
    "    # create payoff matrix for maxmizer\n",
    "    column = int(column_index(v1,max_opinion))\n",
    "#     print(payoff_matrix)\n",
    "#     print(\"column\"+str(column))\n",
    "    payoff_vector = payoff_matrix[:,column]\n",
    "    \n",
    "#     print('payoff vector')\n",
    "#     print(payoff_vector)\n",
    "\n",
    "    #calculate fictitious payoff - equi_max   \n",
    "    payoff_cal = payoff_vector * fla_min_fre #payoff * frequency\n",
    "    \n",
    "#     print('max_payoff_calculation')\n",
    "#     print(payoff_cal)\n",
    "    mixed_pol = np.sum(payoff_cal) # add up\n",
    "#     print(\"Max_mixed_polarization\")\n",
    "#     print(mixed_pol)\n",
    "\n",
    "    return mixed_pol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_pol = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines if value of opinion at v should be set to 0 or 1 to maximize equilibrium polarization \n",
    "def max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre):\n",
    "    \n",
    "    por_arr = np.zeros(2)  # create a two_element array to store polarization value of each option\n",
    "\n",
    "\n",
    "    max_opi_option = [0, 1.0]   # Maximizer has two options to change agent v1's opinion\n",
    "    \n",
    "    # objective if set opinion to 0, 1.0\n",
    "    j = 0\n",
    "    for new_op in max_opi_option:\n",
    "#         print('change op to '+ str(i/10))\n",
    "        max_opinion = new_op\n",
    "\n",
    "        por_arr[j] = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)\n",
    "    \n",
    "        j = j + 1   # index increase 1, put the polarization in array\n",
    "\n",
    "#     print('Polarization Options')\n",
    "#     print(por_arr)\n",
    "    \n",
    "    maxmize_op = np.argmax(por_arr)  # the index of maximum polarization = max_opinion --[0,1]\n",
    "    max_por = np.max(por_arr)        # find the maximum polarization in the record\n",
    " \n",
    "#     print('new_op', 'innat_equi_por', 'max_por')\n",
    "#     print(maxmize_op, innat_equi_por, max_por)\n",
    "\n",
    "    return (maxmize_op, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fla_min_fre = [0, 0, 0, 0, 0.65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.35, 0, 0, 0]\n",
    "# v1 = 2\n",
    "# champion = max_mixed_opinion(payoff_matrix, n, v1, v2, fla_min_fre)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which agent maximizer should select to maximizer the equilibrium polarization\n",
    "def mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre):\n",
    "#     print('Check if op has been updated by minimizer')\n",
    "#     print(op)\n",
    "    max_por = obj_polarization(A, L, op, n)  # use \"innate\"(after min action) polarization as a comparable standard to find max_por\n",
    "    minup_por = max_por # store innate min_update polarization\n",
    "#     print('check minup por')\n",
    "#     print(minup_por)\n",
    "    champion = (None, None, max_por)  # assume champion is the best action\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in min_touched]  # for the vertice that Minimizer has not touched\n",
    "    for v1 in C1:  \n",
    "#             print('Maximizer start from agent'+str(v1))\n",
    "            (changed_opinion, por) = max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre)\n",
    "#             print('changed_opinion, por, minup_por')\n",
    "#             print(changed_opinion, por,minup_por)\n",
    "            \n",
    "            if por > max_por: # if the polarization of most recent action > maximum polarization of previous actions\n",
    "                max_por = por\n",
    "                champion = (v1, changed_opinion,max_por)   # save the this action as champion    \n",
    "#             else:\n",
    "#                 print('Innate polarization is bigger than max action')\n",
    " \n",
    "    return (champion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'payoff_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpayoff_matrix\u001b[49m)\n\u001b[0;32m      2\u001b[0m champion \u001b[38;5;241m=\u001b[39m mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'payoff_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)\n",
    "champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # min_touched = []\n",
    "# # payoff_matrix = np.empty((0, 2*n), float)\n",
    "# # fla_min_fre = np.empty((0,n))\n",
    "# # champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)\n",
    "# # print(champion)\n",
    "# print(c1)\n",
    "# vertices = np.where(c1)\n",
    "# print(vertices)\n",
    "# por=0\n",
    "# for i in c1:\n",
    "#     print(i)\n",
    "#     max_por = 0.75\n",
    "#     if por > max_por:\n",
    "#         max_por = por\n",
    "#         print('yes')\n",
    "#     else:\n",
    "#         print('por<max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre): \n",
    "    op = copy.copy(s)   # op is a copy of innate opinion\n",
    "    \n",
    "    #update innat opinion \n",
    "    op[v2,0] = min_opinion  # Op has been updated by minimizer, so maximizer react to the innate op after that\n",
    "    \n",
    "\n",
    "    max_champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre) # The best choice among all opinions and vertexs\n",
    "    (v1, max_opinion, max_pol) = max_champion\n",
    "\n",
    "    if v1 == None:\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        \n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Testing function -- NO NEED TO RUN\n",
    "# min_touched = []\n",
    "# v2 = 0\n",
    "# min_opinion = 0\n",
    "# b = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "# print('v1,max_opinion,max_pol')\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Player's Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\n",
      "0.5\n",
      "Equi_polarization:\n",
      "0.4449982200071198\n",
      "Difference:\n",
      "-0.055001779992880195\n"
     ]
    }
   ],
   "source": [
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, op, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = 50\n",
    "\n",
    "\n",
    "# with open('Network_'+str(Network)+'.txt', \"a\") as fi:\n",
    "#     print('Innate Opinion', file=fi)\n",
    "#     print(s, file=fi)\n",
    "#     print('Adjacency Matrix', file=fi)\n",
    "#     print(G,file=fi)\n",
    "\n",
    "# Game Preparation\n",
    "def push(obj, element):\n",
    "    if len(obj) >= memory:\n",
    "        obj.pop(0)\n",
    "        print('pop')\n",
    "    obj.append(element)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. ]\n",
      " [0.5]\n",
      " [0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Parameters\n",
    "Game_rounds =201 # Rounds + 1- use for printing data\n",
    "memory = 50\n",
    "def all_fre_limited_touch(s, n):\n",
    "    # Preparation for the game\n",
    "    op = copy.copy(s)\n",
    "    payoff_matrix = np.empty((0, 2*n), float)\n",
    "    max_history = np.zeros([n, 2])  # n*2 matrix, agent i & opinion options\n",
    "    min_history = []  # append a list of (agent i, min_opinion), min_opinion can be any value\n",
    "#     print(type(min_history))\n",
    "\n",
    "    max_history_last_100 = np.zeros([n, 2]) \n",
    "    min_history_last_100= []\n",
    "\n",
    "    max_touched = []\n",
    "    min_touched = []\n",
    "    min_touched_all = []\n",
    "    min_touched_last_100 =[]\n",
    "    print('min_touched')\n",
    "    print(min_touched)\n",
    "    \n",
    "    \n",
    "    # Game start from maximizer random play\n",
    "    print('Maximizer first selection')\n",
    "    (v1, max_opinion, max_pol) = random_play(op,n)   # Maximizer does random action \n",
    "    #(v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "    #(v1, max_opinion, max_pol) = (11, 1, 0.14833274000237331)\n",
    "    First_max = (v1, max_opinion, max_pol) \n",
    "\n",
    "\n",
    "#     (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    # Maximizer start with greedy play\n",
    "    # (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)   # Maximizer choose action greedily\n",
    "    max_touched.append(v1)    # save Maximizer's action history\n",
    "\n",
    "    # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "    max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "    # print('max_history')\n",
    "    # print(max_history)\n",
    "    print('history at spot')\n",
    "    print(max_history[v1,int(max_opinion)])\n",
    "\n",
    "    max_frequency = max_history/1  # its frequency, only played  1 time so far, divided by 1 \n",
    "    # print('fre_max at spot')\n",
    "    # print(max_frequency[v1,int(max_opinion)])\n",
    "\n",
    "    fla_max_fre = max_frequency.flatten()   # flatten the n*2 matrix to a 2n*1 matrix\n",
    "                                            # so we can multiply the freuency (2n*1)with payoff array (1*2n) \n",
    "                                            # to get average payoff of fictitious play\n",
    "    print('fre_max at spot')\n",
    "    print(fla_max_fre)\n",
    "\n",
    "    column = int(column_index(v1,max_opinion))    # the frequency of maximizer's most recent action (v1,max_opinion)\n",
    "\n",
    "    print(fla_max_fre[column])\n",
    "\n",
    "    # print(np.shape(fla_max_fre.shape))\n",
    "\n",
    "\n",
    "    # if game start from minimizer random play - make sure two random play are not same agent!!!\n",
    "    print('Minimizer first selection')\n",
    "    (v2, min_opinion, min_pol) = random_play(op,n) \n",
    "    #(v2, min_opinion, min_pol) = minimizer_fir_play(s,n,min_touched)\n",
    "    \n",
    "    #(v2, min_opinion, min_pol) = (29, 1, 0.5933309600094931)\n",
    "    First_min = (v2, min_opinion, min_pol)\n",
    "\n",
    "    if v1==v2:   # if Max and Min randomly selected the same agent, then we need to restart - cannot choose same agent\n",
    "        sys.exit()\n",
    "\n",
    "    # Minimizer start with greedy play\n",
    "    # (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    min_touched.append(v2)\n",
    "   \n",
    "\n",
    "    # store minimizer play history\n",
    "    min_history.append((v2,min_opinion))\n",
    "    print('min_history')\n",
    "    print(min_history)\n",
    "\n",
    "\n",
    "    counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/1 #return only frequency of all min options in order\n",
    "#     print('fla_min_fre')\n",
    "#     print(fla_min_fre)\n",
    "\n",
    "\n",
    "    (a,payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "    payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "#     print('Payoff Matrix')\n",
    "#     print(payoff_matrix)\n",
    "    print('fla_min_fre at the spot')\n",
    "    min_counter = dict(counter)\n",
    "    print(min_counter) \n",
    "    print(min_counter[(v2,min_opinion)]) \n",
    "#     print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "\n",
    "    equi_min = min_pol\n",
    "    equi_max = max_pol\n",
    "    # print(equi_min)\n",
    "    # print(equi_max)\n",
    "\n",
    "\n",
    "\n",
    "    Flag = 0\n",
    "\n",
    "    i = 0\n",
    "    while Flag == 0: \n",
    "        i = i + 1\n",
    "        print(\"Game \" + str(i))\n",
    "        print(\"_____________________\")\n",
    "\n",
    "    #     if max_pol == min_pol:\n",
    "        if i == Game_rounds:            # i == # of iterations you want to run + 2\n",
    "                                # because Game 101 is skipped for collecting data, to get 200 game result, we need to run 201 iteration\n",
    "            print('min_recent_'+str(memory)+'_touched')# then stop at Game 202\n",
    "            print(min_touched)\n",
    "            print('max_recent_'+str(memory)+'_touched')\n",
    "            print(max_touched)\n",
    "            print('Min last 100 action')\n",
    "            print(min_touched_last_100)\n",
    "\n",
    "            break\n",
    "\n",
    "        elif equi_min == equi_max:\n",
    "            print(\"Reached Nash Equilibrium at game\"+ str(i) + \"and Equi_Por = \" + str(equi_min))\n",
    "            print('max_distribution')\n",
    "            print(max_frequency)\n",
    "            print('min_distribution')\n",
    "            print(fla_min_fre)\n",
    "            Flag = 1\n",
    "            break\n",
    "        ############################## maximizer play  \n",
    "        else:\n",
    "            if i == Game_rounds-100:    #if Game_round = 200, after 100 iteration, Game 101 print previous historical result\n",
    "    #             max_touched_100 = max_touched \n",
    "    #             min_touched_100 = min_touched\n",
    "    #             max_fre_100 = max_frequency  # store the max_frequency of first 100 iterataions\n",
    "    #             print('max_history')\n",
    "    #             print(max_history)\n",
    "    #             min_fre_100 = fla_min_fre  # max_frequency of first 100 iterations\n",
    "    #             print('min_history')\n",
    "    #             print(min_history)\n",
    "    # Remove max frequncy less than 0.1--\n",
    "                max_history_last_100 = np.zeros([n, 2]) \n",
    "                min_history_last_100 = [] \n",
    "                min_touched_last_100 =[]\n",
    "\n",
    "            (v1, max_opinion, equi_max) = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "            max_touched = push(max_touched, v1)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "    #             print('equi_max')\n",
    "    #             print(equi_max)\n",
    "    #         print(v1, max_opinion, max_pol)\n",
    "            # cumulate strategy \n",
    "            max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "\n",
    "            max_history_last_100[v1,int(max_opinion)] = max_history_last_100[v1,int(max_opinion)] +1\n",
    "    #         print('max_history')\n",
    "    #         print(max_history)\n",
    "    #________________________________________________________________\n",
    "            max_frequency = max_history/(i+1)  # its frequency \n",
    "    #         print('max_distribution')\n",
    "    #         print(max_frequency)\n",
    "        #     print(i+1) \n",
    "            fla_max_fre = max_frequency.flatten() #flaten max_frequency to calculate average payoff\n",
    "#             print('fla_max_fre')\n",
    "#             print(fla_max_fre)\n",
    "            print('fre_max at spot')\n",
    "            print(fla_max_fre[column])\n",
    "            # create payoff matrix for maxmizer\n",
    "            row = int(row_index(v2, min_opinion))\n",
    "            column = int(column_index(v1,max_opinion))\n",
    "\n",
    "    # _________________________________________________________________\n",
    "    #         ######################Visualize Maximizer's selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v1:\n",
    "    #                 color_map.append('Red')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True,node_size = 50)\n",
    "    #         plt.figure(figsize=(200, 200))\n",
    "    #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    ############################### minimizer play\n",
    "            (v2, payoff_row, min_opinion, equi_min) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre, min_touched_all)\n",
    "            min_touched = push(min_touched, v2)\n",
    "            min_touched_all.append(v2) \n",
    "            min_touched_last_100.append(v2)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('equi_min')\n",
    "    #         print(equi_min)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "            #         print(v2, min_opinion, min_pol)\n",
    "            if (v2,min_opinion) in counter.keys():\n",
    "                payoff_matrix = payoff_matrix # if this min_option is in min_history, no need to update paryoff matrix, only update frequency\n",
    "                print(\"Same history\")\n",
    "                print((str(v2),str(min_opinion)))\n",
    "            else:\n",
    "                payoff_matrix = np.vstack([payoff_matrix, payoff_row]) # if this is a new option, append to previous matrix\n",
    "    #                 print('payoff_row')\n",
    "    #                 print(payoff_row)\n",
    "            min_history.append((v2,min_opinion))\n",
    "            min_history_last_100.append((v2,min_opinion))\n",
    "            #         print('min_history')\n",
    "            #         print(min_history)\n",
    "            counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "            #print(counter)\n",
    "    #         print('counter.keys')\n",
    "    #         print(counter.keys())\n",
    "            fla_min_fre = np.array(list(counter.values()))/(i+1) #return only frequency of all min options in order\n",
    "    #         print('fla_min_fre')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "    #         print('fla_min_fre at the spot')\n",
    "    #         min_counter = dict(counter)\n",
    "    #         print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "            # create payoff matrix for minimizer\n",
    "            row = row_index(v2, min_opinion)\n",
    "            column = column_index(v1,max_opinion)\n",
    "            #     print('row, column')\n",
    "            #     print(row, column)\n",
    "\n",
    "            print(\"Not Reached Nash Equilibrium at Equi_Min = \" + str(equi_min) + \" and Equi_Max = \"+ str(equi_max)) \n",
    "    #         print('min_distribution')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "            ######################Visualize Minimizer selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G1, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G1)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v2:\n",
    "    #                 color_map.append('Blue')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True)\n",
    "    #         plt.figure(figsize=(25, 25))\n",
    "    #         plt.show()\n",
    "    return (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, fla_max_fre, max_history_last_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_touched\n",
      "[]\n",
      "Maximizer first selection\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "Network reaches equilibrium Polarization: 0.5933309600094931\n",
      "history at spot\n",
      "1.0\n",
      "fre_max at spot\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "1.0\n",
      "Minimizer first selection\n",
      "    Agent0 's opinion 1.0 changed to 1\n",
      "Network reaches equilibrium Polarization: 0.4449982200071198\n",
      "min_history\n",
      "[(0, 1)]\n",
      "Counter({(0, 1): 1})\n",
      "fla_min_fre at the spot\n",
      "{(0, 1): 1}\n",
      "1\n",
      "Game 1\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.5933309600094931\n",
      "Game 2\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.6666666666666666\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.329628311116385...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.16666666666666669\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.667 0.333]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.18129557111401184 and Equi_Max = 0.5377061825086031\n",
      "Game 3\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.38044600908016113\n",
      "Game 4\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.21359914560341756...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.30259878960484154...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.20000000000000007\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.6 0.4]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.21359914560341756 and Equi_Max = 0.39349379639518467\n",
      "Game 5\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.33536384372981026\n",
      "Game 6\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.42857142857142855\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.22703990816689787...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.29061108245362927...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.21428571428571433\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.571 0.429]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.22703990816689787 and Equi_Max = 0.34663163816480524\n",
      "Game 7\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.31533368629510033\n",
      "Game 8\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4444444444444444\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2344023545716516...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.28384660123910943...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.22222222222222227\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.556 0.444]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2344023545716516 and Equi_Max = 0.32363950287207155\n",
      "Game 9\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.3039109909945213\n",
      "Game 10\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.45454545454545453\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2390486305823371...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.27950301421934803...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.22727272727272732\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.545 0.455]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2390486305823371 and Equi_Max = 0.31004977223057417\n",
      "Game 11\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2964920344395585\n",
      "Game 12\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.46153846153846156\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.24224755171985227...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.27647818402809227...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.23076923076923084\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.538 0.462]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.24224755171985227 and Equi_Max = 0.301102198970377\n",
      "Game 13\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2912693462583337\n",
      "Game 14\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4666666666666667\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.24458420684835777...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.27425075484883243...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2333333333333334\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.533 0.467]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.24458420684835777 and Equi_Max = 0.2947789499570218\n",
      "Game 15\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2873850220125966\n",
      "Game 16\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.47058823529411764\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.24636579654373417...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.27254216242650603...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.23529411764705888\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.529 0.471]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.24636579654373417 and Equi_Max = 0.290080642430964\n",
      "Game 17\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.28437829196255393\n",
      "Game 18\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.47368421052631576\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.247769092026125...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.271190050973868...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.23684210526315794\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.526 0.474]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.247769092026125 and Equi_Max = 0.2864566920430908\n",
      "Game 19\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2819790405280967\n",
      "Game 20\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.47619047619047616\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2489030104348214...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2700934018637318...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.23809523809523814\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.524 0.476]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2489030104348214 and Equi_Max = 0.28357921385650703\n",
      "Game 21\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.28001819908580183\n",
      "Game 22\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4782608695652174\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.24983832011741888...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26918606881338064...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.23913043478260876\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.522 0.478]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.24983832011741888 and Equi_Max = 0.28124096473129573\n",
      "Game 23\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2783844128752852\n",
      "Game 24\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.48\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2506229975080099...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26842292630829473...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24000000000000005\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.52 0.48]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2506229975080099 and Equi_Max = 0.2793046370772043\n",
      "Game 25\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2770012829614366\n",
      "Game 26\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.48148148148148145\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2512907186597133...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2677721342155326...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24074074074074078\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.519 0.481]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2512907186597133 and Equi_Max = 0.2776757137752756\n",
      "Game 27\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.27581460871123653\n",
      "Game 28\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4827586206896552\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25186581774481465...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26721058395195674...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24137931034482765\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.517 0.483]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25186581774481465 and Equi_Max = 0.27628703979069164\n",
      "Game 29\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2747848476000685\n",
      "Game 30\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4838709677419355\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25236631623712835...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2667210975276806...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2419354838709678\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.516 0.484]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25236631623712835 and Equi_Max = 0.27508961124229714\n",
      "Game 31\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.27388245877678924\n",
      "Game 32\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.48484848484848486\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25280584521983923...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2662906397655094...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2424242424242425\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.515 0.485]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25280584521983923 and Equi_Max = 0.27404684537032953\n",
      "Game 33\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2730849191017535\n",
      "Game 34\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4857142857142857\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2531949055877245...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2659091404450708...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2428571428571429\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.514 0.486]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2531949055877245 and Equi_Max = 0.2731308890768447\n",
      "Game 35\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.27237474947133766\n",
      "Game 36\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4864864864864865\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2535417177542393...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26556869667335065...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2432432432432433\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.514 0.486]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2535417177542393 and Equi_Max = 0.27232016809627474\n",
      "Game 37\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.27173817312924325\n",
      "Game 38\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.48717948717948717\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25385280882720423...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26526301959661763...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24358974358974364\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.513 0.487]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25385280882720423 and Equi_Max = 0.2715977166520536\n",
      "Game 39\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2711641814968577\n",
      "Game 40\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4878048780487805\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2541334272497532...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2649870423718781...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2439024390243903\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.512 0.488]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2541334272497532 and Equi_Max = 0.27095001254459217\n",
      "Game 41\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.27064386986421424\n",
      "Game 42\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4883720930232558\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25438784129125247...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26473663710537154...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24418604651162795\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.512 0.488]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25438784129125247 and Equi_Max = 0.2703661480031343\n",
      "Game 43\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2701699560360299\n",
      "Game 44\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4888888888888889\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25461955765345656...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26450840698694816...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2444444444444445\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.511 0.489]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25461955765345656 and Equi_Max = 0.2698372286964417\n",
      "Game 45\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26973642563993605\n",
      "Game 46\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.48936170212765956\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25483148406926504...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2642995313034592...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24468085106382984\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.511 0.489]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25483148406926504 and Equi_Max = 0.26935593095174154\n",
      "Game 47\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2693382667871401\n",
      "Game 48\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "fre_max at spot\n",
      "0.4897959183673469\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2550260519491033...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26410764827577915...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24489795918367352\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.51 0.49]\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2550260519491033 and Equi_Max = 0.2689161706923717\n",
      "Game 49\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26897126884251804\n",
      "Game 50\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49019607843137253\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25520531007713204...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26393076537138926...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24509803921568632\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.51 0.49]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25520531007713204 and Equi_Max = 0.26851285257492163\n",
      "Game 51\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26863186790432597\n",
      "Game 52\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49056603773584906\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2553709970279378...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2637671898582608...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24528301886792458\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.509 0.491]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2553709970279378 and Equi_Max = 0.26814167756702384\n",
      "Game 53\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26831702679550384\n",
      "Game 54\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4909090909090909\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2555245977363197...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2636154744637219...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2454545454545455\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.509 0.491]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2555245977363197 and Equi_Max = 0.26779899369597915\n",
      "Game 55\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2680241408823403\n",
      "Game 56\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49122807017543857\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25566738812351203...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26347437443942634...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24561403508771934\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.509 0.491]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25566738812351203 and Equi_Max = 0.26748167909001336\n",
      "Game 57\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2677509634499244\n",
      "Game 58\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4915254237288136\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2558004706217314...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26334281333371645...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24576271186440685\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.508 0.492]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2558004706217314 and Equi_Max = 0.26718704945492683\n",
      "Game 59\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26749554604744225\n",
      "Game 60\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4918032786885246\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2559248026915444...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2632198554785463...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24590163934426235\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.508 0.492]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2559248026915444 and Equi_Max = 0.2669127842383637\n",
      "Game 61\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2672561904075028\n",
      "Game 62\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49206349206349204\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25604121989323747...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26310468370287426...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24603174603174607\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.508 0.492]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25604121989323747 and Equi_Max = 0.2666568672273883\n",
      "Game 63\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26703140939746584\n",
      "Game 64\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49230769230769234\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.256150454688122...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26299658114977004...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24615384615384622\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.508 0.492]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.256150454688122 and Equi_Max = 0.26641753839600313\n",
      "Game 65\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2668198950801662\n",
      "Game 66\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4925373134328358\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25625315186420244...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26289491634192075...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24626865671641796\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.507 0.493]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25625315186420244 and Equi_Max = 0.26619325459637544\n",
      "Game 67\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2666204924159178\n",
      "Game 68\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4927536231884058\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25634988127274266...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26279913083806317...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24637681159420297\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.507 0.493]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25634988127274266 and Equi_Max = 0.26598265725770426\n",
      "Game 69\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26643217747465364\n",
      "Game 70\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49295774647887325\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25644114840719767...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26270872897067826...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24647887323943668\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.507 0.493]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25644114840719767 and Equi_Max = 0.2657845456793342\n",
      "Game 71\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.266254039279369\n",
      "Game 72\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4931506849315068\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2565274032392329...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26262326926672774...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24657534246575347\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.507 0.493]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2565274032392329 and Equi_Max = 0.26559785482106024\n",
      "Game 73\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.266085264592684\n",
      "Game 74\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49333333333333335\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2566090476378834...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2625423572379785...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24666666666666673\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.507 0.493]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2566090476378834 and Equi_Max = 0.2654216367324941\n",
      "Game 75\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26592512510365074\n",
      "Game 76\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4935064935064935\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25668644163001353...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26246563929244354...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2467532467532468\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.506 0.494]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25668644163001353 and Equi_Max = 0.2652550449453387\n",
      "Game 77\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26577296658357724\n",
      "Game 78\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4936708860759494\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25675990870784154...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26239279756869116...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24683544303797475\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.506 0.494]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25675990870784154 and Equi_Max = 0.26509732129213553\n",
      "Game 79\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2656281996660731\n",
      "Game 80\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49382716049382713\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2568297403485689...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2623235455338421...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24691358024691362\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.506 0.494]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2568297403485689 and Equi_Max = 0.2649477847231328\n",
      "Game 81\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2654902919739331\n",
      "Game 82\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4939759036144578\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25689619987927365...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2622576242167088...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24698795180722896\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.506 0.494]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25689619987927365 and Equi_Max = 0.264805821777126\n",
      "Game 83\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26535876136838504\n",
      "Game 84\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49411764705882355\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2569595257951147...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26219479897166914...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24705882352941183\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.506 0.494]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2569595257951147 and Equi_Max = 0.2646708784281654\n",
      "Game 85\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26523317013805225\n",
      "Game 86\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4942528735632184\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25701993461898875...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2621348566880362...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24712643678160925\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.506 0.494]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25701993461898875 and Equi_Max = 0.2645424530821574\n",
      "Game 87\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26511311997821724\n",
      "Game 88\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4943820224719101\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25707762337489976...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2620776033749797...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2471910112359551\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.506 0.494]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25707762337489976 and Equi_Max = 0.2644200905387838\n",
      "Game 89\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2649982476375555\n",
      "Game 90\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4945054945054945\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2571327717345814...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26202286206433...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24725274725274732\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2571327717345814 and Equi_Max = 0.2643033767672276\n",
      "Game 91\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26488822113088356\n",
      "Game 92\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4946236559139785\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2571855438866447...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26197047098349546...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2473118279569893\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2571855438866447 and Equi_Max = 0.26419193437074484\n",
      "Game 93\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.264782736433734\n",
      "Game 94\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49473684210526314\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2572360901692127...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2619202819587613...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24736842105263163\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2572360901692127 and Equi_Max = 0.2640854186365579\n",
      "Game 95\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26468151458860345\n",
      "Game 96\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4948453608247423\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2572845485002372...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2618721590157746...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2474226804123712\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2572845485002372 and Equi_Max = 0.2639835140849304\n",
      "Game 97\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2645842991641687\n",
      "Game 98\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.494949494949495\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2573310456341551...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26182597714937844...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24747474747474754\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2573310456341551 and Equi_Max = 0.2638859314454611\n",
      "Game 99\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26449085401815753\n",
      "Game 100\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49504950495049505\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25737569826899404...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2617816212393615...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24752475247524758\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25737569826899404 and Equi_Max = 0.26379240500023343\n",
      "Game 101\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2644009613222866\n",
      "Game 102\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49514563106796117\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2574186140242903...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26173898509232046...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24757281553398064\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2574186140242903 and Equi_Max = 0.2637026902430054\n",
      "Game 103\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2643144198140754\n",
      "Game 104\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49523809523809526\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2574598923070672...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2616979705928492...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24761904761904768\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2574598923070672 and Equi_Max = 0.2636165618115004\n",
      "Game 105\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2642310432456499\n",
      "Game 106\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4953271028037383\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2574996250805458...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2616584869497713...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24766355140186921\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2574996250805458 and Equi_Max = 0.2635338116563922\n",
      "Game 107\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2641506590040692\n",
      "Game 108\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4954128440366973\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2575378975480983...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26162045002522794...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2477064220183487\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2575378975480983 and Equi_Max = 0.26345424741601864\n",
      "Game 109\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2640731068814134\n",
      "Game 110\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4954954954954955\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25757478876315043...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26158378173618757...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2477477477477478\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.505 0.495]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25757478876315043 and Equi_Max = 0.26337769097039054\n",
      "Game 111\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26399823797596716\n",
      "Game 112\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49557522123893805\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25761037217422117...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26154840951941694...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24778761061946908\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25761037217422117 and Equi_Max = 0.2633039771518771\n",
      "Game 113\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26392591370845603\n",
      "Game 114\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4956521739130435\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.257644716113007...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2615142658521993...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2478260869565218\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.257644716113007 and Equi_Max = 0.26323295259314344\n",
      "Game 115\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26385600493949496\n",
      "Game 116\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49572649572649574\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2576778842323353...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2614812878221397...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24786324786324793\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2576778842323353 and Equi_Max = 0.2631644746956178\n",
      "Game 117\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26378839117628583\n",
      "Game 118\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4957983193277311\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25770993589989344...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2614494167402893...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2478991596638656\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25770993589989344 and Equi_Max = 0.2630984107040559\n",
      "Game 119\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2637229598581894\n",
      "Game 120\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49586776859504134\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2577409265528568...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26141859779258503...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24793388429752072\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2577409265528568 and Equi_Max = 0.26303463687470197\n",
      "Game 121\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26365960571215624\n",
      "Game 122\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4959349593495935\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25777090801787267...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2613887797252477...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2479674796747968\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25777090801787267 and Equi_Max = 0.2629730377262078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 123\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26359823017016015\n",
      "Game 124\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.496\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2577999288002848...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2613599145603417...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24800000000000005\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2577999288002848 and Equi_Max = 0.2629135053638739\n",
      "Game 125\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26353874084177487\n",
      "Game 126\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49606299212598426\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25782803434599394...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26133195733817594...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24803149606299218\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25782803434599394 and Equi_Max = 0.262855938868988\n",
      "Game 127\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2634810510358866\n",
      "Game 128\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49612403100775193\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2578552672789288...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2613048658836352...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24806201550387602\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2578552672789288 and Equi_Max = 0.2628002437460742\n",
      "Game 129\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26342507932627557\n",
      "Game 130\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4961832061068702\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2578816676167361...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26127860059388963...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24809160305343517\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2578816676167361 and Equi_Max = 0.26274633142175313\n",
      "Game 131\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26337074915643427\n",
      "Game 132\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49624060150375937\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25790727296698474...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26125312424523384...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24812030075187974\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25790727296698474 and Equi_Max = 0.26269411878968824\n",
      "Game 133\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26331798847954097\n",
      "Game 134\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4962962962962963\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25793211870591015...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.261228401817074...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2481481481481482\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25793211870591015 and Equi_Max = 0.2626435277967573\n",
      "Game 135\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2632667294299885\n",
      "Game 136\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49635036496350365\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25795623814148144...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2612044003313144...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24817518248175188\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25795623814148144 and Equi_Max = 0.26259448506616123\n",
      "Game 137\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26321690802328085\n",
      "Game 138\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49640287769784175\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2579796626623752...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26118108870559176...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24820143884892093\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2579796626623752 and Equi_Max = 0.2625469215536886\n",
      "Game 139\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26316846388147597\n",
      "Game 140\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49645390070921985\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25800242187425526...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2611584376189866...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24822695035460998\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.504 0.496]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25800242187425526 and Equi_Max = 0.2625007722337877\n",
      "Game 141\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26312133998166753\n",
      "Game 142\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4965034965034965\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2580245437246037...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2611364193889891...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2482517482517483\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2580245437246037 and Equi_Max = 0.2624559758124789\n",
      "Game 143\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2630754824252787\n",
      "Game 144\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.496551724137931\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25804605461720836...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2611150078586368...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24827586206896557\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25804605461720836 and Equi_Max = 0.26241247446446947\n",
      "Game 145\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2630308402261813\n",
      "Game 146\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4965986394557823\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2580669795172948...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26109417829285353...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2482993197278912\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2580669795172948 and Equi_Max = 0.26237021359213175\n",
      "Game 147\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2629873651158703\n",
      "Game 148\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4966442953020134\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2580873420481814...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26107390728312857...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24832214765100677\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2580873420481814 and Equi_Max = 0.2623291416042525\n",
      "Game 149\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26294501136410897\n",
      "Game 150\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4966887417218543\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2581071645802448...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2610541726597621...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2483443708609272\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2581071645802448 and Equi_Max = 0.2622892097126912\n",
      "Game 151\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26290373561362873\n",
      "Game 152\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49673202614379086\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2581264683129001...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2610349534109859...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2483660130718955\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2581264683129001 and Equi_Max = 0.2622503717452822\n",
      "Game 153\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26286349672761306\n",
      "Game 154\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4967741935483871\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2581452733502281...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26101622960833853...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2483870967741936\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2581452733502281 and Equi_Max = 0.26221258397348735\n",
      "Game 155\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26282425564882367\n",
      "Game 156\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4968152866242038\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25816359877081485...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2609979823377392...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24840764331210197\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25816359877081485 and Equi_Max = 0.26217580495346504\n",
      "Game 157\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.262785975269344\n",
      "Game 158\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4968553459119497\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2581814626923157...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2609801936357567...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2484276729559749\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2581814626923157 and Equi_Max = 0.2621399953793517\n",
      "Game 159\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2627486203100168\n",
      "Game 160\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4968944099378882\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25819888233120186...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26096284643062495...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24844720496894415\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25819888233120186 and Equi_Max = 0.2621051179476797\n",
      "Game 161\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2627121572087419\n",
      "Game 162\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49693251533742333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2582158740581041...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2609459244875957...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24846625766871172\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2582158740581041 and Equi_Max = 0.2620711372319572\n",
      "Game 163\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2626765540168833\n",
      "Game 164\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49696969696969695\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25823245344912704...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26092941235826117...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24848484848484853\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25823245344912704 and Equi_Max = 0.26203801956653505\n",
      "Game 165\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26264178030310464\n",
      "Game 166\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49700598802395207\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25824863533347325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26091329533351587...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2485029940119761\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25824863533347325 and Equi_Max = 0.2620057329389699\n",
      "Game 167\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2626078070640194\n",
      "Game 168\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4970414201183432\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2582644338376815...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2608975593998537...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24852071005917165\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2582644338376815 and Equi_Max = 0.26197424689016646\n",
      "Game 169\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2625746066410964\n",
      "Game 170\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49707602339181284\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25827986242675816...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2608821911987297...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24853801169590647\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25827986242675816 and Equi_Max = 0.26194353242165475\n",
      "Game 171\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2625421526433158\n",
      "Game 172\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49710982658959535\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2582949339424534...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26086717798873726...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24855491329479773\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2582949339424534 and Equi_Max = 0.26191356190941284\n",
      "Game 173\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2625104198751151\n",
      "Game 174\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49714285714285716\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2583096606389085...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2608525076103776...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24857142857142864\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2583096606389085 and Equi_Max = 0.2618843090237059\n",
      "Game 175\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26247938426920747\n",
      "Game 176\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4971751412429379\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25832405421588583...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26083816845321417...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.248587570621469\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25832405421588583 and Equi_Max = 0.26185574865445493\n",
      "Game 177\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26244902282389143\n",
      "Game 178\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4972067039106145\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2583381258497687...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26082414942522747...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2486033519553073\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2583381258497687 and Equi_Max = 0.26182785684169907\n",
      "Game 179\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2624193135445046\n",
      "Game 180\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4972375690607735\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25835188622250305...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26081043992419983...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2486187845303868\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25835188622250305 and Equi_Max = 0.261800610710749\n",
      "Game 181\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2623902353887055\n",
      "Game 182\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4972677595628415\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.258365345548641...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26079702981097486...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24863387978142082\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.258365345548641 and Equi_Max = 0.26177398841166827\n",
      "Game 183\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2623617682152937\n",
      "Game 184\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4972972972972973\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2583785136006278...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2607839093844501...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24864864864864872\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2583785136006278 and Equi_Max = 0.26174796906274944\n",
      "Game 185\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26233389273630253\n",
      "Game 186\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49732620320855614\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.258391399732465...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26077106935817157...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24866310160427813\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.258391399732465 and Equi_Max = 0.2617225326976802\n",
      "Game 187\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26230659047212507\n",
      "Game 188\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4973544973544973\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2584040129018696...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2607585008384153...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24867724867724872\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2584040129018696 and Equi_Max = 0.26169766021612434\n",
      "Game 189\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2622798437094487\n",
      "Game 190\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4973821989528796\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2584163616910401...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2607461953036429...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24869109947643986\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2584163616910401 and Equi_Max = 0.261673333337461\n",
      "Game 191\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2622536354617959\n",
      "Game 192\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49740932642487046\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25842845432613...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2607341445852343...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24870466321243528\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25842845432613 and Equi_Max = 0.26164953455745116\n",
      "Game 193\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2622279494324853\n",
      "Game 194\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49743589743589745\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2584402986955223...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2607223408494049...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24871794871794878\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2584402986955223 and Equi_Max = 0.2616262471076162\n",
      "Game 195\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2622027699798391\n",
      "Game 196\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49746192893401014\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25845190236698923...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2607107765802232...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24873096446700513\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.25845190236698923 and Equi_Max = 0.2616034549171355\n",
      "Game 197\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.26217808208448234\n",
      "Game 198\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.49748743718592964\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2584632726038172...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.26069944456365196...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.24874371859296487\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.503 0.497]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2584632726038172 and Equi_Max = 0.26158114257708015\n",
      "Game 199\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 1\n",
      "pop\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.25958229500415325...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2595822950041532...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent2 's opinion 0.0 changed to 0.75\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.5 0.5]\n",
      "pop\n",
      "Same history\n",
      "('2', '0.75')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2595822950041532 and Equi_Max = 0.2621538713185848\n",
      "Game 200\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent1 's opinion 0.5 changed to 0\n",
      "pop\n",
      "fre_max at spot\n",
      "0.4975124378109453\n",
      "_______________________\n",
      "Minimizer Play\n",
      "____\n",
      "Min start with agent 0\n",
      "Weighted polarization\n",
      "0.2584744163799678...\n",
      "____\n",
      "Min start with agent 2\n",
      "Weighted polarization\n",
      "0.2606883378725406...\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent0 's opinion 1.0 changed to 0.2487562189054727\n",
      "fla_max_fre\n",
      "(array([2, 3], dtype=int64),)\n",
      "[0.502 0.498]\n",
      "pop\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.2584744163799678 and Equi_Max = 0.2615592953068198\n",
      "Game 201\n",
      "_____________________\n",
      "min_recent_50_touched\n",
      "[2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0]\n",
      "max_recent_50_touched\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Min last 100 action\n",
      "[2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "Experiment = 10\n",
    "\n",
    "Experiment_note = str('Note: This experiement has initial condition. Game round:'+str(Game_rounds)+'.')\n",
    "(First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, fla_max_fre, max_history_last_100) = all_fre_limited_touch(s, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_distribution\n",
      "[0.5 0.5]\n",
      "(array([1, 1], dtype=int64), array([0, 1], dtype=int64))\n",
      "Min_distribution_last_100\n",
      "dict_keys([(2, 0.75), (0, 0.24757281553398064), (0, 0.24761904761904768), (0, 0.24766355140186921), (0, 0.2477064220183487), (0, 0.2477477477477478), (0, 0.24778761061946908), (0, 0.2478260869565218), (0, 0.24786324786324793), (0, 0.2478991596638656), (0, 0.24793388429752072), (0, 0.2479674796747968), (0, 0.24800000000000005), (0, 0.24803149606299218), (0, 0.24806201550387602), (0, 0.24809160305343517), (0, 0.24812030075187974), (0, 0.2481481481481482), (0, 0.24817518248175188), (0, 0.24820143884892093), (0, 0.24822695035460998), (0, 0.2482517482517483), (0, 0.24827586206896557), (0, 0.2482993197278912), (0, 0.24832214765100677), (0, 0.2483443708609272), (0, 0.2483660130718955), (0, 0.2483870967741936), (0, 0.24840764331210197), (0, 0.2484276729559749), (0, 0.24844720496894415), (0, 0.24846625766871172), (0, 0.24848484848484853), (0, 0.2485029940119761), (0, 0.24852071005917165), (0, 0.24853801169590647), (0, 0.24855491329479773), (0, 0.24857142857142864), (0, 0.248587570621469), (0, 0.2486033519553073), (0, 0.2486187845303868), (0, 0.24863387978142082), (0, 0.24864864864864872), (0, 0.24866310160427813), (0, 0.24867724867724872), (0, 0.24869109947643986), (0, 0.24870466321243528), (0, 0.24871794871794878), (0, 0.24873096446700513), (0, 0.24874371859296487), (0, 0.2487562189054727)])\n",
      "fla_min_fre\n",
      "[0.5 0.5]\n",
      "Counter({(2, 0.75): 50, (0, 0.24757281553398064): 1, (0, 0.24761904761904768): 1, (0, 0.24766355140186921): 1, (0, 0.2477064220183487): 1, (0, 0.2477477477477478): 1, (0, 0.24778761061946908): 1, (0, 0.2478260869565218): 1, (0, 0.24786324786324793): 1, (0, 0.2478991596638656): 1, (0, 0.24793388429752072): 1, (0, 0.2479674796747968): 1, (0, 0.24800000000000005): 1, (0, 0.24803149606299218): 1, (0, 0.24806201550387602): 1, (0, 0.24809160305343517): 1, (0, 0.24812030075187974): 1, (0, 0.2481481481481482): 1, (0, 0.24817518248175188): 1, (0, 0.24820143884892093): 1, (0, 0.24822695035460998): 1, (0, 0.2482517482517483): 1, (0, 0.24827586206896557): 1, (0, 0.2482993197278912): 1, (0, 0.24832214765100677): 1, (0, 0.2483443708609272): 1, (0, 0.2483660130718955): 1, (0, 0.2483870967741936): 1, (0, 0.24840764331210197): 1, (0, 0.2484276729559749): 1, (0, 0.24844720496894415): 1, (0, 0.24846625766871172): 1, (0, 0.24848484848484853): 1, (0, 0.2485029940119761): 1, (0, 0.24852071005917165): 1, (0, 0.24853801169590647): 1, (0, 0.24855491329479773): 1, (0, 0.24857142857142864): 1, (0, 0.248587570621469): 1, (0, 0.2486033519553073): 1, (0, 0.2486187845303868): 1, (0, 0.24863387978142082): 1, (0, 0.24864864864864872): 1, (0, 0.24866310160427813): 1, (0, 0.24867724867724872): 1, (0, 0.24869109947643986): 1, (0, 0.24870466321243528): 1, (0, 0.24871794871794878): 1, (0, 0.24873096446700513): 1, (0, 0.24874371859296487): 1, (0, 0.2487562189054727): 1})\n",
      "dict_keys([(0, 1), (2, 0.75), (0, 0.16666666666666669), (0, 0.20000000000000007), (0, 0.21428571428571433), (0, 0.22222222222222227), (0, 0.22727272727272732), (0, 0.23076923076923084), (0, 0.2333333333333334), (0, 0.23529411764705888), (0, 0.23684210526315794), (0, 0.23809523809523814), (0, 0.23913043478260876), (0, 0.24000000000000005), (0, 0.24074074074074078), (0, 0.24137931034482765), (0, 0.2419354838709678), (0, 0.2424242424242425), (0, 0.2428571428571429), (0, 0.2432432432432433), (0, 0.24358974358974364), (0, 0.2439024390243903), (0, 0.24418604651162795), (0, 0.2444444444444445), (0, 0.24468085106382984), (0, 0.24489795918367352), (0, 0.24509803921568632), (0, 0.24528301886792458), (0, 0.2454545454545455), (0, 0.24561403508771934), (0, 0.24576271186440685), (0, 0.24590163934426235), (0, 0.24603174603174607), (0, 0.24615384615384622), (0, 0.24626865671641796), (0, 0.24637681159420297), (0, 0.24647887323943668), (0, 0.24657534246575347), (0, 0.24666666666666673), (0, 0.2467532467532468), (0, 0.24683544303797475), (0, 0.24691358024691362), (0, 0.24698795180722896), (0, 0.24705882352941183), (0, 0.24712643678160925), (0, 0.2471910112359551), (0, 0.24725274725274732), (0, 0.2473118279569893), (0, 0.24736842105263163), (0, 0.2474226804123712), (0, 0.24747474747474754), (0, 0.24752475247524758), (0, 0.24757281553398064), (0, 0.24761904761904768), (0, 0.24766355140186921), (0, 0.2477064220183487), (0, 0.2477477477477478), (0, 0.24778761061946908), (0, 0.2478260869565218), (0, 0.24786324786324793), (0, 0.2478991596638656), (0, 0.24793388429752072), (0, 0.2479674796747968), (0, 0.24800000000000005), (0, 0.24803149606299218), (0, 0.24806201550387602), (0, 0.24809160305343517), (0, 0.24812030075187974), (0, 0.2481481481481482), (0, 0.24817518248175188), (0, 0.24820143884892093), (0, 0.24822695035460998), (0, 0.2482517482517483), (0, 0.24827586206896557), (0, 0.2482993197278912), (0, 0.24832214765100677), (0, 0.2483443708609272), (0, 0.2483660130718955), (0, 0.2483870967741936), (0, 0.24840764331210197), (0, 0.2484276729559749), (0, 0.24844720496894415), (0, 0.24846625766871172), (0, 0.24848484848484853), (0, 0.2485029940119761), (0, 0.24852071005917165), (0, 0.24853801169590647), (0, 0.24855491329479773), (0, 0.24857142857142864), (0, 0.248587570621469), (0, 0.2486033519553073), (0, 0.2486187845303868), (0, 0.24863387978142082), (0, 0.24864864864864872), (0, 0.24866310160427813), (0, 0.24867724867724872), (0, 0.24869109947643986), (0, 0.24870466321243528), (0, 0.24871794871794878), (0, 0.24873096446700513), (0, 0.24874371859296487), (0, 0.2487562189054727)])\n",
      "Min_distribution_all\n",
      "[0.498 0.498]\n",
      "Max_distribution_all\n",
      "[0.502 0.498]\n",
      "[(array([1, 1], dtype=int64), array([0, 1], dtype=int64))]\n"
     ]
    }
   ],
   "source": [
    "# MAXimizer's distribution of LAST 100 iteration \n",
    "print('Max_distribution')  \n",
    "max_l100_fre = max_history_last_100/100\n",
    "print(max_l100_fre [np.nonzero(max_l100_fre)])\n",
    "# print for small network\n",
    "#print(max_history_last_100)\n",
    "# # Print for Large Network\n",
    "print(np.nonzero(max_l100_fre))\n",
    "\n",
    "# MINimizer's Strategy in the last 100 round\n",
    "print('Min_distribution_last_100')\n",
    "counter_h=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter_h.keys())\n",
    "counter=collections.Counter(min_touched_last_100)\n",
    "fla_min_fre = np.array(list(counter.values()))/(100) #return only frequency of all min options in order\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)\n",
    "print(counter_h)\n",
    "# print(min_touched_last_100)\n",
    "\n",
    "\n",
    "counter_1h=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter_1h.keys())\n",
    "counter_1=collections.Counter(min_touched_all)  #return a dictionary include {'min_option': count of this choice}\n",
    "# print(counter_1)\n",
    "fla_min_fre_1 = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "print('Min_distribution_all')\n",
    "print(fla_min_fre_1)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "max_fre = max_history/Game_rounds\n",
    "print('Max_distribution_all')\n",
    "print(max_fre[np.nonzero(max_fre)])\n",
    "print([np.nonzero(max_fre)])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 1, 29: 1})\n"
     ]
    }
   ],
   "source": [
    "print(counter_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.111 0.151 0.12  ... 0.106 0.135 0.098]\n",
      " [0.115 0.157 0.109 ... 0.112 0.138 0.104]\n",
      " [0.115 0.157 0.109 ... 0.112 0.138 0.105]\n",
      " ...\n",
      " [0.114 0.157 0.109 ... 0.111 0.138 0.104]\n",
      " [0.114 0.157 0.109 ... 0.111 0.138 0.104]\n",
      " [0.114 0.157 0.109 ... 0.111 0.138 0.104]]\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = 'Extr.2'\n",
    "Experiment = 'Mixed_1'\n",
    "pd.DataFrame(payoff_matrix).to_csv('Extr.2 Payoff Matrix'+ str(Experiment)+'.csv')\n",
    "with open('Result'+str(Network)+'.'+str(Experiment)+'.txt', \"a\") as f:\n",
    "#     print(Experiment_note, file=f)\n",
    "    print('Initial Condition -(agent, opinion, pol)', file=f)\n",
    "    print('Innate op'+str(s),file=f)\n",
    "    print('Adjacency matrix'+ str(G), file=f)\n",
    "    print('Max:'+ str(First_max), file=f)\n",
    "    print('Min' + str(First_min), file=f)\n",
    "\n",
    "    print(\"In the Last 100 Rounds\", file=f) \n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # MAX distribution of LAST 100 iteration \n",
    "    print('Max_distribution', file=f)  \n",
    "    max_l100_fre = max_history_last_100/100\n",
    "    print(max_l100_fre [np.nonzero(max_l100_fre)], file=f)\n",
    "    # print for small network\n",
    "    #print(max_history_last_100, file=f)\n",
    "    # # Print for Large Network\n",
    "    print(np.nonzero(max_l100_fre),file=f)\n",
    "\n",
    "    # MIN Strategy in the last 100 round\n",
    "    counter=collections.Counter(min_touched_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    # print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/100 #return only frequency of all min options in order\n",
    "#     print('Min_frequency', file=f)\n",
    "#     print(list(counter.keys()), file=f)\n",
    "    print('Min_distribution_last_100', file=f)\n",
    "    print(fla_min_fre, file=f)\n",
    "    counter_h=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_h, file=f)\n",
    "    \n",
    "    print('min_recent_'+str(memory)+'_touched', file=f)# then stop at Game 202\n",
    "    print(min_touched, file=f)\n",
    "    print('max_recent_'+str(memory)+'_touched', file=f)\n",
    "    print(max_touched, file=f)\n",
    "    \n",
    "    print('In Overall'+str(Game_rounds)+' Rounds', file=f)\n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # Max action Overall \n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    max_fre = max_history/Game_rounds\n",
    "#     print('Max_frequency', file=f)\n",
    "#     print(max_history, file=f)\n",
    "    print('Max_distribution', file=f)\n",
    "    print(max_fre [np.nonzero(max_fre)], file=f)\n",
    "    print(np.nonzero(max_fre),file=f)\n",
    "\n",
    "\n",
    "    # Min Strategy in the Overall    \n",
    "    counter_1=collections.Counter(min_touched_all)  #return a dictionary include {'min_option': count of this choice}\n",
    "    fla_min_fre_all = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "    print('Min_dist_all', file=f)\n",
    "    print(fla_min_fre_all, file=f)\n",
    "    print('Min_distribution', file=f)\n",
    "    counter_a=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_a, file=f)\n",
    "#     print(payoff_matrix, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(6, 0.0): 358, (29, 1.0): 37, (29, 0.9999999999999999): 5, (13, 1): 1})\n",
      "fla_min_fre\n",
      "[0.002 0.893 0.092 0.012]\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(min_history) \n",
    "print(counter)\n",
    "fla_min_fre = np.array(list(counter.values()))/Game_rounds\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
