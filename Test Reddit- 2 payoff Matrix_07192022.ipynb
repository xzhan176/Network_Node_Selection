{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This code is from Updated Testing Reddit - No Con- bias (Fictitious Play)-01092022\n",
    "##### This code replace the big real datanetwork with small sythetic network \n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import time\n",
    "import random\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib inline\n",
    "%run pure_strategy_selection.ipynb  #include simple selection algorithm\n",
    "import scipy.io\n",
    "import collections\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathmatic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centers the opinion vector around 0\\n\",\n",
    "def mean_center(op, n):\n",
    "    ones = np.ones((n, 1))\n",
    "    x = op - (np.dot(np.transpose(op),ones)/n) * ones\n",
    "    return x\n",
    "    \n",
    "# compute number of edges, m\\n\n",
    "def num_edges(L, n):\n",
    "    m = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i > j and L[i,j] < 0:\n",
    "                m += 1            \n",
    "    return m\n",
    "\n",
    "# maximizing polarization only: \\\\bar{z}^T \\\\bar{z}   \n",
    "def obj_polarization(A, L, op, n):\n",
    "    op_mean = mean_center(op, n)\n",
    "    z_mean = np.dot(A, op_mean) \n",
    "    return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "def obj_polarization_1(A, L, op, n):\n",
    "    z = np.dot(A, op) \n",
    "    z_mean = mean_center(z, n)\n",
    "    return np.dot(np.transpose(z_mean), z_mean)[0,0] \n",
    "\n",
    "# Calculate innate polarization\n",
    "def obj_innate_polarization(s, n):  \n",
    "#     np.set_printoptions(precision=5)\n",
    "    op_mean = mean_center(s, n)\n",
    "    return np.dot(np.transpose(op_mean), op_mean)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the network\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Network\n",
    "### 1. Make Random Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.sparse' has no attribute 'coo_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m nxG \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mfrom_numpy_matrix(G)          \n\u001b[0;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m))\n\u001b[1;32m---> 29\u001b[0m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnxG\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:120\u001b[0m, in \u001b[0;36mdraw\u001b[1;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[0;32m    118\u001b[0m     kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds\n\u001b[1;32m--> 120\u001b[0m draw_networkx(G, pos\u001b[38;5;241m=\u001b[39mpos, ax\u001b[38;5;241m=\u001b[39max, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    121\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_axis_off()\n\u001b[0;32m    122\u001b[0m plt\u001b[38;5;241m.\u001b[39mdraw_if_interactive()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\nx_pylab.py:331\u001b[0m, in \u001b[0;36mdraw_networkx\u001b[1;34m(G, pos, arrows, with_labels, **kwds)\u001b[0m\n\u001b[0;32m    328\u001b[0m label_kwds \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwds\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m valid_label_kwds}\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrawing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspring_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# default to spring layout\u001b[39;00m\n\u001b[0;32m    333\u001b[0m draw_networkx_nodes(G, pos, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnode_kwds)\n\u001b[0;32m    334\u001b[0m draw_networkx_edges(G, pos, arrows\u001b[38;5;241m=\u001b[39marrows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39medge_kwds)\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 8:4\u001b[0m, in \u001b[0;36margmap_spring_layout_5\u001b[1;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m splitext\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_random_state, create_py_random_state\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\drawing\\layout.py:476\u001b[0m, in \u001b[0;36mspring_layout\u001b[1;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(G) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:  \u001b[38;5;66;03m# sparse solver for large graphs\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_scipy_sparse_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m fixed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# We must adjust k by domain size for layouts not near 1x1\u001b[39;00m\n\u001b[0;32m    479\u001b[0m     nnodes, _ \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\networkx\\convert_matrix.py:921\u001b[0m, in \u001b[0;36mto_scipy_sparse_array\u001b[1;34m(G, nodelist, dtype, weight, format)\u001b[0m\n\u001b[0;32m    919\u001b[0m         r \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m diag_index\n\u001b[0;32m    920\u001b[0m         c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m diag_index\n\u001b[1;32m--> 921\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoo_array\u001b[49m((d, (r, c)), shape\u001b[38;5;241m=\u001b[39m(nlen, nlen), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m A\u001b[38;5;241m.\u001b[39masformat(\u001b[38;5;28mformat\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy.sparse' has no attribute 'coo_array'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = scipy.io.loadmat(\"Reddit.mat\")\n",
    "n = data['Reddit'][0,0][0].shape[0]     # number of vertices = 556\n",
    "G = data['Reddit'][0,0][0].toarray()     # adjacency matrix in compressed sparse column format, convert to array\n",
    "nodemap = data['Reddit'][0, 0][1]     # mapping from node ID to labels 1-556 (not important)\n",
    "edges = data['Reddit'][0,0][2]     # list of edges (same as G, not used)\n",
    "s = data['Reddit'][0,0][5]     # labeled \"recent innate opinions\"\n",
    "\n",
    "# remove isolated vertices from the graph\n",
    "s = np.delete(s, 551)\n",
    "s = np.delete(s, 105)\n",
    "s = np.delete(s, 52)\n",
    "n -= 3\n",
    "s = s.reshape((n , 1))\n",
    "G = np.delete(G, 551, 1)\n",
    "G = np.delete(G, 551, 0)\n",
    "G = np.delete(G, 105, 1)\n",
    "G = np.delete(G, 105, 0)\n",
    "G = np.delete(G, 52, 1)\n",
    "G = np.delete(G, 52, 0)\n",
    "\n",
    "L = scipy.sparse.csgraph.laplacian(G, normed=False)  # Return the Laplacian matrix\n",
    "A = np.linalg.inv(np.identity(n) + L)  # A = (I + L)^(-1)\\n  Stanford paper theory\n",
    "m = num_edges(L, n)                    # call the function to calculate the number of edges\n",
    "columnsum_ij = np.sum(A, axis=0)\n",
    "# print(columnsum_ij)\n",
    "# what the twitter graph looks like \n",
    "nxG = nx.from_numpy_matrix(G)          \n",
    "plt.figure(figsize=(20, 20))\n",
    "nx.draw(nxG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Equilibrium & Polarization  - based on derivation\n",
    "$$P(z) = z ^T * z $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\n",
      "0.9625703647920433\n",
      "Equi_polarization:\n",
      "0.005278437803904968\n",
      "Difference:\n",
      "-0.9572919269881383\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## s =  make_innat_opinions(n, c1)\n",
    "# print('Innate Opinion')\n",
    "# print(s)\n",
    "# print('Equilibrium Opinion')\n",
    "# print(np.dot(A, s))\n",
    "\n",
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, s, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing players' behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "  \n",
    "    v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "    new_op = random.randint(0, 1)  # randomly select an opininon between 0 and 1\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_play1(s,n):  # player randomly choose an agent and randomly change the agent\n",
    "    \n",
    "    op = copy.copy(s)\n",
    "#     max_opi_option = random.uniform(0, 1)   # options that maximizer have\n",
    "    \n",
    "    v = random.randint(0,n-1)  # randomly select an agent index\n",
    "#     print(v)\n",
    "#     v = 1\n",
    "    new_op = random.uniform(0, 1)  # randomly select an opininon between 0 and 1\n",
    "    #new_op = 0\n",
    "#     print(new_op)\n",
    "    \n",
    "    # Store old opinion\n",
    "    old_opinion = op[v,0]\n",
    "    \n",
    "    #update the opinion\n",
    "    op[v,0] = new_op \n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    print(\"    \"+\"Agent\" + str(v) +\" 's opinion \" + str(old_opinion) + \" changed to \"+ str(new_op))\n",
    "    por = obj_polarization(A, L, op, n)\n",
    "    \n",
    "    #restore op op array to innate opinion\n",
    "    op[v] = old_opinion\n",
    "    print(\"Network reaches equilibrium Polarization: \" + str(por))\n",
    "#     print('Should be restored')\n",
    "#     print(op)\n",
    "    return (v, new_op, por)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# min_touched =[]\n",
    "# (v1, maxmize_op, innat_equi_por, max_por) = choose_max_vertex(s, n, min_touched)\n",
    "# print(v1, maxmize_op, innat_equi_por, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing to see if random_play works -- NO NEED TO RUN\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# (v2, min_opinion, min_pol) = random_play(s,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximizer_fir_play(s,n,min_touched):    # maxmizer first-time play, greedy algorithm\n",
    "    op = copy.copy(s)\n",
    "\n",
    "    print('Maximizer Play')\n",
    "\n",
    "    max_champion = choose_max_vertex(op, n, min_touched) # The best choice among all opinions and vertexs, function is in \"pure_strategy_selection.ipynb\"\n",
    "    (v1, max_opinion, innate_obj, max_pol) = max_champion # find agent v1, and max_opinion that can maxmize the equi_polarization(max_pol)\n",
    "\n",
    "    if v1 == None:   # if maximizer cannot find one\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "#         print('v1', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(max_champion)\n",
    "\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        ##### change the agent's opinion with best action(agent v1, max_op)\n",
    "        op[v1,0] = max_opinion\n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_touched = []\n",
    "# min_touched = []\n",
    "# (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "# print(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### minimizer first-time play, greedy algorithm\n",
    "def minimizer_fir_play(s,n,max_touched): \n",
    "    \n",
    "    op = copy.copy(s)\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = choose_min_vertex(op, n, max_touched)\n",
    "    (v2, min_opinion, innat_equi_por, min_pol) = min_champion\n",
    "    \n",
    "   #Store innate_op of the min_selected vertex\n",
    "    old_opinion_min = op[v2,0]\n",
    "    \n",
    "    if v2 == None:\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "\n",
    "        ##### change the agent's opinion\n",
    "        op[v2,0] = min_opinion   #-------------------------------------------------> store minimize strategy\n",
    "\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "\n",
    "        print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return (v2,min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_touched = []\n",
    "# min_touched = []\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "# print(v2, min_opinion, min_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing above functions\n",
    "# min_touched=[]\n",
    "# max_touched=[]\n",
    "# # Game start from maximizer random play\n",
    "# print('Maximizer random selection')\n",
    "# (v1, max_opinion, max_pol) = random_play(s,n)\n",
    "# max_touched.append(v1)\n",
    "# # print('v1, max_opinion, max_pol')\n",
    "# # print(v1, max_opinion, max_pol)\n",
    "# # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Game start from minimizer random play \n",
    "# print('Minimizer random selection')\n",
    "# (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "# min_touched.append(v2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row are Column are depended on min and max's choice: agent v and opinion \n",
    "def row_index(v2, min_opinion):\n",
    "    row = 11*v2 + min_opinion*10 \n",
    "    return int(row)\n",
    "def column_index(v1,max_opinion):\n",
    "    column = 2*v1 + max_opinion\n",
    "    return int(column)  #the python dataframe index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Strategy Payoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_payoff_row(op1,v2):\n",
    "    payoff_row = np.zeros(2*n)\n",
    "    v1 = 0\n",
    "#     print('one opinion changed -min')\n",
    "#     print(op1)\n",
    "    for column in range(2*n):\n",
    "#         print(column)\n",
    "        v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "        max_opinion = column%2\n",
    "#         print(v1, max_opinion)\n",
    "        # update the maximizer's change to the opinion array that has changed by minimizer(op1)\n",
    "        op2 = copy.copy(op1)\n",
    "#         temp = op1[v1]\n",
    "        op2[v1,0] = max_opinion\n",
    "#         print('max_opinion')\n",
    "#         print(v1, max_opinion)\n",
    "#         print('two opinion changed -min +  max')\n",
    "#         print(op2)\n",
    "        # calculate the polarization with both max and min's action\n",
    "        payoff_row[column] = obj_polarization(A, L, op2, n)\n",
    "#         op1[v1,0] = temp # restore\n",
    "#         print(op2,payoff_row[column])\n",
    "    # when v1 == v2, the polarization should be negative for max, infinet for min. \n",
    "    # Replace the the column_index of agent v2 with 0 for max\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "    \n",
    "    return payoff_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.009 0.008 0.005 ... 0.005 0.005 0.005]\n"
     ]
    }
   ],
   "source": [
    "# #(1,0) (2,0.3928571428571428)\n",
    "# op1=copy.copy(s)\n",
    "# print(op1)\n",
    "\n",
    "op1 = copy.copy(s)\n",
    "# print(op1)\n",
    "op1[2,0] = 1  #op1 is the opinion array that updated by minimizer\n",
    "# print(op1)\n",
    "payoff_row_1 = make_payoff_row(op1,2)\n",
    "print(payoff_row_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEEDDDDDDD UPDAE\n",
    "\n",
    "# Calculate polarization of minimizer's Mixed Strategy\n",
    "def mixed_min_polarization(s,v2,min_opinion,fla_max_fre):\n",
    "\n",
    "    op1 =  copy.copy(s) # make a copy of the innate opinion array \n",
    "    op1[v2,0] = min_opinion # then only updated by minimizer's current change\n",
    "#     print('update')\n",
    "#     print(v2, min_opinion)\n",
    "    # calculate the polarization with both min(did here) and max's action(in make_payoff_row)\n",
    "    payoff_row = make_payoff_row(op1,v2)  # the vector list out 2*n payoffs after min's action combine with 2*n possible max's actions\n",
    "    #print(payoff_row)\n",
    "\n",
    "    # Replace the the column_index of agent v2 with 100 for min\n",
    "    j_1 = 2*v2 + 0\n",
    "    j_2 = 2*v2 + 1\n",
    "    payoff_row[j_1] = 100\n",
    "    payoff_row[j_2] = 100\n",
    "    \n",
    "#     print('Min Payoff Row')\n",
    "#     print(payoff_row)\n",
    "    #calculate fictitious payoff - equi_min  \n",
    "    payoff_cal = payoff_row * fla_max_fre # fla_max_fre recorded the frequency of each maximizer's action, frequency sum = 1\n",
    "                                             # payoff (2*n array) * maximizer_action_frequency (2*n array)\n",
    "\n",
    "    mixed_pol = np.sum(payoff_cal) # add up all, calculate average/expected payoff\n",
    "\n",
    "\n",
    "#     print('min_mixed_polarization')\n",
    "#     print(mixed_pol)\n",
    "        # Replace the the column_index of agent v2 with 100 for min\n",
    "\n",
    "    payoff_row[j_1] = -100\n",
    "    payoff_row[j_2] = -100\n",
    "\n",
    "    return (mixed_pol,payoff_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # op2=op\n",
    "# # op2[0,0]=1\n",
    "# # min_opinion1 = derivate_s(op2,n,1)\n",
    "# # # print(min_opinion1)\n",
    "# # min_opinion2 = derivate_s1(op2,n,1)\n",
    "# # print(min_opinion2)\n",
    "# v2 = 254\n",
    "# min_opinion = 0\n",
    "# (mixed_pol, payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "# print(np.nonzero(fla_max_fre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_s(op,n,v2):\n",
    "               #op - opinion array that updated by maximizer\n",
    "    c = [1/n] * n\n",
    "#     print(c)\n",
    "    sum_term = 0\n",
    "    j = 0\n",
    "\n",
    "    sum_term = np.dot(np.dot((A-c),(A[v2]-c)),op)  # sum up all terms\n",
    "    \n",
    "    term_out = op[v2]*np.dot((A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "    sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "    s_star = -sum_s/np.dot((A[v2]-c),(A[v2]-c))\n",
    "    s_star = s_star[0] #take value out of array\n",
    "    min_opinion =min(max(0,s_star),1)\n",
    "            \n",
    "    return min_opinion\n",
    "\n",
    "# def derivate_s1(op,n,v2):\n",
    "#                #op - opinion array that updated by maximizer\n",
    "#     c = [1/n] * n\n",
    "# #     print(c)\n",
    "#     sum_term = 0\n",
    "#     j = 0\n",
    "#     for j in range(0,n):\n",
    "#         term = op[j]*np.dot(np.transpose(A[j]-c),(A[v2]-c))\n",
    "# #             print(A[j])\n",
    "# #             print(A[v])\n",
    "#         sum_term = sum_term + term  # sum up all terms\n",
    "    \n",
    "#     term_out = op[v2]*np.dot(np.transpose(A[v2]-c),(A[v2]-c)) # exclude the term that j = v2\n",
    "#     sum_s = sum_term - term_out    # numerator\n",
    "    \n",
    "#     s_star = -sum_s/np.dot(np.transpose(A[v2]-c),(A[v2]-c))\n",
    "#     s_star = s_star[0] #take value out of array\n",
    "#     min_opinion =min(max(0,s_star),1)\n",
    "            \n",
    "#     return min_opinion\n",
    "\n",
    "\n",
    "\n",
    "def min_mixed_opinion(op, n, v2, fla_max_fre):\n",
    "    \n",
    "    weight_op = 0\n",
    "    \n",
    "    # loop for each max_action(in total 2*n) \n",
    "    for column in range(2*n):\n",
    "\n",
    "        if fla_max_fre[column] !=0:\n",
    "            v1 = int(column/2)  #i.e., column 11 is agent 5, opinion 1\n",
    "            max_opinion = column%2\n",
    "            \n",
    "##             temp = op[v1,0] \n",
    "          \n",
    "##             op[v1,0]= max_opinion #update innate opinion array with max_action    \n",
    "\n",
    "            min_opinion = derivate_s(op, n, v2)# find min_s_star for each max_action\n",
    "#             print(fla_max_fre[column],min_opinion)\n",
    "            weight_op = weight_op + fla_max_fre[column]*min_opinion # sum up p_i*s_i\n",
    "#     print(weight_op)\n",
    "##             op[v1,0] = temp \n",
    "    \n",
    "    (mixed_por, payoff_row) = mixed_min_polarization(s, v2, weight_op,fla_max_fre)\n",
    "    \n",
    "    return(weight_op,payoff_row,mixed_por)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(v2,fla_max_fre)\n",
    "# (weight_op_1,payoff_row,min_por) = min_mixed_opinion_1(s, n, v2, fla_max_fre)\n",
    "# print(weight_op_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = obj_polarization(A, L, s, n) #min_por- set a standard to compare with pol after min's action\n",
    "# # maxup_por = min_por # store innate max updated polarization\n",
    "# print(a)\n",
    "# print(s[253])\n",
    "# op = copy.copy(s)\n",
    "# op[253] = 0\n",
    "# b = obj_polarization(A, L, op, n)\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizer search: Go through each agent \n",
    "\n",
    "def mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre):\n",
    "    # current polarization that changed by maximizer, \"innate\" objective that min start with\n",
    "    op = copy.copy(s)\n",
    "    op[v1,0] = max_opinion\n",
    "#     print('Check if op has been updated by Maximizer')\n",
    "#     print(op)\n",
    "    min_por = obj_polarization(A, L, op, n) #min_por- set a standard to compare with pol after min's action\n",
    "    maxup_por = min_por # store innate max updated polarization\n",
    "#     print('check maxup por')\n",
    "#     print(maxup_por)\n",
    "#     payoffs = []    # create an empty list to store all polarizations   \n",
    "    champion = (None, None, 0, None)  # assume the best action is champion\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in max_touched]  # for the vertice that Maximizer has not touched\n",
    "    \n",
    "    for v2 in C1:   \n",
    "#         print('Min start with agent '+ str(v2) )\n",
    "        (changed_opinion, payoff_row, por) = min_mixed_opinion(op, n, v2, fla_max_fre)   # find the best new_op option           \n",
    "#         print('changed opinion, por, Maxup_por')\n",
    "#         print(changed_opinion, por, maxup_por)\n",
    "\n",
    "        if por < min_por:  # if the recent polarization is smaller than the minimum polarization in the history\n",
    "            min_por = por\n",
    "                                 # update the recent option as champion\n",
    "            champion = (v2, changed_opinion, payoff_row, min_por)  \n",
    "#         else:\n",
    "#             print('Innate polarization is smaller than Min action')\n",
    "\n",
    "    return (champion)  # find the best minimizer's action after going through every new_op option of every agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('v1,max_opinion')\n",
    "# print(v1,max_opinion)\n",
    "# champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by maximizer, fla_max_fre includes max's hisotry, so minimizer react to the innate op after that\n",
    "def mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre): \n",
    "\n",
    "    print('_______________________')\n",
    "    print('Minimizer Play')\n",
    "#     print('Only 1 opinion changed')\n",
    "#     print(op)\n",
    "    \n",
    "    min_champion = mixed_choose_min_vertex(s, n, v1, max_opinion, max_touched, fla_max_fre)\n",
    "    (v2, min_opinion, payoff_row, min_pol) = min_champion\n",
    "    \n",
    "    if v2 == None:    # if minimizer cannot find a action to minimize polarization after maximizer's action\n",
    "        print('Minimizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Minimizer finds its target agent:\")\n",
    "#         print('v2', 'changed_opinion', 'innate_obj', 'obj')\n",
    "#         print(v2, min_opinion, innat_equi_por, min_pol)\n",
    "\n",
    "        # Store innate_op of the min_selected vertex\n",
    "        old_opinion_min = op[v2,0]\n",
    "\n",
    "        print(\"    \"+\"Agent\" + str(v2) +\" 's opinion \" + str(old_opinion_min) + \" changed to \"+ str(min_opinion))\n",
    "#         print(\"Payoff row\")\n",
    "#         print(payoff_row)\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(min_pol))\n",
    "#         print('2 opinion changed')\n",
    "    return (v2, payoff_row, min_opinion, min_pol)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_touched' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3428/10731619.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_touched\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpayoff_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_opinion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolarization\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixed_min_play\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_opinion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_touched\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfla_max_fre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# print('v2, payoff_row, min_opinion, polarization')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# print(v2, payoff_row, min_opinion, polarization)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_touched' is not defined"
     ]
    }
   ],
   "source": [
    "print(max_touched)\n",
    "(v2, payoff_row, min_opinion, polarization) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "# print('v2, payoff_row, min_opinion, polarization')\n",
    "# print(v2, payoff_row, min_opinion, polarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizer Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Op has been updated by minimizer, fla_min_fre includes min's hisotry, so maxmizer react to the innate op after that\n",
    "def mixed_max_polarization(payoff_matrix,v1,max_opinion,fla_min_fre):\n",
    "\n",
    "    # create payoff matrix for maxmizer\n",
    "    column = int(column_index(v1,max_opinion))\n",
    "#     print(payoff_matrix)\n",
    "#     print(\"column\"+str(column))\n",
    "    payoff_vector = payoff_matrix[:,column]\n",
    "    \n",
    "#     print('payoff vector')\n",
    "#     print(payoff_vector)\n",
    "\n",
    "    #calculate fictitious payoff - equi_max   \n",
    "    payoff_cal = payoff_vector * fla_min_fre #payoff * frequency\n",
    "    \n",
    "#     print('max_payoff_calculation')\n",
    "#     print(payoff_cal)\n",
    "    mixed_pol = np.sum(payoff_cal) # add up\n",
    "#     print(\"Max_mixed_polarization\")\n",
    "#     print(mixed_pol)\n",
    "\n",
    "    return mixed_pol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_pol = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines if value of opinion at v should be set to 0 or 1 to maximize equilibrium polarization \n",
    "def max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre):\n",
    "    \n",
    "    por_arr = np.zeros(2)  # create a two_element array to store polarization value of each option\n",
    "\n",
    "\n",
    "    max_opi_option = [0, 1.0]   # Maximizer has two options to change agent v1's opinion\n",
    "    \n",
    "    # objective if set opinion to 0, 1.0\n",
    "    j = 0\n",
    "    for new_op in max_opi_option:\n",
    "#         print('change op to '+ str(i/10))\n",
    "        max_opinion = new_op\n",
    "\n",
    "        por_arr[j] = mixed_max_polarization(payoff_matrix,v1,max_opinion, fla_min_fre)\n",
    "    \n",
    "        j = j + 1   # index increase 1, put the polarization in array\n",
    "\n",
    "#     print('Polarization Options')\n",
    "#     print(por_arr)\n",
    "    \n",
    "    maxmize_op = np.argmax(por_arr)  # the index of maximum polarization = max_opinion --[0,1]\n",
    "    max_por = np.max(por_arr)        # find the maximum polarization in the record\n",
    " \n",
    "#     print('new_op', 'innat_equi_por', 'max_por')\n",
    "#     print(maxmize_op, innat_equi_por, max_por)\n",
    "\n",
    "    return (maxmize_op, max_por)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fla_min_fre = [0, 0, 0, 0, 0.65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.35, 0, 0, 0]\n",
    "# v1 = 2\n",
    "# champion = max_mixed_opinion(payoff_matrix, n, v1, v2, fla_min_fre)\n",
    "# print(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which agent maximizer should select to maximizer the equilibrium polarization\n",
    "def mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre):\n",
    "#     print('Check if op has been updated by minimizer')\n",
    "#     print(op)\n",
    "    max_por = obj_polarization(A, L, op, n)  # use \"innate\"(after min action) polarization as a comparable standard to find max_por\n",
    "    minup_por = max_por # store innate min_update polarization\n",
    "#     print('check minup por')\n",
    "#     print(minup_por)\n",
    "    champion = (None, None, max_por)  # assume champion is the best action\n",
    "\n",
    "    all = list(range(n))    # for all agent \n",
    "    C1 = [x for x in all if x not in min_touched]  # for the vertice that Minimizer has not touched\n",
    "    for v1 in C1:  \n",
    "#             print('Maximizer start from agent'+str(v1))\n",
    "            (changed_opinion, por) = max_mixed_opinion(payoff_matrix, n, v1, fla_min_fre)\n",
    "#             print('changed_opinion, por, minup_por')\n",
    "#             print(changed_opinion, por,minup_por)\n",
    "            \n",
    "            if por > max_por: # if the polarization of most recent action > maximum polarization of previous actions\n",
    "                max_por = por\n",
    "                champion = (v1, changed_opinion,max_por)   # save the this action as champion    \n",
    "#             else:\n",
    "#                 print('Innate polarization is bigger than max action')\n",
    " \n",
    "    return (champion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'payoff_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3428/3732989827.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpayoff_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mchampion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixed_choose_max_vertex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpayoff_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_touched\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfla_min_fre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'payoff_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)\n",
    "champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # min_touched = []\n",
    "# # payoff_matrix = np.empty((0, 2*n), float)\n",
    "# # fla_min_fre = np.empty((0,n))\n",
    "# # champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre)\n",
    "# # print(champion)\n",
    "# print(c1)\n",
    "# vertices = np.where(c1)\n",
    "# print(vertices)\n",
    "# por=0\n",
    "# for i in c1:\n",
    "#     print(i)\n",
    "#     max_por = 0.75\n",
    "#     if por > max_por:\n",
    "#         max_por = por\n",
    "#         print('yes')\n",
    "#     else:\n",
    "#         print('por<max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre): \n",
    "    op = copy.copy(s)   # op is a copy of innate opinion\n",
    "    \n",
    "    #update innat opinion \n",
    "    op[v2,0] = min_opinion  # Op has been updated by minimizer, so maximizer react to the innate op after that\n",
    "    \n",
    "\n",
    "    max_champion = mixed_choose_max_vertex(payoff_matrix,op, n, min_touched, fla_min_fre) # The best choice among all opinions and vertexs\n",
    "    (v1, max_opinion, max_pol) = max_champion\n",
    "\n",
    "    if v1 == None:\n",
    "        print('Maximizer fail')\n",
    "\n",
    "    else:\n",
    "        print(\"                                \")\n",
    "        print(\"Maximizer finds its target agent:\")\n",
    "        #Store innate_op of the max_selected vertex\n",
    "        old_opinion_max = op[v1, 0]\n",
    "        \n",
    "        ## check if agent's opinionis is changed or not\n",
    "        print(\"    \"+\"Agent\" + str(v1) +\" 's opinion \" + str(old_opinion_max) + \" changed to \"+ str(max_opinion))\n",
    "#         print(\"Network reaches equilibrium Polarization: \" + str(max_pol))\n",
    "#         print('2 opinion changed')\n",
    "#         print(op)\n",
    "\n",
    "    return(v1, max_opinion, max_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Testing function -- NO NEED TO RUN\n",
    "# min_touched = []\n",
    "# v2 = 0\n",
    "# min_opinion = 0\n",
    "# b = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "# print('v1,max_opinion,max_pol')\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Player's Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Innate Op and Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fictitious Play Start !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate_polarization:\n",
      "0.9625703647920427\n",
      "Equi_polarization:\n",
      "0.005278437803904969\n",
      "Difference:\n",
      "-0.9572919269881377\n"
     ]
    }
   ],
   "source": [
    "op = s\n",
    "y = mean_center(s,n)\n",
    "# print(y)\n",
    "innat_pol = np.dot(np.transpose(y), y)[0,0] \n",
    "print('Innate_polarization:')\n",
    "print(innat_pol)\n",
    "\n",
    "# Test equilibrium polarization\n",
    "equ_pol = obj_polarization(A, L, op, n)\n",
    "print('Equi_polarization:')\n",
    "print(equ_pol)\n",
    "\n",
    "di = equ_pol-innat_pol\n",
    "print(\"Difference:\")\n",
    "print(di)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = 'Reddit'\n",
    "memory = 50\n",
    "\n",
    "\n",
    "# with open('Network_'+str(Network)+'.txt', \"a\") as fi:\n",
    "#     print('Innate Opinion', file=fi)\n",
    "#     print(s, file=fi)\n",
    "#     print('Adjacency Matrix', file=fi)\n",
    "#     print(G,file=fi)\n",
    "\n",
    "# Game Preparation\n",
    "def push(obj, element):\n",
    "    if len(obj) >= memory:\n",
    "        obj.pop(0)\n",
    "        print('pop')\n",
    "    obj.append(element)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Parameters\n",
    "Game_rounds =801 # Rounds + 1- use for printing data\n",
    "\n",
    "def all_fre_limited_touch(s, n):\n",
    "    # Preparation for the game\n",
    "    op = copy.copy(s)\n",
    "    payoff_matrix = np.empty((0, 2*n), float)\n",
    "    max_history = np.zeros([n, 2])  # n*2 matrix, agent i & opinion options\n",
    "    min_history = []  # append a list of (agent i, min_opinion), min_opinion can be any value\n",
    "#     print(type(min_history))\n",
    "\n",
    "    max_history_last_100 = np.zeros([n, 2]) \n",
    "    min_history_last_100= []\n",
    "\n",
    "    max_touched = []\n",
    "    min_touched = []\n",
    "    min_touched_all = []\n",
    "    min_touched_last_100 =[]\n",
    "    print('min_touched')\n",
    "    print(min_touched)\n",
    "    \n",
    "    \n",
    "    # Game start from maximizer random play\n",
    "    print('Maximizer first selection')\n",
    "    (v1, max_opinion, max_pol) = random_play(op,n)   # Maximizer does random action \n",
    "    #(v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)\n",
    "    #(v1, max_opinion, max_pol) = (2, 1, 0.14833274000237331)\n",
    "    First_max = (v1, max_opinion, max_pol) \n",
    "\n",
    "\n",
    "#     (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    # Maximizer start with greedy play\n",
    "    # (v1, max_opinion, max_pol) = maximizer_fir_play(s,n,min_touched)   # Maximizer choose action greedily\n",
    "    max_touched.append(v1)    # save Maximizer's action history\n",
    "\n",
    "    # store maximizer play history, using agent(row) and changed opinion(column) as indicator to locate history\n",
    "    max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "    # print('max_history')\n",
    "    # print(max_history)\n",
    "    print('history at spot')\n",
    "    print(max_history[v1,int(max_opinion)])\n",
    "\n",
    "    max_frequency = max_history/1  # its frequency, only played  1 time so far, divided by 1 \n",
    "    # print('fre_max at spot')\n",
    "    # print(max_frequency[v1,int(max_opinion)])\n",
    "\n",
    "    fla_max_fre = max_frequency.flatten()   # flatten the n*2 matrix to a 2n*1 matrix\n",
    "                                            # so we can multiply the freuency (2n*1)with payoff array (1*2n) \n",
    "                                            # to get average payoff of fictitious play\n",
    "    print('fre_max at spot')\n",
    "    print(fla_max_fre)\n",
    "\n",
    "    column = int(column_index(v1,max_opinion))    # the frequency of maximizer's most recent action (v1,max_opinion)\n",
    "\n",
    "    print(fla_max_fre[column])\n",
    "\n",
    "    # print(np.shape(fla_max_fre.shape))\n",
    "\n",
    "\n",
    "    # if game start from minimizer random play - make sure two random play are not same agent!!!\n",
    "    print('Minimizer first selection')\n",
    "    (v2, min_opinion, min_pol) = random_play(op,n) \n",
    "    #(v2, min_opinion, min_pol) = minimizer_fir_play(s,n,min_touched)\n",
    "    \n",
    "    #(v2, min_opinion, min_pol) = (1, 0, 0.5933309600094931)\n",
    "    First_min = (v2, min_opinion, min_pol)\n",
    "\n",
    "    if v1==v2:   # if Max and Min randomly selected the same agent, then we need to restart - cannot choose same agent\n",
    "        sys.exit()\n",
    "\n",
    "    # Minimizer start with greedy play\n",
    "    # (v2, min_opinion, min_pol) = minimizer_fir_play(s,n,max_touched)\n",
    "\n",
    "    min_touched.append(v2)\n",
    "   \n",
    "\n",
    "    # store minimizer play history\n",
    "    min_history.append((v2,min_opinion))\n",
    "    print('min_history')\n",
    "    print(min_history)\n",
    "\n",
    "\n",
    "    counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/1 #return only frequency of all min options in order\n",
    "#     print('fla_min_fre')\n",
    "#     print(fla_min_fre)\n",
    "\n",
    "\n",
    "    (a,payoff_row) = mixed_min_polarization(s,v2,min_opinion,fla_max_fre)\n",
    "    payoff_matrix = np.vstack([payoff_matrix, payoff_row])\n",
    "#     print('Payoff Matrix')\n",
    "#     print(payoff_matrix)\n",
    "    print('fla_min_fre at the spot')\n",
    "    min_counter = dict(counter)\n",
    "    print(min_counter) \n",
    "    print(min_counter[(v2,min_opinion)]) \n",
    "#     print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "\n",
    "    equi_min = min_pol\n",
    "    equi_max = max_pol\n",
    "    # print(equi_min)\n",
    "    # print(equi_max)\n",
    "\n",
    "\n",
    "\n",
    "    Flag = 0\n",
    "\n",
    "    i = 0\n",
    "    while Flag == 0: \n",
    "        i = i + 1\n",
    "        print(\"Game \" + str(i))\n",
    "        print(\"_____________________\")\n",
    "\n",
    "    #     if max_pol == min_pol:\n",
    "        if i == Game_rounds:            # i == # of iterations you want to run + 2\n",
    "                                # because Game 101 is skipped for collecting data, to get 200 game result, we need to run 201 iteration\n",
    "            print('min_recent_'+str(memory)+'_touched')# then stop at Game 202\n",
    "            print(min_touched)\n",
    "            print('max_recent_'+str(memory)+'_touched')\n",
    "            print(max_touched)\n",
    "            print('Min last 100 action')\n",
    "            print(min_touched_last_100)\n",
    "\n",
    "            break\n",
    "\n",
    "        elif equi_min == equi_max:\n",
    "            print(\"Reached Nash Equilibrium at game\"+ str(i) + \"and Equi_Por = \" + str(equi_min))\n",
    "            print('max_distribution')\n",
    "            print(max_frequency)\n",
    "            print('min_distribution')\n",
    "            print(fla_min_fre)\n",
    "            Flag = 1\n",
    "            break\n",
    "        ############################## maximizer play  \n",
    "        else:\n",
    "            if i == Game_rounds-100:    #if Game_round = 200, after 100 iteration, Game 101 print previous historical result\n",
    "    #             max_touched_100 = max_touched \n",
    "    #             min_touched_100 = min_touched\n",
    "    #             max_fre_100 = max_frequency  # store the max_frequency of first 100 iterataions\n",
    "    #             print('max_history')\n",
    "    #             print(max_history)\n",
    "    #             min_fre_100 = fla_min_fre  # max_frequency of first 100 iterations\n",
    "    #             print('min_history')\n",
    "    #             print(min_history)\n",
    "    # Remove max frequncy less than 0.1--\n",
    "                max_history_last_100 = np.zeros([n, 2]) \n",
    "                min_history_last_100 = [] \n",
    "                min_touched_last_100 =[]\n",
    "\n",
    "            (v1, max_opinion, equi_max) = mixed_max_play(payoff_matrix,s,v2,min_opinion,n,min_touched,fla_min_fre)\n",
    "            max_touched = push(max_touched, v1)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "    #             print('equi_max')\n",
    "    #             print(equi_max)\n",
    "    #         print(v1, max_opinion, max_pol)\n",
    "            # cumulate strategy \n",
    "            max_history[v1,int(max_opinion)] = max_history[v1,int(max_opinion)] +1\n",
    "\n",
    "            max_history_last_100[v1,int(max_opinion)] = max_history_last_100[v1,int(max_opinion)] +1\n",
    "    #         print('max_history')\n",
    "    #         print(max_history)\n",
    "    #________________________________________________________________\n",
    "            max_frequency = max_history/(i+1)  # its frequency \n",
    "    #         print('max_distribution')\n",
    "    #         print(max_frequency)\n",
    "        #     print(i+1) \n",
    "            fla_max_fre = max_frequency.flatten() #flaten max_frequency to calculate average payoff\n",
    "#             print('fla_max_fre')\n",
    "#             print(fla_max_fre)\n",
    "            print('fre_max at spot')\n",
    "            print(fla_max_fre[column])\n",
    "            # create payoff matrix for maxmizer\n",
    "            row = int(row_index(v2, min_opinion))\n",
    "            column = int(column_index(v1,max_opinion))\n",
    "\n",
    "    # _________________________________________________________________\n",
    "    #         ######################Visualize Maximizer's selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v1:\n",
    "    #                 color_map.append('Red')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True,node_size = 50)\n",
    "    #         plt.figure(figsize=(200, 200))\n",
    "    #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    ############################### minimizer play\n",
    "            (v2, payoff_row, min_opinion, equi_min) = mixed_min_play(s,v1,max_opinion,n, max_touched,fla_max_fre)\n",
    "            min_touched = push(min_touched, v2)\n",
    "            min_touched_all.append(v2) \n",
    "            min_touched_last_100.append(v2)\n",
    "    #         print('min_touched')\n",
    "    #         print(min_touched)\n",
    "    #         print('equi_min')\n",
    "    #         print(equi_min)\n",
    "    #         print('max_touched')\n",
    "    #         print(max_touched)\n",
    "            #         print(v2, min_opinion, min_pol)\n",
    "            if (v2,min_opinion) in counter.keys():\n",
    "                payoff_matrix = payoff_matrix # if this min_option is in min_history, no need to update paryoff matrix, only update frequency\n",
    "                print(\"Same history\")\n",
    "                print((str(v2),str(min_opinion)))\n",
    "            else:\n",
    "                payoff_matrix = np.vstack([payoff_matrix, payoff_row]) # if this is a new option, append to previous matrix\n",
    "    #                 print('payoff_row')\n",
    "    #                 print(payoff_row)\n",
    "            min_history.append((v2,min_opinion))\n",
    "            min_history_last_100.append((v2,min_opinion))\n",
    "            #         print('min_history')\n",
    "            #         print(min_history)\n",
    "            counter=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "            #print(counter)\n",
    "    #         print('counter.keys')\n",
    "    #         print(counter.keys())\n",
    "            fla_min_fre = np.array(list(counter.values()))/(i+1) #return only frequency of all min options in order\n",
    "    #         print('fla_min_fre')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "    #         print('fla_min_fre at the spot')\n",
    "    #         min_counter = dict(counter)\n",
    "    #         print(min_counter[(v2,min_opinion)]/(i+1)) #get the value from dictionary by using key (v2,opinion)\n",
    "\n",
    "            # create payoff matrix for minimizer\n",
    "            row = row_index(v2, min_opinion)\n",
    "            column = column_index(v1,max_opinion)\n",
    "            #     print('row, column')\n",
    "            #     print(row, column)\n",
    "\n",
    "            print(\"Not Reached Nash Equilibrium at Equi_Min = \" + str(equi_min) + \" and Equi_Max = \"+ str(equi_max)) \n",
    "    #         print('min_distribution')\n",
    "    #         print(fla_min_fre)\n",
    "\n",
    "            ######################Visualize Minimizer selection\n",
    "    #         La = scipy.sparse.csgraph.laplacian(G1, normed=False)\n",
    "\n",
    "    #         nxG = nx.from_numpy_matrix(G1)\n",
    "\n",
    "    #         color_map = []\n",
    "    #         for node in nxG:\n",
    "    #             if node == v2:\n",
    "    #                 color_map.append('Blue')\n",
    "    #             else: \n",
    "    #                 color_map.append('Grey')  \n",
    "\n",
    "    #         #nxG1 = nx.DiGraph(G)\n",
    "    #         nx.draw(nxG, node_color=color_map, with_labels=True)\n",
    "    #         plt.figure(figsize=(25, 25))\n",
    "    #         plt.show()\n",
    "    return (First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, max_history_last_100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_touched\n",
      "[]\n",
      "Maximizer first selection\n",
      "    Agent296 's opinion 0.45455 changed to 1\n",
      "Network reaches equilibrium Polarization: 0.005309096380369718\n",
      "history at spot\n",
      "1.0\n",
      "fre_max at spot\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "1.0\n",
      "Minimizer first selection\n",
      "    Agent480 's opinion 0.5 changed to 0\n",
      "Network reaches equilibrium Polarization: 0.005699065816250973\n",
      "min_history\n",
      "[(480, 0)]\n",
      "Counter({(480, 0): 1})\n",
      "fla_min_fre at the spot\n",
      "{(480, 0): 1}\n",
      "1\n",
      "Game 1\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent481 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent284 's opinion 0.625 changed to 0.0\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.03630565273040436 and Equi_Max = 0.07053567726529063\n",
      "Game 2\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent50 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.3333333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent245 's opinion 0.4091 changed to 0.5038040096966088\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.04726304577612197 and Equi_Max = 0.07015268075934114\n",
      "Game 3\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent50 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.5\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent359 's opinion 0.5 changed to 0.0\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.05239621902975951 and Equi_Max = 0.06962960963664552\n",
      "Game 4\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent80 's opinion 0.55555 changed to 1\n",
      "fre_max at spot\n",
      "0.4\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent245 's opinion 0.4091 changed to 0.503531992767091\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.055786522819871835 and Equi_Max = 0.0696722976883874\n",
      "Game 5\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent80 's opinion 0.55555 changed to 1\n",
      "fre_max at spot\n",
      "0.3333333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent484 's opinion 0.4737 changed to 0.0\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.05751038791984847 and Equi_Max = 0.06944986349223146\n",
      "Game 6\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent80 's opinion 0.55555 changed to 0\n",
      "fre_max at spot\n",
      "0.2857142857142857\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent245 's opinion 0.4091 changed to 0.4969082824628156\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.05936825596705517 and Equi_Max = 0.06949050542991982\n",
      "Game 7\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent77 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.125\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent245 's opinion 0.4091 changed to 0.5038052314767938\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.06050551101371199 and Equi_Max = 0.06930382950580936\n",
      "Game 8\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent77 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.2222222222222222\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent245 's opinion 0.4091 changed to 0.5038052314767938\n",
      "Same history\n",
      "('245', '0.5038052314767938')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.06139251716535653 and Equi_Max = 0.06920192161489733\n",
      "Game 9\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent481 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.2\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent245 's opinion 0.4091 changed to 0.5037705664684997\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.062139389862309075 and Equi_Max = 0.06914146977638778\n",
      "Game 10\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent481 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.2727272727272727\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent245 's opinion 0.4091 changed to 0.5037705664684996\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.06275047448077797 and Equi_Max = 0.0691134548652957\n",
      "Game 11\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent481 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.3333333333333333\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent245 's opinion 0.4091 changed to 0.5037705664684996\n",
      "Same history\n",
      "('245', '0.5037705664684996')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.06325971166283538 and Equi_Max = 0.06909053357440217\n",
      "Game 12\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent481 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.38461538461538464\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent245 's opinion 0.4091 changed to 0.5037705664684997\n",
      "Same history\n",
      "('245', '0.5037705664684997')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.0636906046630378 and Equi_Max = 0.06907143249865758\n",
      "Game 13\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent481 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.42857142857142855\n",
      "_______________________\n",
      "Minimizer Play\n",
      "                                \n",
      "Minimizer finds its target agent:\n",
      "    Agent245 's opinion 0.4091 changed to 0.5037705664684996\n",
      "Same history\n",
      "('245', '0.5037705664684996')\n",
      "Not Reached Nash Equilibrium at Equi_Min = 0.06405994152035416 and Equi_Max = 0.0690552700499506\n",
      "Game 14\n",
      "_____________________\n",
      "                                \n",
      "Maximizer finds its target agent:\n",
      "    Agent481 's opinion 0.5 changed to 1\n",
      "fre_max at spot\n",
      "0.4666666666666667\n",
      "_______________________\n",
      "Minimizer Play\n"
     ]
    }
   ],
   "source": [
    "Experiment = 3\n",
    "\n",
    "Experiment_note = str('Note: This experiement has initial condition. Game round:'+str(Game_rounds)+'.')\n",
    "(First_max, First_min, max_touched, min_touched, payoff_matrix, min_history, fla_min_fre, min_history_last_100, min_touched_last_100, min_touched_all, max_history, max_history_last_100) = all_fre_limited_touch(s, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91 0.07 0.02]\n",
      "[(245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684996), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684996), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684994), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684996), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684996), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684996), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684994), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684996), (245, 0.5037705664684996), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995), (245, 0.5037705664684995)]\n"
     ]
    }
   ],
   "source": [
    "print(fla_min_fre)\n",
    "\n",
    "print(min_history_last_100)\n",
    "\n",
    "import pandas as pd \n",
    "pd.DataFrame(payoff_matrix).to_csv(\"Reddit\"+str(Experiment)\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_distribution\n",
      "[1.]\n",
      "(array([481]), array([1]))\n",
      "Min_distribution\n",
      "dict_keys([(245, 0.5037705664684995), (245, 0.5037705664684996), (245, 0.5037705664684994)])\n",
      "fla_min_fre\n",
      "[0.91 0.07 0.02]\n",
      "[245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245]\n",
      "Counter({(245, 0.5037705664684995): 733, (245, 0.5037705664684996): 40, (245, 0.5037705664684994): 27, (57, 1): 1})\n",
      "fla_min_fre_1\n",
      "[0.001 0.915 0.034 0.05 ]\n"
     ]
    }
   ],
   "source": [
    "# MAXimizer's distribution of LAST 100 iteration \n",
    "print('Max_distribution')  \n",
    "max_l100_fre = max_history_last_100/100\n",
    "print(max_l100_fre [np.nonzero(max_l100_fre)])\n",
    "# print for small network\n",
    "#print(max_history_last_100)\n",
    "# # Print for Large Network\n",
    "print(np.nonzero(max_l100_fre))\n",
    "\n",
    "# MINimizer's Strategy in the last 100 round\n",
    "print('Min_distribution')\n",
    "counter=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter.keys())\n",
    "fla_min_fre = np.array(list(counter.values()))/(100) #return only frequency of all min options in order\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)\n",
    "\n",
    "print(min_touched_last_100)\n",
    "counter_1=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "print(counter_1)\n",
    "fla_min_fre_1 = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "print('fla_min_fre_1')\n",
    "print(fla_min_fre_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.009 0.009 0.006 ... 0.006 0.006 0.006]\n",
      " [0.008 0.007 0.005 ... 0.005 0.005 0.004]\n",
      " [0.008 0.007 0.005 ... 0.005 0.005 0.004]\n",
      " [0.008 0.007 0.005 ... 0.005 0.005 0.004]]\n"
     ]
    }
   ],
   "source": [
    "print(payoff_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Result'+str(Network)+'.'+str(Experiment)+'.txt', \"a\") as f:\n",
    "#     print(Experiment_note, file=f)\n",
    "    print('Initial Condition -(agent, opinion, pol)', file=f)\n",
    "    print('Innate op'+str(s),file=f)\n",
    "    print('Adjacency matrix'+ str(G), file=f)\n",
    "    print('Max:'+ str(First_max), file=f)\n",
    "    print('Min' + str(First_min), file=f)\n",
    "\n",
    "    print(\"In the Last 100 Rounds\", file=f) \n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # MAX distribution of LAST 100 iteration \n",
    "    print('Max_distribution', file=f)  \n",
    "    max_l100_fre = max_history_last_100/100\n",
    "    print(max_l100_fre [np.nonzero(max_l100_fre)], file=f)\n",
    "    # print for small network\n",
    "    #print(max_history_last_100, file=f)\n",
    "    # # Print for Large Network\n",
    "    print(np.nonzero(max_l100_fre),file=f)\n",
    "\n",
    "    # MIN Strategy in the last 100 round\n",
    "    counter=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    # print(counter)\n",
    "    fla_min_fre = np.array(list(counter.values()))/100 #return only frequency of all min options in order\n",
    "    print('Min_frequency', file=f)\n",
    "    print(list(counter.keys()), file=f)\n",
    "    print('Min_distribution_last_100', file=f)\n",
    "    print(fla_min_fre, file=f)\n",
    "    counter_h=collections.Counter(min_history_last_100)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_h.keys(), file=f)\n",
    "    \n",
    "    print('min_recent_'+str(memory)+'_touched', file=f)# then stop at Game 202\n",
    "    print(min_touched, file=f)\n",
    "    print('max_recent_'+str(memory)+'_touched', file=f)\n",
    "    print(max_touched, file=f)\n",
    "    \n",
    "    print('In Overall'+str(Game_rounds)+' Rounds', file=f)\n",
    "    print('_____________________', file=f)\n",
    "    \n",
    "    # Max action Overall \n",
    "    np.set_printoptions(precision=3)\n",
    "\n",
    "    max_fre = max_history/Game_rounds\n",
    "    print('Max_frequency', file=f)\n",
    "    print(max_history, file=f)\n",
    "    print('Max_distribution', file=f)\n",
    "    print(max_fre [np.nonzero(max_fre)], file=f)\n",
    "    \n",
    "\n",
    "\n",
    "    # Min Strategy in the Overall    \n",
    "    counter_1=collections.Counter(min_touched_all)  #return a dictionary include {'min_option': count of this choice}\n",
    "    fla_min_fre_all = np.array(list(counter_1.values()))/Game_rounds #return only frequency of all min options in order\n",
    "    print('Min_dist_all', file=f)\n",
    "    print(fla_min_fre_all, file=f)\n",
    "    print('Min_distribution', file=f)\n",
    "    counter_a=collections.Counter(min_history)  #return a dictionary include {'min_option': count of this choice}\n",
    "    print(counter_a.keys(), file=f)\n",
    "    print(payoff_matrix, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(1, 0.49999999999999983): 366, (1, 0.4999999999999998): 31, (1, 0.4999999999999999): 2, (1, 0): 1, (1, 0.9999999999999997): 1})\n",
      "fla_min_fre\n",
      "[0.002 0.002 0.077 0.913 0.005]\n"
     ]
    }
   ],
   "source": [
    "counter=collections.Counter(min_history) \n",
    "print(counter)\n",
    "fla_min_fre = np.array(list(counter.values()))/Game_rounds\n",
    "print('fla_min_fre')\n",
    "print(fla_min_fre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
